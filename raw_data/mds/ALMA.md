# A PARADIGM SHIFT IN MACHINE TRANSLATION: BOOSTING TRANSLATION PERFORMANCE OF LARGE LANGUAGE MODELS  

Haoran $\mathbf{X}\mathbf{u}^{\pmb{\phi}}$ , Young Jin $\mathbf{Kim}^{\odot}$ , Amr Sharaf , Hany Hassan Awadalla  

\* Johns Hopkins University, Microsoft  

hxu64@jhu.edu {youki,amrsharaf,hanyh}@microsoft.com  

# ABSTRACT  

Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two finetuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. Weintroduce theLLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 (Touvron et al., 2023b) as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model (NLLB TEAM et al., 2022) and GPT3.5-text -davinci -003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation. 1  

# 1 INTRODUCTION  

Generative (decoder-only) large language models (LLMs) such as GPT models (Brown et al., 2020; OpenAI, 2023), PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), BLOOM (Sca0 et al., 2022), LLaMA (Touvron et al., 2023a;b), and others have exhibited remarkable capabilities across various NLP tasks. However, for the translation task, only very large models such as GPT-3.5 and GPT-4 can rival the supervised encoder-decoder state-of-the-art (SoTA) models like NLLB (NLLB TEAM et al., 2022), while they still fall short in translation for low-resource languages (Hendy et al., 2023; Jiao et al., 2023). The discrepancy becomes more evident when comparing other LLMs with traditional translation models (Zhu et al., 2023a). For instance, the OPT-175B model trails behind the NLLB-1.3B model by an average of more than 15 BLEU (Papineni et al., 2002) points for languages within the Indo-European-Romance family. The gap is even larger in smaller LLMs; for example, XGLM (Lin et al., 2021), with a parameter size of 7B, lags behind the NLLB-1.3B by a substantial 30 BLEU points (Zhu et al., 2023a). Therefore, there is an urgent need to narrow this performance gap between LLMs and conventional SoTA models.  

As exemplified by NLLB-1.3B, traditional machine translation models demonstrate proficiency in producing high-quality translations with a small number of parameters. By extension, smaller LLMs should similarly possess the capability to adeptly manage the translation task. Recent research has sought to enhance translation performance by commencing with smaller LLMs (Yang et al., 2023; Zeng et al., 2023; Chen et al., 2023; Zhu et al., 2023b; Li et al., 2023; Zhang et al., 2023b), especially 7B or 13B parameters. Nevertheless, the achieved improvements remain modest and limited. As depicted in Figure 1, contemporary studies such as Balyling (Zhang et al., 2023b) and BigTranslate (Yang et al., 2023), which use LLaMA as their backbone, exhibit a maximum increment of 3 to 4 BLEU or COMET in relation to the zero-shot performance of LLaMA on the WMT'22 test set (8 directions).2 While these gains represent promising research direction for smaller LLMs in the translation task, a significant performance chasm persists when benchmarked against very large LLMs such as GPT-3.5-text -davinci-003 and SoTA translation models such as NLLB-54B. We posit that the modest translation gains observed in prior studies can be ascribed to an unsuitable training recipe.  

![](https://cdn-mineru.openxlab.org.cn/extract/463a5bf0-ef11-4af2-a29c-d808701c1ff7/b804f744599a3f25ea2609bd3af964b3bb97342631e9e95711d8042e06b52fce.jpg)  
Figure 1: Translation performance of contemporary decoder-only LLM translation systems based on LLaMA (Yang et al., 2023; Zhang et al., 2023b), and zero-shot performance of LLaMA, for the WMT'22 test data across 8 directions (translating to or from English for German, Czech, Chinese, and Russian). Benchmark comparisons also include two leading translation models, NLLB-54B and GPT-3.5-text -davinci -003. Our systems, developed on LLaMA-2 with 7B and 13B parameters, surpass previous models by an impressive margin of nearly 10 BLEU and 7 COMET. Furthermore, they even slightly outperform GPT-3.5 and NLLB-54B on average.  

We hypothesize that an efficacious training recipe ought to follow two stages: learning general multilingual linguistic knowledge and inducing (instructing) models toward translation generation. Consequently, we propose a two-stage fine-tuning approach and introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Specifically, given most LLMs are trained on English-dominant data, the first stage is fine-tuning non-English monolingual data to enhance the model's proficiency in other languages involved in the translation task. Secondly, drawing inspiration from the recognized significance of data quality in other applications (Zhou et al., 2023; Maillard et al., 2023; Gunasekar et al., 2023), we fine-tune the model with a small amount of high-quality parallel data.  

Our main contributions are summarized as follows:  

Diminished Necessity of Parallel Data Traditional translation frameworks rely on large amounts of parallel data, which may lead to a false impression that such data is essential for the translation task with LLMs. Prior studies have fine-tuned LLMs with datasets containing over 300M parallel instances (Yang et al., 2023). However, our empirical evaluations suggest that this strategy may not be optimal, and even harm the translation capabilities of LLMs.  

LLM Via A New Training Recipe: ALMA We introduce a novel two-stage fine-tuning method for translation with decoder-only LLMs. Leveraging LLaMA-2 as the base model, we attain an average improvement of more than 12 BLEU and COMET scores over its zero-shot performance across 10 translation directions from WMT'21 and WMT'22 test datasets. Notably, the performance surpasses all previous work and is even better than the NLLB-54B model and GPT-3.5- text-davinci-003.  

Efficient Computational Cost Our ablation study reveals both stages are crucial factors for achieving large improvements. The most computationally intensive part is monolingual data fine-tuning, however, we show that only fine-tuning 1B monolingual tokens is sufficient to have comparable performance to NLLB-54B in 10 translation directions, which only requires around 18 hours to complete with 16 MI200 GPUs.  

## 2 PRELIMINARY  

### 2.1 TASK DEFINITION  

We consider a decoder-only transformer model parameterized by $\theta$ for machine translation. Let x represent the source sentence and $\mathbf{y}$ its corresponding target sentence. We utilize a fixed prompt template, denoted as $\mathcal{T}$ , to guide the model in generating translation. The log-likelihood loss of the parallel sentence $(\mathbf{x},\mathbf{y})$ with regard to the model parameters $\theta$ can be formulated as follows:  

$$
\begin{array}{r}{\mathcal{L}_{\mathrm{NLL}}(\mathbf{x},\mathbf{y},\boldsymbol{\theta})=-\log P(\mathbf{y}|\mathbf{x},\mathcal{L};\boldsymbol{\theta})\qquad}\ {=-\displaystyle\sum_{t=1}^{T}\log P(y_{t}|\mathbf{y}_{<t},\mathbf{x},\mathcal{L};\boldsymbol{\theta}),}\end{array}
$$  

where $T$ is length of the target sentence, and $y_{t}$ is the $t$ -th target token. The loss is a standard causal language modeling (CLM) loss, which predicts the next token based on prior tokens. We use the same sentence-level translation prompt template suggested by Hendy et al. (2023), and illustrate the prompt and the model input/target in Figure 2. Note that we do not compute the loss for the prompt template and source sentence during training (Zhang et al., 2023a). In Appendix A, we show that CLM is more suitable for the translation task compared with other modeling methods, such as prefix language modeling (Wang et al., 2022) and mixture-of-denoisers (Tay et al., 2022a).  

![](https://cdn-mineru.openxlab.org.cn/extract/463a5bf0-ef11-4af2-a29c-d808701c1ff7/2aed13062605c47cb8df320fb9666351f83d6068d47ba7bbffa776044aa5596f.jpg)  
Figure 2: The prompt used for training and evaluation. [source language] and [target language] represent the full name of the language, e.g., Translate this from German to English. Note that we do not compute loss for the prompt.  

### 2.2A BACKBONE LLM FOR TRANSLATION  

We seek a robust LLM to serve as our foundational model. With the recent emergence of numerous LLMs, we prioritize evaluating the zero-shot translation performance of these models before delving into optimal training recipes. As most of these models provide a 7B version, our comparative analysis centers on this magnitude: OPT-7B (Zhang et al., 2022), Falcon-7B (Almazrouei et al., 2023), BLOOM-7B (Sca0 et al., 2022), MPT-7B (MosaicML, 2023), LLaMA-1-7B (Touvron et al., 2023a), and LLaMA-2-7B (Touvron et al., 2023b). We additionally present results from GPT-3.5- text -davinci-003 (hereinafter referred to as GPT-3.5-D) and GPT-3.5-turbo-0301 (hereinafter referred to as GPT-3.5-T) to show the performance gap.3  

Zero-Shot Evaluation We conduct zero-shot evaluations on 5 English-centric language pairs, considering both from English and to English directions: German (de), Czech (cs), Icelandic (is), Chinese (zh) and Russian (ru), where Icelandic test data is from WMT'21 and the others are from WMT'22. We choose these test dataset because they are the recent and less likely to overlap the training data used by LLMs, and importantly, they have high-quality data to avoid problems of “translationese" (Zhang & Toral, 2019). The beam size is 5.  We report sacreBLEU (zh tokenizer for Chinese and ${}^{13a}$ for the others) (Post, 2018). We also report COMET (Unbabe1 /wmt 22-comet -da) (Rei et al., 2022) because BLEU only refects the degree of lexical match. In this paper, We rely more on COMET than BLEU due to its better alignment with human evaluations (Freitag et al., 2022).4  

![](https://cdn-mineru.openxlab.org.cn/extract/463a5bf0-ef11-4af2-a29c-d808701c1ff7/c4a89275d77ba2d85069589f7a7d49c213bcca353457348efb03c2182bb36ae7.jpg)  
Figure 3: Averaged zero-shot translation performance on 10 directions: $\mathsf{C S}\longleftrightarrow\mathsf{e n}$ $\mathsf{d e}{\leftrightarrow}\mathsf{e n}$ ${\mathrm{i}}S{\leftrightarrow}{\in}\Pi$ $z\ln{\leftrightarrow}\Theta\pi$ $\mathtt{r u}{\leftrightarrow}\Theta\ n$ ,where ${\mathrm{i}}S{\leftrightarrow}{\in}\ n$ is from WMT'21 test data and the others from WMT'22 test data.  

LLM Translation Performance The overall results for the LLMs are presented in Figure 3, with scores averaged across five languages for translations to and from English. Among the 7B LLMs, LLaMA-2-7B exhibits superior performance translating into English, while MPT-7B leads in translations out of English, as measured by BLEU. However, when evaluated with COMET, LLaMA-2- 7B wins in both directions. We show the numeric results in Appendix B. Consequently, we select LLaMA-2-7B and MPT-7B for further investigation into the necessity of parallel data for LLMs.  

## 3 DO LLMS HAVE AN APPETITE FOR PARALLEL DATA?  

Conventional machine translation training predominantly relies on utilizing large volumes of parallel datasets within encoder-decoder frameworks. This trend is not confined to training models from scratch but also pertains to strategies that fine-tune pre-trained LLMs, often involving millions of parallel sentences (Rothe et al., 2020; Liu et al., 2020; Xu et al., 2021; 2023; Yang et al., 2023). In this section, we examine whether the recently proposed decoder-only LLMs retain a dependence on substantial parallel data and adhere to the traditional training paradigm.  

### 3.1  EXPERIMENTAL DESIGN  

Following Section 2.2, our focus narrows to fine-tuning LLaMA-2-7B and MPT-7B. To allow for a deep analysis, we concentrate on one language pair, English $\rightarrow$ Russian $(\mathtt{e n}{\to}\mathtt{r u})$ 0.We opted for a language pair that is translating out of English and to a non-Latin language, since those categories show larger gaps with SoTA models in our initial investigation in Section 2.2. We use the clean data filtered from 75M parallel sentences from Hendy et al. (2023) and split the data size in 5 levels: 10K, 100K, 1M, 5M, and 20M. We use the same prompt template and training scheme as described in Section 2.1, and train the model by updating all parameters. Detailed training settings can be found in Appendix C.  

### 3.2 OBSERVATIONS  

The fine-tuning results for LLaMA-2-7B and MPT-7B at each data size step are presented in Figure 4. Additionally, we benchmark these against the performance of the NLLB-54B model to show the disparity with one of the SoTA multilingual translation models.  

![](https://cdn-mineru.openxlab.org.cn/extract/463a5bf0-ef11-4af2-a29c-d808701c1ff7/032c3b99ab99cf985631fea34e2fb9e01aa52a0fc86bd17f3b79d82b945a2c70.jpg)  
Figure 4: BLEU and COMET scores obtained during the fine-tuning of MPT-7B and LLaMA-2- 7B across each data step for $\tt e n\to\tt r u$ . Additionally, we present the results for NLLB-54B and a 7B model trained from scratch. A notable decline in LLaMA-2-7B's COMET score suggests that substantial parallel data might dilute its pre-existing knowledge.  

Small Training Data Is Enough According to COMET, there is a notable difference in the curve of LLaMA-2-7B and MPT-7B: LLaMA-2-7B peaks with 10K and 100K training data before experiencing a decline, while MPT-7B exhibits continuous improvement with more training data. LLaMA2-7B requires only limited training examples (10K and 100K) to achieve competent translation. However, a surplus of examples (5M or 20M) seems to dilute its existing knowledge in Russian. Conversely, MPT-7B, potentially due to its inherently weaker translation capability, exhibits improved performance with an increase in training data. This may suggest that LLaMA-2 or other well-trained LLMs may not necessitate substantial parallel data.  

Large Parallel Data Wash Out the Knowledge Both LLMs eventually achieve similar BLEU and COMET with 20M training data, regardless of their performance on smaller data. We hypothesize that this phenomenon is caused by catastrophic forgetting (French, 1999; Kirkpatrick et al., 2017), suggesting that too many parallel data wash out the pre-existing knowledge. To validate this hypothesis, we consider an extreme case: training the model from scratch using 20M data, thereby erasing all prior knowledge.? As expected, it tends up with a similar performance in both BLEU and COMET evaluations (triangle in Figure 4), strengthening our speculation regarding the dilution of LLM's intrinsic knowledge with extensive data training.  

Beyond BLEU COMET reveals a decline in translation performance for LLaMA-2-7B as the amount of parallel data increases, a trend not captured by BLEU which shows an increase. This discrepancy arises since BLEU primarily evaluates lexical overlap, and the extensive WMT training data, being similar in domain to the test set, likely enhances this measure. This highlights the necessity of utilizing additional metrics (like COMET) for a comprehensive evaluation of translation.  

From our observations, LLaMA-2 (potentially other well-trained LLMs) should not adopt the same training approach as earlier models——- whether randomly initialized or pre-trained—-that rely heavily on vast amounts of training data.  

## 4  A NEW TRAINING RECIPE  

We demonstrate that LLMs like LLaMA-2-7B do not voraciously consume parallel data. We introduce a novel training strategy that markedly enhances translation performance without relying heavily on parallel data. The recipe comprises two stages: continuous monolingual data finetuning and high-quality parallel data fine-tuning. After applying our training recipe to LLMs, we name the resulting model as ALMA (Advanced Language Model-based trAnslator).  

Monolingual Data Fine-tuning LLMs like LLaMA are pre-trained on English-dominated corpora. This potentially explains their inadequate translation performance which necessitates cross-lingual capabilities. To ameliorate this, our first stage is fine-tuning LLMs with monolingual data of nonEnglish languages involved in translation tasks, enhancing their proficiency in these languages. Note that we also add English monolingual data during fine-tuning to prevent English knowledge forgetting. Previous studies also offer some clues that monolingual data help in translation. For instance, Tan et al. (2023) utilizes a monolingual target corpus to bridge the gap in translation mismatches caused by domain discrepancies. BigTranslate (Yang et al., 2023) and PolyLM (Wei et al., 2023) use a huge amount of Chinese monolingual data and improve translation to or from Chinese. Furthermore, Li et al. (2023) utilizes monolingual generation instructions to improve translation. In Section 6.1, we show that utilizing small monolingual data and modest computational cost (e.g., 1B monolingual tokens mixed by 6 languages and fine-tuning under 18 hours), can facilitate significant improvements in 10 translation directions. Note that we employ full-weight fine-tuning at this stage.  

High-Quality Data Fine-tuning Drawing on insights from Section 3.2 that LLMs may require only small parallel data, coupled with previous research emphasizing training data quality (Zhou et al., 2023; Maillard et al., 2023; Gunasekar et al., 2023), we fine-tune the model using a small, yet high-quality parallel dataset in this stage. To ensure the data quality, we collect human-written datasets from WMT test data and Flores-200 (NLLB TEAM et al., 2022) development and test sets. Here, we explore both full-weight and lightweight Low-Rank Adaptation (LoRA) (Hu et al., 2022; Mangrulkar et al., 2022) fine-tuning, where we apply LoRA to the down-projection layer in each feed-forward network.  

## 5 EXPERIMENTS  

### 5.1DATA  

For our parallel training data, we collect human-written test datasets from WMT'17 to WMT'20, plus the development and test sets from Flores-200 (NLLB TEAM et al., 2022), resulting in a total of 58K training examples across all languages. For the test data, we still use the same 10 translation directions to be consistent with our study in Section 2: $\subset S{\longleftrightarrow}\in\ncap$ ， $\mathsf{d e}{\leftrightarrow}\mathsf{e n}$ ， ${\mathrm{i}}S{\leftrightarrow}{\in}\Pi$ ， $z\ln{\leftrightarrow}\Theta\pi$ ， $\mathtt{r u}{\leftrightarrow}\Theta\ n$ , where ${\mathrm{i}}S{\leftrightarrow}{\in}\Pi$ is from WMT'21 and the others are from WMT'22. Test data in WMT'21 (except for $\perp S$ ) is used for the development dataset (a total of 8K parallel sentences).6 The monolingual dataset is sourced from OsCAR (Ortiz Su'arez et al., 2019; Kreutzer et al., 2022). We mix the monolingual data and fine-tune the model with a sampling ratio of $20\%$ ， $14\%$ ， $8\%$ $19\%$ $22\%$ , and $17\%$ respectively for de, cs, is, zh, ru and en. We explain the reasoning behind the sampling ratios and show the detailed parallel data information in Appendix D.  

### 5.2  TRAINING SETUP  

We train the model in a many-to-many multilingual translation manner, and use LLaMA-2-7B (or 13B) as our backbone model given its best zero-shot performance. Our two-stage fine-tuning process yields two model types, differentiated based on the utilization of LoRA:  

ALMA-7B/ALMA-13B Full-Weight fine-tuning on monolingual data followed by Full-Weight fine.   
tuning on high-quality parallel data for LLaMA-2-7B or -13B models.  

ALMA-7B-LoRA/ALMA-13B-LoRA Full-Weight fine-tuning on monolingual data followed by LoRA fine-tuning on high-quality parallel data for LLaMA-2-7B or -13B models.  

If using LoRA, the LoRA rank is 16 and only updates $0.1\%$ parameters (7.7M for 7B and 12M for 13B model). Both monolingual data fine-tuning and human-written data fine-tuning basically share the same hyperparameter settings. Specifically, we fine-tune LLaMA-2 with a batch size of 256, a warm-up ratio of 0.01, and a sequence containing a maximum of 512 tokens. For monolingual data fine-tuning, we train the LLaMA-2-7B up to 20B tokens and LLaMA-2-13B up to 12B tokens. However, it is very likely that the model would be better in translation with more monolingual data fine-tuning. For human-written data fine-tuning, we train the model for 2 epochs (enough to see a clear convergence) and pick the best model with the lowest validation loss. For both stages, we adopt deepspeed (Rasley et al., 2020) to accelerate our training.  

### 5.3 BASELINES  

We evaluate our method against two baseline categories. First, we consider prior studies with the goal aligning with ours: leveraging LLMs for translation. Secondly, we benchmark against the current SoTA translation models. It's worth noting that this comparison isn't entirely fair due to discrepancies in training data and model architectures (e.g., 175B GPT-3.5 vs. our 7B models). Nevertheless, utilizing the same test set provides insights into our model's current standing.  

Prior Similar Work We compare our model with BigTranslate (Yang et al., 2023), which extends LLaMA-1-13B to over 100 translation directions; TIM (Zeng et al., 2023), which uses correct and incorrect examples to help LLM to learn translation; SWIE (Chen et al., 2023), which improves LLM in translation via instruction augmentation; and BayLing (Zhang et al., 2023b), which uses interactive translation instructions. Given that the same test data and evaluation metrics are utilized, we directly report BLEU and COMET from their papers (except for BigTranslate, we assess their released model using the prompt they provided).  

SoTA Models We consider the NLLB-54B model, which is the largest and best translation model released in the NLLB family (NLLB TEAM et al., 2022); and the zero-shot performance of GPT3.5-text -davinci-0 03 (GPT-3.5-D) and GPT-3.5-turbo-0301 (GPT-3.5-T). Additionally, we present the zero-shot results for GPT-4.7  

<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">de</td><td colspan="2">CS</td><td colspan="2">is</td><td colspan="2">zh</td><td colspan="2">ru</td><td colspan="2">Avg.</td></tr><tr><td>BLEUCOMET</td><td></td><td>BLEU</td><td></td><td>COMETBLEUCOMET</td><td></td><td>BLEU</td><td>COMET</td><td>BLEU</td><td>COMET</td><td>BLEUCOMET</td><td></td></tr><tr><td colspan="11">SoTAModels</td></tr><tr><td>NLLB-54B</td><td>34.50</td><td>86.45</td><td>37.60</td><td>90.15</td><td>24.15</td><td>81.76</td><td>27.38</td><td>78.91</td><td>30.96</td><td>87.92</td><td>30.92</td><td>85.04</td></tr><tr><td>GPT-3.5-D,zero-shot</td><td>31.80</td><td>85.61</td><td>31.30</td><td>88.57</td><td>15.90</td><td>76.28</td><td>38.30</td><td>85.76</td><td>27.50</td><td>86.74</td><td>28.96</td><td>84.59</td></tr><tr><td>GPT-3.5-T,zero-shot</td><td>34.40</td><td>87.00</td><td>32.92</td><td>90.17</td><td>18.74</td><td>81.04</td><td>44.90</td><td>87.00</td><td>29.90</td><td>87.60</td><td>32.17</td><td>86.56</td></tr><tr><td>GPT-4,zero-shot</td><td>35.38</td><td>87.44</td><td>34.53</td><td>90.77</td><td></td><td></td><td>43.98</td><td>87.49</td><td>30.45</td><td>88.87</td><td></td><td></td></tr><tr><td colspan="11">PriorSimilarStudies</td></tr><tr><td>TIM-BLOOMZ-7B</td><td>20.63</td><td>74.16</td><td></td><td></td><td></td><td></td><td>37.20</td><td>84.89</td><td></td><td></td><td></td><td></td></tr><tr><td>TIM-LLaMA-1-7B</td><td>25.59</td><td>82.56</td><td></td><td></td><td></td><td></td><td>19.33</td><td>75.46</td><td></td><td></td><td></td><td></td></tr><tr><td>SWIE-BLOOMZ-7B</td><td>21.83</td><td>75.17</td><td></td><td></td><td></td><td></td><td>36.88</td><td>84.53</td><td></td><td></td><td></td><td></td></tr><tr><td>SWIE-LLaMA-1-7B</td><td>27.21</td><td>82.36</td><td></td><td></td><td></td><td></td><td>31.24</td><td>80.63</td><td></td><td></td><td></td><td></td></tr><tr><td>BigTranslate-13B</td><td>21.48</td><td>78.81</td><td>20.67</td><td>80.65</td><td>2.28</td><td>35.56</td><td>28.56</td><td>81.31</td><td>17.66</td><td>78.21</td><td>18.13</td><td>70.91</td></tr><tr><td>Bayling-13B</td><td>25.62</td><td>82.69</td><td>16.43</td><td>78.22</td><td></td><td></td><td>37.92</td><td>84.62</td><td>12.77</td><td>71.01</td><td></td><td></td></tr><tr><td></td><td>OurRecipewithBackboneModel:LLaMA-2-7B</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMA-2-7B,zero-shot</td><td>19.00</td><td>76.39</td><td>16.02</td><td>79.13</td><td>1.33</td><td>43.83</td><td>16.97</td><td>71.80</td><td>16.00</td><td>73.24</td><td>13.86</td><td>68.88</td></tr><tr><td>ALMA-7B (Ours)</td><td>30.31</td><td>85.59</td><td>29.88</td><td>89.10</td><td>25.71</td><td>85.52</td><td>36.48</td><td>85.05</td><td>27.09</td><td>87.17</td><td></td><td></td></tr><tr><td>ALMA-7B-LoRA (Ours)</td><td>30.16</td><td>85.45</td><td>30.17</td><td>89.05</td><td>25.19</td><td>85.44</td><td>36.47</td><td></td><td></td><td></td><td>29.89</td><td>86.49</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>84.87</td><td>26.93</td><td>87.05</td><td>29.78</td><td>86.37</td></tr><tr><td colspan="2">OurRecipewithBackboneModel:LLaMA-2-13B LLaMA-2-13B,zer0-shot 13.69</td><td colspan="3"></td><td colspan="3">2.36</td><td colspan="2"></td><td colspan="2"></td><td></td></tr><tr><td>ALMA-13B (Ours)</td><td>31.37</td><td>75.55 85.45</td><td>0.87 31.12</td><td>68.57 89.42</td><td></td><td>38.47</td><td>30.00</td><td>79.70</td><td>0.59</td><td>63.84</td><td>9.50</td><td>65.23</td></tr><tr><td>ALMA-13B-LoRA(Ours)</td><td>31.47</td><td>85.62</td><td></td><td></td><td>26.67</td><td>85.85</td><td>39.05</td><td>85.76</td><td>28.76</td><td>87.50</td><td>31.39</td><td>86.80</td></tr><tr><td></td><td></td><td></td><td>32.38</td><td>89.79</td><td>26.68</td><td>86.08</td><td>39.84</td><td>85.96</td><td>28.96</td><td>87.53</td><td>31.87</td><td>87.00</td></tr></table></body></html>  

Table 1: The overall results in $\tt e n\to\tt x x$ .ALMA models significantly outperform all prior similar studies and are comparable to SoTA models. We categorize BLEU and COMET scores into three groups: scores that are more than 10 points below the higher value of GPT-4/GPT-3.5-T are emphasized in dark red boxes, those that are more than 5 points below are emphasized in shallow red boxes, and all other scores are emphasized in green boxes. Bold numbers represent the highest scores among ALMA models and prior similar studies.  

### 5.4 RESULTS  

We show our main results of $\tt e n\to\tt x x$ and $\mathrm{xx}\mathrm{\longrightarrow}\mathrm{en}$ respectively in Table 1 and 2. In summary, our best system (ALMA-13B-LoRA) outperforms all previous studies, NLLB-54B, and GPT-3.5-D, while it marginally underperforms compared to GPT-3.5-T and GPT-4.  

Comparing With LLaMA-2 Zero-Shot For all 10 translation directions and both 7B and 13B models, LLaMA-2 trained by our recipe significantly outperforms its original zero-shot performance. For instance, ALMA-7B achieves $+16.12$ BLEU and $+17.61$ COMET for $\tt e n\to\tt x x$ on average. It is worth noting that LLaMA-2-13B suffers from the off-target issue in $\tt e n\to\tt x x$ zero-shot translation. However, it can be substantially alleviated by few-shot in-context learning (Brown et al., 2020), but still largely lag behind our methods (e.g., over 10 BLEU and COMET when translating from English). We discuss this further in Appendix E.  

Table 2: The overall results in $_{\mathrm{XX}\to\in\ n}$ . ALMA models significantly outperform all prior similar studies and are comparable to SoTA models. The color and boldface are the same in Table 1.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">de</td><td colspan="2">CS</td><td colspan="2">is</td><td colspan="2">zh</td><td colspan="2">ru</td><td colspan="2">Avg.</td></tr><tr><td>BLEU</td><td>COMET</td><td>BLEU</td><td></td><td>COMETBLEUCOMET</td><td></td><td>BLEU</td><td>COMET</td><td>BLEU</td><td>COMET</td><td></td><td>BLEUCOMET</td></tr><tr><td colspan="10">SoTAModels</td><td></td></tr><tr><td>NLLB-54B</td><td>26.89</td><td>78.94</td><td>39.11</td><td>80.13</td><td>23.09 71.66</td><td>16.56</td><td>70.70</td><td>39.11</td><td></td><td>81.88</td><td>28.95</td><td>76.66</td></tr><tr><td>GPT-3.5-D,zero-shot</td><td>30.90</td><td>84.79</td><td>44.50</td><td>86.16</td><td>31.90</td><td>82.13</td><td>25.00</td><td>81.62</td><td>38.50</td><td>84.80</td><td>34.16</td><td>83.90</td></tr><tr><td>GPT-3.5-T,zero-shot</td><td>33.10</td><td>85.50</td><td>47.20</td><td>87.30</td><td>37.50</td><td>85.50</td><td>26.60</td><td>82.90</td><td>42.40</td><td>86.10</td><td>37.36</td><td>85.46</td></tr><tr><td>GPT-4,zero-shot</td><td>33.87</td><td>85.62</td><td>48.67</td><td>87.43</td><td></td><td></td><td>27.20</td><td>82.79</td><td>43.51</td><td>86.18</td><td></td><td></td></tr><tr><td colspan="10">PriorSimilarStudies</td><td colspan="2"></td></tr><tr><td>TIM-BLOOMZ-7B</td><td>24.31</td><td>77.65</td><td></td><td></td><td></td><td></td><td>23.42</td><td>79.50</td><td></td><td></td><td></td><td></td></tr><tr><td>TIM-LLaMA-1-7B</td><td>27.91</td><td>82.80</td><td></td><td></td><td></td><td></td><td>19.33</td><td>75.46</td><td></td><td></td><td></td><td></td></tr><tr><td>SWIE-BLOOMZ-7B</td><td>25.95</td><td>78.80</td><td></td><td></td><td></td><td></td><td>23.40</td><td>79.36</td><td></td><td></td><td></td><td></td></tr><tr><td>SWIE-LLaMA-1-7B</td><td>30.48</td><td>82.97</td><td></td><td></td><td></td><td></td><td>21.30</td><td>76.48</td><td></td><td></td><td></td><td></td></tr><tr><td>BigTranslate-13B</td><td>23.35</td><td>80.68</td><td>33.67</td><td>81.19</td><td>6.51</td><td>54.71</td><td>14.16</td><td>74.26</td><td>26.81</td><td>77.80</td><td>20.90</td><td>73.80</td></tr><tr><td>Bayling-13B</td><td>27.34</td><td>83.02</td><td>33.87</td><td>81.65</td><td></td><td></td><td>20.12</td><td>77.72</td><td>33.95</td><td>82.07</td><td></td><td></td></tr><tr><td colspan="10">OurRecipewithBackboneModel:LLaMA-2-7B</td><td colspan="2"></td><td></td></tr><tr><td>LLaMA-2-7B,zero-shot</td><td>30.42</td><td>82.74</td><td>36.56</td><td>82.42</td><td>10.98</td><td>62.79</td><td>18.19</td><td>75.00</td><td>36.02</td><td>82.84</td><td>26.43</td><td>77.16</td></tr><tr><td>ALMA-7B (Ours)</td><td>29.49</td><td>83.98</td><td>42.91</td><td>85.90</td><td>35.26</td><td>85.97</td><td>23.52</td><td>79.73</td><td>38.93</td><td>84.81</td><td>34.02</td><td>84.08</td></tr><tr><td>ALMA-7B-LoRA(Ours)</td><td>29.56</td><td>83.95</td><td>43.49</td><td>85.93</td><td>35.64</td><td>86.09</td><td>23.64</td><td>79.78</td><td>39.21</td><td>84.84</td><td>34.31</td><td>84.12</td></tr><tr><td colspan="10">OurRecipewithBackboneModel:LLaMA-2-13B</td><td colspan="2"></td></tr><tr><td>LLaMA-2-13B,zer0-shot</td><td>31.06</td><td>83.01</td><td>40.02</td><td>83.27</td><td>15.77</td><td>66.35</td><td>21.81</td><td>78.10</td><td>36.50</td><td>82.91</td><td>29.03</td><td>78.73</td></tr><tr><td>ALMA-13B (Ours)</td><td>30.73</td><td>84.42</td><td>44.68</td><td>86.29</td><td>36.46</td><td>86.30</td><td>24.65</td><td>79.90</td><td>40.37</td><td>85.09</td><td>35.38</td><td>84.40</td></tr><tr><td>ALMA-13B-LoRA (Ours)</td><td>31.14</td><td>84.56</td><td>45.28</td><td>86.47</td><td>36.95</td><td>86.42</td><td>25.46</td><td>80.21</td><td>40.27</td><td>85.27</td><td>35.82</td><td>84.59</td></tr></table></body></html>  

Compared with Prior Similar Studies ALMA significantly outperforms all prior studies. BigTranslate, which is fine-tuned on Chinese corpus and 300M parallel corpus, struggles to surpass LLaMA-2's zero-shot performance, except for $\tt e n\rightarrow z h$ . This observation also aligns with our findings that an excessive amount of parallel data may damage the model, whereas target monolingual data is helpful to translation. Both TIM and SWIE specifically target two high-resource languages, de and zh. Their performance, however, is predominantly determined by their backbone models: effectivetranslationis observedfor $_{\textrm Z\mathrm{h}}$ but is lackluster for de when using BLOOMZ, and vice versa with LLaMA-1. In contrast, ALMA is versatile, showcasing strong results across all directions.  

Compared with SoTA models Our best model (ALMA-13B-LoRA) substantially outperforms NLLB-54B and GPT-3.5-D on average. In $\tt e n{\to}\tt X X$ direction, it even outperforms GPT-3.5-T on average COMET (87.00 vs. 86.56) and has close performance when it comes to $_{\mathrm{XX}\to\mathrm{en}}$ .Notably, SoTA models typically excel with high-resource languages but falter with low-resource languages such as is. With our recipe, the performance of i s remains strong and performs the best.  

![](https://cdn-mineru.openxlab.org.cn/extract/463a5bf0-ef11-4af2-a29c-d808701c1ff7/d3fbec7c0b9b77609cc71ec9522ae98cdf34f4089b1d774d1fdb5115f96bd283.jpg)  
Figure 5: The average performance of ALMA-7B at the completion of each 1B-token fine-tuning. The scores in the figure are averaged across 10 directions  

## 6 ANALYSES  

### 6.1 HOW MUCH MONOLINGUAL DATA TO USE?  

In our main results, we present ALMA with our best settings, fine-tuned on either 20B or 12B tokens. Yet, we snapshot all ALMA models after every 1B monolingual tokens (and human-written parallel data) they have been fine-tuned with, and evaluate all their translation performance. As illustrated in Figure 5, we report the ALMA-7B's average performance across all directions after fine-tuning every 1B tokens. The test dataset remains the same, i.e., the 10 aforementioned directions. We provide detailed numeric results and similar analysis for ALMA-13B to Appendix F. Importantly, merely fine-tuning on 1B monolingual tokens, followed by fine-tuning on human-written data, yields performance comparable to NLLB-54B and GPT-3.5-D. In practice, we employ 16 MI200 GPUs with a batch size of 256 and sequence length of 512, which requires only 18 hours to complete the fine-tuning of 1B tokens and an additional hour allocated for human-written data fine-tuning. It takes around 19 hours of training to have a strong MMT model.  

Table 3: Ablation study on the effect of monolingual data and parallel data quality. The backbone model is LLaMA-2-7B. A red cross $({\pmb x})$ in the table denotes the omission of monolingual data finetuning or parallel data (indicative of zero-shot translation). A green check $(\nu)$ signifies that the model undergoes fine-tuning with monolingual data.   


<html><body><table><tr><td rowspan="2">UseMono.</td><td rowspan="2">Parallel DataQuality</td><td colspan="2">Avg.xx→en</td><td colspan="2">Avg.en→xx</td></tr><tr><td>BLEU</td><td>COMET</td><td>BLEU</td><td>COMET</td></tr><tr><td>x</td><td></td><td>26.43</td><td>77.16</td><td>13.86</td><td>68.88</td></tr><tr><td>x</td><td>Random</td><td>28.24</td><td>78.69</td><td>19.68</td><td>73.89</td></tr><tr><td></td><td>Filtered</td><td>28.39</td><td>78.94</td><td>19.56</td><td>74.35</td></tr><tr><td></td><td>HW</td><td>29.39</td><td>80.00</td><td>22.17</td><td>76.52</td></tr><tr><td></td><td></td><td>28.49</td><td>80.32</td><td>26.35</td><td>84.73</td></tr><tr><td></td><td>Random</td><td>32.47</td><td>83.02</td><td>26.98</td><td>83.15</td></tr><tr><td></td><td>Filtered</td><td>32.32</td><td>83.03</td><td>27.38</td><td>83.98</td></tr><tr><td></td><td>HW</td><td>34.02</td><td>84.08</td><td>29.89</td><td>86.49</td></tr></table></body></html>  

### 6.2 THE EFFECT OF MONOLINGUAL DATA AND PARALLEL DATA QUALITY  

To scrutinize the impact of monolingual data, we juxtapose LLaMA-2-7B models fine-tuned with and without monolingual data (20B tokens), while keeping the same parallel data. Furthermore, to evaluate the impact of parallel data quality, we introduce three distinct parallel datasets for stage 2 fine-tuning. The first dataset is the human-written data (HW) utilized in prior experiments. The second is the filtered data (Filtered) referenced in Section 3.1. Lastly, we employ a randomly selected dataset (Random) sourced from the comprehensive WMT data. We anticipate the quality hierarchy as HW, followed by Filtered, and lastly, Random. For both Filtered and Random, each translation direction has 10K parallel data, aligning the total training dataset size with HW. We show the ablation results in Table 3. Using the LLaMA-2-7B as our foundational model, it's evident that with the same parallel data, incorporation of monolingual data largely enhances translation results, e.g., an increase from 74.35 to 83.98 in $\tt e n\to\tt X X$ COMET scores when training on the same Filtered data. Moreover, regardless of the monolingual data's presence, models fine-tuned with higher-quality data exhibit better performance. Both monolingual and human-written data emerge as critical factors in improving translation. Detailed results for each language pair are deferred to the Appendix G.  

### 6.3  OTHER ANALYSES  

We also explore additional in-depth analyses and elaborate on them in the appendix: 1) The impact of the volume and domain of human-written data on translation performance is explored in Appendix H; 2) A comparison between stage 2 fine-tuning (parallel data fine-tuning) and in-context few-shot learning can be found in Appendix I; 3) An evaluation of the zero-shot cross-lingual capabilities of LLaMA-2 after stage 1 fine-tuning on other tasks is presented in Appendix J.  

## 7 CONCLUSION  

In this paper, we show that LLMs do not require as extensive a collection of parallel data as traditional translation models do. Subsequently, we introduce a novel training recipe for decoder-only LLMs in translation, resulting in strong translation models, ALMA. When using our LLaMA-2 as our foundational model, ALMA exceeds the zero-shot translation performance of LLaMA-2 by more than 12 BLEU and COMET scores across 10 directions on average. Moreover, ALMA models surpass all preceding studies and even outperform NLLB-54B and GPT-3.5-D.  

# ACKNOWLEDGMENTS  

We extend our gratitude to Hieu Hoang, Marcin Junczys-Dowmunt, Yunmo Chen, Steven Tan, Huda Khayrallah, Thamme Gowda, Vikas Raunak, Matt Post, Anoop Kunchukuttan, Roman Grundkiewicz, Tom Kocmi, Kenton Murray and Arul Menezes for their insightful and valuable suggestions.  

REFERENCES   
Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. arXiv preprint arXiv:1903.00089, 2019.   
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.   
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.   
Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. Improving translation faithfulness of large language models via augmenting instructions. arXiv preprint arXiv:2308.12674,2023.   
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Marten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.0231l, 2022.   
Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053, 2018.   
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ot, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL ht tps : / /aclanthology.org/2020.acl-main.747.   
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 46-68, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https : //aclanthology.org/2022.wmt-1.2.   
Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128-135, 1999.   
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https : //doi.org/10.5281/ zenodo. 5371628.   
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.   
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210, 2023.  

Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https: / /openreview.net /forum? id $=$ nZeVKeeFYf9.  

Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745, 2023.   
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521-3526, 2017.   
Julia Kreutzer, Isaac Caswell Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzi-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoit Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias Muller, Andre Muller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Cabuk Balli, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50-72, 2022. doi: 10.1162/ tacl-a-00447. URL https: //aclanthology.org/2022.tacl-1.4.   
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Chen, and Jiajun Chen. Eliciting the translation ability of large language models via multilingual finetuning with translation instructions. arXiv preprint arXiv:2305.15083, 2023.   
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti hosale,JingfeiDu,et al. Few-sht larning withmultilingual language models. arXiv preprint arXiv:2112.10668, 2021.   
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950-1965, 2022.   
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation.Transactions of the Association for Computational Linguistics, 8:726-742, 2020.doi: 10.1162/tacl-a-00343. URL https: //aclanthology.org/2020.tacl-1.47.   
Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj Goswami, Philipp Koehn, Angela Fan, and Francisco Guzman. Small data, big impact: Leveraging minimal data for effective machine translation. In Proceedings of the 6lst Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2740-2756, 2023.   
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. Peft: State-of-the-art parameter-effcient fine-tuning methods. https: //github.com/ huggingface/peft,2022.   
MosaicML. Introducing mpt-7b: A new standard for open-source commercially usable ms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.   
Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. Fewshot fine-tuning vs.in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938,2023.   
Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. LSDSem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pp. 46-51, Valencia, Spain,  

April 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-0906. URL https: //aclanthology.0rg/W17-0906.  

Marta R NLLB TEAM, Costa-jussa, James Cross, Onur Celebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.   
OpenAI. Gpt-4 technical report, 2023.   
Pedro Javier Ortiz Su'arez, Benoit Sagot, and Laurent Romary. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pp. 9 - 16, Mannheim, 2019. Leibniz-Institut f'ur Deutsche Sprache. doi: 10.14618/ids-pub-9021. URL http: //nbn-resolving.de/urn:nbn:de:bsz:mh39-90215.   
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Procedings of the 4oth annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.   
Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186-191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https: //aclanthology.org/W18-6319.   
Colin Raffel, Noam Shazeer, Adam oberts, Katin L,Sharan Narang, Michal Matna, Yaqi Zhou, Wei Li, and Peter JLiu. Exploring the lmits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67, 2020.   
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505-3506, 2020.   
Vikas Raunak, Arul Menezes, and Hany Awadalla. Dissecting in-context learning of translations in GPT-3. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 866-872, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.61. URL https : / / aclanthology.org/2023.findings-emnlp.61.   
Ricardo Rei, Jose G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578-585, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https: //aclanthology.org/2022. wmt-1.52.   
Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8: 264-280, 2020. doi: 10.1162/tacl-a_00313. URL https : //aclanthology .org/2020. tac1-1.18.   
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana li, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Gallé, et al. Bloom: A 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.   
Weiting Tan, Haoran Xu, Lingfeng Shen, Shuyue Stella Li, Kenton Murray, Philipp Koehn, Benjamin Van Durme, and Yunmo Chen. Narrowing the gap between zero- and few-shot machine translation by matching styles, 2023.   
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. U12: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations, 2022a.  

Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q Tran, David R So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending scaling laws With $0.1\%$ extra compute. arXiv preprint arXiv:2210.11399, 2022b.  

Alexey Tikhonov and Max Ryabinin. It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 3534-3546, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.310. URL https : //aclanthology.org/2021.findings-acl.310.   
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.   
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.   
Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffl. What language model architecture and pretraining objective works best for zero-shot generalization? In International Conference on Machine Learning, Pp. 22964-22984. PMLR, 2022.   
Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al. Polylm: An open source polyglot large language model. arXiv preprint arXiv:2307.06018, 2023.   
Haoran Xu, Benjamin Van Durme, and Kenton Murray. BERT, mBERT, or BiBERT? a study on contextualized embeddings for neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 6663-6675, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.534. URL https : / /aclanthology . org/2021. emnlp-main.534.   
Haoran Xu, Jean Maillard, and Vedanuj Goswami. Language-aware multilingual machine translation with self-supervised learning. In Findings of the Association for Computational Linguistics: EACL 2023, pp. 526-539, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-eacl.38. URL https : / /aclanthology . org/ 2023.findings-eacl.38.   
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages.  arXiv preprint arXiv:2305.18098, 2023.   
Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. Tim: Teaching large language models to translate with comparison. arXiv preprint arXiv:2307.04408, 2023.   
Mike Zhang and Antonio Toral. The effect of translationese in machine translation test sets. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pp. 73-81, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/ vl/Wi9-5208. URL https: //aclanthology.org/W19-5208.   
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Effcient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023a.   
Shaolei Zhang, Qingkai Fang,Zhuocheng Zhang,Zhengrui Ma, YanZhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, et al. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. arXiv preprint arXiv:2306.10968,2023b.   
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.   
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675, 2023a.   
Wenhao Zhu, Yunzhe Ly, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Extrapolating large language models to non-english by aligning languages. arXiv preprint arXiv:2308.04948, 2023b.  

# A COMPARING LLM TRAINING OBJECTIVES FOR MACHINE TRANSLATION  

We evaluate three potential training objectives for decoder-only LLM in machine translation.  

Causal Language Modeling (CLM) We first consider a standard language modeling loss that predicts the next token based on all prior tokens.  

Prefix Language Modeling (Prefix LM) For decoder-only models, a prefix can be defined with a non-causal attention mask. Analogous to standard language modeling, the model predicts each token outside the prefix based on previous tokens. In the context of machine translation, the provided prompt serves as the prefix, as depicted in Figure 2.  

Mixture-of-Denoisers (MoD) The UL2 model (Tay et al., 2022a) introduces a unified approach to masking methods, utilizing a mixture-of-denoisers (MoD) strategy, which has also been implemented in the fine-tuning of PaLM (Tay et al., 2022b). This strategy is grounded in three objectives:  

· Regular Denoising: In this approach, noise is sampled in spans and replaced with sentinel tokens, aligning with the standard span corruption technique delineated in Raffel et al. (2020). The parameters set for this objective include a mean of 3 and a corruption rate of 15 · Extreme Denoising: This method amplifies the noise to a comparatively 'extreme’ level, characterized by a mean length of 32 and a corruption rate reaching up to 25 · Sequential Denoising: This is known as the Prefix LM objective previously mentioned.  

In our training process, we allocate a $25\%$ probability each for both regular and extreme denoising, and a $50\%$ probability for sequential denoising.  

We employ the MPT-7B as our backbone model. Our investigation considers four distinct training data sizes: 0 (zero-shot), 100K, 1M, and 5M, with translation directed from Russian to English. We use the parallel dataset previously described in Section 3.1. For each data size, the MPT-7B is fine-tuned using the corresponding training objective, noting that all trainings utilize full-weight fine-tuning.  

The results of the comparison between training objectives can be viewed in Figure 6. Although three objectives end up with similar performance under 5M training data, both prefix LM and MoD markedly lag behind CLM under limited parallel data (100K or 1M). Surprisingly, with 100K, models fine-tuned using prefix LM and MoD even underperform their zero-shot performance. Conversely, CLM demonstrates a healthy improvement as the amount of parallel data increases. Consequently, we adopt CLM as our primary training objective for machine translation.  

![](https://cdn-mineru.openxlab.org.cn/extract/463a5bf0-ef11-4af2-a29c-d808701c1ff7/8b08b3f82494e53abdbc9764fd88cbb8541dd8690be9b43a5c3cda18033d5107.jpg)  
Figure 6: The comparison of translation performance across various training objectives and parallel data sizes is depicted. or datasets of 100K and 1M, both prefix LM and MoD lag behind CLM and even undeperform the zero-shot performance. Notably, only CLM demonstrates a healthy improvement as the volume of training data increases.  

# BFULL RESULTS OF ZERO-SHOT EVALUATION  

In Section 2.2, we present the average zero-shot translation performance of recently released LLMs. Detailed results for each translation direction can be found in Table 4.   
Table 4: The detailed results of LLM zero-shot performance in Figure 3   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">de</td><td colspan="2">CS</td><td colspan="2">is</td><td colspan="2">zh</td><td colspan="2">ru</td><td colspan="2">Avg.</td></tr><tr><td>BLEUCOMETBLEU</td><td></td><td></td><td>COMETBLEU</td><td></td><td></td><td></td><td></td><td>COMETBLEUCOMETBLEUCOMET</td><td></td><td>BLEUCOMET</td><td></td></tr><tr><td></td><td colspan="10">TranslatingfromEnglish(en→xx)</td></tr><tr><td>OPT-7B</td><td>9.79</td><td>65.74</td><td>2.95</td><td>51.55</td><td>1.42</td><td>45.66</td><td>1.59</td><td>48.84</td><td>1.31</td><td>41.57</td><td>3.41</td><td>50.67</td></tr><tr><td>BLOOM-7B</td><td>7.31</td><td>62.21</td><td>3.09</td><td>56.22</td><td>1.49</td><td>49.97</td><td>20.41</td><td>74.03</td><td>5.89</td><td>56.55</td><td>7.64</td><td>59.80</td></tr><tr><td>Faclon-7B</td><td>19.23</td><td>77.30</td><td>5.86</td><td>57.04</td><td>1.69</td><td>37.53</td><td>26.90</td><td>79.28</td><td>4.61</td><td>53.55</td><td>11.66</td><td>60.94</td></tr><tr><td>LLaMA-1-7B</td><td>21.00</td><td>79.50</td><td>16.31</td><td>78.16</td><td>2.42</td><td>34.92</td><td>15.63</td><td>68.03</td><td>17.61</td><td>76.73</td><td>14.59</td><td>67.47</td></tr><tr><td>MPT-7B</td><td>20.91</td><td>78.56</td><td>11.95</td><td>69.80</td><td>3.21</td><td>41.71</td><td>25.41</td><td>80.20</td><td>13.99</td><td>72.43</td><td>15.09</td><td>68.54</td></tr><tr><td>LLaMA-2-7B</td><td>19.00</td><td>76.39</td><td>16.02</td><td>79.13</td><td>1.33</td><td>43.83</td><td>16.97</td><td>71.80</td><td>16.00</td><td>73.24</td><td>13.86</td><td>68.88</td></tr><tr><td colspan="14">TranslatingtoEnglish(xx→en)</td></tr><tr><td>OPT-7B</td><td>24.43</td><td>78.37</td><td>14.82</td><td>66.86</td><td>3.13</td><td>52.63</td><td>3.35</td><td>54.34</td><td>4.47</td><td>53.30</td><td>10.04</td><td>61.10</td></tr><tr><td>BLOOM-7B</td><td>22.06</td><td>74.10</td><td>6.06</td><td>55.18</td><td>2.14</td><td>48.70</td><td>13.66</td><td>74.62</td><td>20.06</td><td>69.27</td><td>12.80</td><td>64.37</td></tr><tr><td>Faclon-7B</td><td>29.21</td><td>82.02</td><td>20.06</td><td>71.15</td><td>4.29</td><td>52.53</td><td>19.45</td><td>76.68</td><td>19.50</td><td>73.19</td><td>18.50</td><td>71.11</td></tr><tr><td>LLaMA-1-7B</td><td>29.14</td><td>81.90</td><td>32.93</td><td>81.18</td><td>6.78</td><td>58.15</td><td>13.29</td><td>72.09</td><td>32.93</td><td>81.71</td><td>23.01</td><td>75.01</td></tr><tr><td>MPT-7B</td><td>29.32</td><td>81.80</td><td>27.45</td><td>76.12</td><td>12.44</td><td>62.76</td><td>19.72</td><td>77.25</td><td>30.55</td><td>79.21</td><td>23.90</td><td>75.43</td></tr><tr><td>LLaMA-2-7B</td><td>30.42</td><td>82.74</td><td>36.56</td><td>82.42</td><td>10.98</td><td>62.79</td><td>18.19</td><td>75.00</td><td>36.02</td><td>82.84</td><td>26.43</td><td>77.16</td></tr></table></body></html>  

# C TRAINING DETAILS  

We fine-tune the backbone model using a warm-up ratio of 0.01, a maximum sequence length of 512 tokens, and a weight decay of 0.01. The test data from WMT'21 serves as our development set. The training spans 3 epochs (for MPT-7B as detailed in Section 3, and 2 epochs for LLaMA-2 human-written data fine-tuning). The best model is selected based on the lowest validation loss, with validation performed every $10\%$ of the total training progress. We utilize 16 MI200 GPUs for training; each GPU manages 4 batches and has a gradient accumulation step of 4, yielding an effective batch size of 256. The peak learning rate is set at 2e-5 , with an inverse square learning rate decay to O. The training operates under $\mathtt{f p16}$ precision, facilitated by deepspeed Rasley et al. (2020), employing ZeRO stage 2.  

# D DATA INFORMATION  

## D.1 SAMPLING RATIO FOR MONOLINGUAL DATA  

In Table 5, we observe a substantial imbalance in the volume of monolingual data available for different languages, denoted by their respective word counts\*. Specifically, the English language dataset contains 523.9B words, vastly outnumbering other languages, such as Icelandic, which contains 0.3B words. Utilizing an unmodified concatenation and shuffing approach for this data would disproportionately prioritize English, undermining our objective of enhancing the model's proficiency in non-English languages. To address this, we straightforwardly set the sampling ratio for English as $\textstyle P(l=\ e\mathrm{n})={\frac{1}{6}}$ ,therebyensuringa balanced learmingemphasis.Theremaining $\frac{5}{6}$ oftheprobabity allocation employs temperature sampling, as suggested by Aharoni et al. (2019), a technique prevalently adopted in the processing of unbalanced multilingual machine translation. Consequently, the process of selecting a monolingual example from language $l$ adheres to the following distribution:  

$$
P(l)\propto(\frac{D_{l}}{\sum_{l^{\prime}\in L}D_{l^{\prime}}})^{\frac{1}{T}}\quad\mathrm{s.t.}\quad\sum_{l^{\prime}\in L}P(l^{\prime})=\frac{5}{6}
$$  

where $D_{l}$ is the amount of the data in language $l,T$ is the temperature, and $L$ is the set of all languages except for English. The temperature we use is 6.  

<html><body><table><tr><td rowspan="2"></td><td colspan="4">ParallelData</td><td colspan="2">MonolingualData</td></tr><tr><td>Train</td><td>Development</td><td>Test (from English)</td><td>Test (toEnglish)</td><td>#Words</td><td>SamplingRatio</td></tr><tr><td>German (de)</td><td>14211</td><td>1002</td><td>2037</td><td>1984</td><td>73.8B</td><td>20%</td></tr><tr><td>Czech (cs)</td><td>12076</td><td>1002</td><td>2037</td><td>1448</td><td>9.7B</td><td>14%</td></tr><tr><td>Icelandic(is)</td><td>2009</td><td></td><td>1000</td><td>1000</td><td>0.3B</td><td>8%</td></tr><tr><td>Chinese (zh)</td><td>15406</td><td>1002</td><td>2037</td><td>1875</td><td>44.4B</td><td>19%</td></tr><tr><td>Russian (ru)</td><td>15000</td><td>1002</td><td>2037</td><td>2016</td><td>78.0B</td><td>22%</td></tr><tr><td>English (en)</td><td></td><td></td><td></td><td></td><td>523.9B</td><td>17%</td></tr></table></body></html>  

Table 5: The statistics for the data we utilize for the monolingual data fine-tuning and human-written data fine-tuning.  

## D.2 DATA STATISTICS  

We show data statistics in Table 5. The training parallel data is sourced from the WMT'17 to WMT'20. The development data was acquired from WMT'21, and the test data was derived from WMT'22, with the exception of the Icelandic dataset, which was procured from WMT'21. This means, Icelandic does not have development dataset. Additionally, the monolingual data was extracted from the Oscar dataset.  

# EOFF-TARGET ISSUE FOR LLAMA-2-13B  

In the zero-shot scenario, the performance of LLaMA-2-13 is reasonable for translations into English. However, we identify a significant off-target issue with LLaMA-2-13B when translating from English to other languages. This issue is highlighted in Table 6 using a red highlighted box .An illustrative example of the off-target issue is provided below:  

TranslatethisfromEnglishtoRussian:   
English: Plug the wall charger (not included) to a power outlet, and then connect your eReader to thewall charger.   
Russian: Comment: I'm voting to close this question as off-topic because it is not aboutprogramming.  

<html><body><table><tr><td>Models</td><td colspan="2">de</td><td colspan="2">CS</td><td colspan="2">is</td><td colspan="2">zh COMETBLEUCOMETBLEU</td><td colspan="2">ru</td><td colspan="2">Avg.</td></tr><tr><td></td><td colspan="9">BLEUCOMETBLEU COMETBLEU</td><td>COMETBLEUCOMET</td><td></td><td></td></tr><tr><td colspan="10">BackboneModel:LLaMA-2-13B,TranslatingfromEnglish(en→xx)</td><td></td><td></td><td></td></tr><tr><td>zero-shot</td><td>13.69</td><td>75.55</td><td>0.87</td><td>68.57</td><td>2.36</td><td>38.47</td><td>30.00</td><td>79.70</td><td>0.59</td><td>63.84</td><td></td><td>9.50 65.23</td></tr><tr><td>Prompt inTarget Language</td><td>25.91</td><td>81.88</td><td>20.80</td><td>81.82</td><td>2.05</td><td>40.80</td><td>31.82</td><td>82.08</td><td>22.66</td><td>83.29</td><td>20.65</td><td>73.97</td></tr><tr><td>Filtered1-shot</td><td>25.71</td><td>80.85</td><td>20.77</td><td>81.30</td><td>2.78</td><td>42.97</td><td>31.70</td><td>82.12</td><td>22.32</td><td>83.03</td><td>20.66</td><td>74.05</td></tr><tr><td>Filtered5-shot</td><td>26.32</td><td>81.67</td><td>20.89</td><td>81.45</td><td>2.78</td><td>42.62</td><td>32.01</td><td>82.02</td><td>23.26</td><td>83.28</td><td>21.05</td><td>74.21</td></tr><tr><td>HW5-shot</td><td>26.33</td><td>82.63</td><td>21.87</td><td>82.66</td><td>3.04</td><td>41.93</td><td>30.73</td><td>82.65</td><td>22.77</td><td>84.21</td><td>20.95</td><td>74.82</td></tr><tr><td>ALMA-13B-LoRA(ours)</td><td>31.47 85.62</td><td></td><td>32.38</td><td>89.79</td><td>26.68</td><td>86.08</td><td>39.84</td><td>85.96</td><td>28.96</td><td>87.53</td><td>31.87</td><td>87.00</td></tr><tr><td colspan="9">BackboneModel:LLaMA-2-13B,TranslatingtoEnglish (xx→→en)</td><td colspan="3"></td><td></td></tr><tr><td>zero-shot</td><td>31.06</td><td>83.01</td><td>40.02</td><td>83.27</td><td>15.77</td><td>66.35</td><td>21.81</td><td>78.10</td><td>36.50</td><td>82.91</td><td>29.03</td><td>78.73</td></tr><tr><td>Prompt inTargetLanguage</td><td>e31.06</td><td>83.01</td><td>40.02</td><td>83.27</td><td>15.77</td><td>66.35</td><td>21.81</td><td>78.10</td><td>36.50</td><td>82.91</td><td>29.03</td><td>78.73</td></tr><tr><td>Filtered1-shot</td><td>30.75</td><td>82.91</td><td>39.47</td><td>82.90</td><td>13.71</td><td>64.73</td><td>21.00</td><td>78.35</td><td>37.13</td><td>82.85</td><td>28.41</td><td>78.35</td></tr><tr><td>Filtered5-shot</td><td>30.92</td><td>83.41</td><td>41.44</td><td>83.81</td><td>17.85</td><td>68.22</td><td>19.86</td><td>78.15</td><td>36.46</td><td>82.26</td><td>29.31</td><td>79.17</td></tr><tr><td>HW5-shot</td><td>31.52</td><td>83.57</td><td>42.10</td><td>84.69</td><td>17.88</td><td>69.93</td><td>23.26</td><td>79.36</td><td>37.42</td><td>84.12</td><td>30.44</td><td>80.33</td></tr><tr><td>ALMA-13B-LoRA(ours)</td><td>31.14</td><td>84.56</td><td>45.28</td><td>86.47</td><td>36.95</td><td>86.42</td><td>25.46</td><td>80.21</td><td>40.27</td><td>85.27</td><td>35.82</td><td>84.59</td></tr></table></body></html>  

Table 6: We demonstrate the off-target problem encountered during zero-shot translation from English to other languages using the LLaMA-2-13B model. Instances of this issue are highlighted within red boxes . Implementing prompts in the target languages and incorporating few-shot learning can markedly alleviate this issue. It is pertinent to note that the quality of the shots also infuences the final outcomes.  

Expectedly, the model should produce translations in Russian. Yet, LLaMA-2-13B outputs “I'm voting to .", indicating a misinterpretation of the task, potentially linked to its pre-training phase. We address this off-target behavior through two methods.  

Prompt in the Target Language  One approach is to utilize prompts in the target language (Raunak et al., 2023). For instance, when translating from English to Chinese, the preferred prompt is:"将其从英文翻译成中文：\n英文：<source sentence>\n中文：”as opposed to"Translate this from English to Chinese:\nEnglish: $<$ source sentence>\nChinese:". Employing this technique markedly enhances the zero-shot performance of LLaMA-2-13B. Specifically, the BLEU score escalates from 0.87 to 20.80 for $\e\mathrm{en}\longrightarrow\subset S$ , and from 0.59 to 22.66 for $\tt e n\to\tt r u$  

In-Context Few-Shot Learning  Employing in-context few-shot learning by including several examples within the prompt has proven effective. We investigate both 1-shot and 5-shot learning scenarios. As delineated in Section I, we utilize two sets of examples: Filtered, extracted from the WMT training data, and another set randomly chosen from human-written data, termed HW. Table 6 demonstrates that both 1-shot and 5-shot configurations effectively counteract the off-target challenges. Few-shot learning exhibits performance comparable to the strategy of using prompts in the target language. Moreover, echoing observations from Section I, examples of human-written quality outperform those from the Filtered set.  

Nevertheless, both strategies trail behind our proposed solution by a margin of approximately 5 BLEU and COMET points during translations into English, and by over 10 BLEU and COMET points in translations originating from English.  

<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">de</td><td colspan="2">CS</td><td colspan="2">is</td><td colspan="2">zh</td><td colspan="2">ru</td><td colspan="2">Avg.</td></tr><tr><td colspan="3">BLEU COMET BLEU</td><td colspan="2">COMET BLEU</td><td colspan="2">JCOMET BLEU</td><td colspan="2">COMET BLEU</td><td colspan="2">COMET BLEU COMET</td></tr><tr><td></td><td rowspan="20"></td><td colspan="2"></td><td colspan="2">Translatingfrom</td><td colspan="2">English (en→xx)</td><td colspan="2"></td><td colspan="2"></td></tr><tr><td>NLLB-54B</td><td>34.50 86.45</td><td>37.60</td><td>90.15</td><td>24.15</td><td>81.76</td><td>27.38</td><td>78.91 30.96</td><td>87.92</td><td>30.92</td><td>85.04</td></tr><tr><td>GPT-3.5-D</td><td>31.80 85.61</td><td>31.30</td><td>88.57</td><td>15.90</td><td>76.28</td><td>38.30</td><td>85.76</td><td>27.50</td><td>86.74 28.96</td><td>84.59</td></tr><tr><td>1B</td><td>28.02 84.24</td><td>25.40</td><td>87.34</td><td>21.35</td><td>83.05</td><td>35.54</td><td>84.80</td><td>25.48</td><td>86.31</td><td>27.16 85.15</td></tr><tr><td>2B</td><td>29.68</td><td>85.04 27.18</td><td>88.00</td><td>23.49</td><td>84.30</td><td>36.12</td><td>85.10</td><td>26.17</td><td>86.56</td><td>28.53 85.80</td></tr><tr><td>3B</td><td>29.25 84.82</td><td>28.26</td><td>88.31</td><td>23.60</td><td>84.62</td><td>37.06</td><td>85.27</td><td>26.38</td><td>86.72 28.91</td><td>85.95</td></tr><tr><td>4B</td><td>29.61 85.24</td><td>28.27</td><td>88.29</td><td>23.90</td><td>84.42</td><td>37.26</td><td>85.40</td><td>27.02</td><td>86.91 29.21</td><td>86.05</td></tr><tr><td>5B</td><td>29.52</td><td>85.04 28.29</td><td>88.43</td><td>23.85</td><td>84.58</td><td>37.19</td><td>85.42</td><td>26.50</td><td>86.85</td><td>29.07 86.06</td></tr><tr><td>6B</td><td>29.49</td><td>85.01 28.45</td><td>88.43</td><td>24.31</td><td>84.63</td><td>37.16</td><td>85.45</td><td>26.92</td><td>86.90</td><td>29.27 86.08</td></tr><tr><td>7B</td><td>29.46</td><td>85.11 28.25</td><td>88.45</td><td>24.27</td><td>84.78</td><td>37.26</td><td>85.43</td><td>26.80</td><td>86.95</td><td>29.21 86.14</td></tr><tr><td>8B 9B</td><td>29.31 29.36</td><td>84.92 27.93 84.85</td><td>88.33</td><td>23.84</td><td>84.74</td><td>37.19</td><td>85.30</td><td>26.27</td><td>86.76</td><td>28.91 86.01</td></tr><tr><td>10B</td><td></td><td>27.86 29.18</td><td>88.11</td><td>24.43</td><td>84.60</td><td>37.15</td><td>85.30</td><td>26.41</td><td>86.52</td><td>29.04 85.88</td></tr><tr><td></td><td>29.47 84.82</td><td></td><td>88.41</td><td>25.59</td><td>85.09</td><td>37.41</td><td>85.31</td><td>27.71</td><td>86.98</td><td>29.87 86.12</td></tr><tr><td>11B</td><td>29.55</td><td>85.14 28.94</td><td>88.41</td><td>25.38</td><td>85.18</td><td>37.60</td><td>85.43</td><td>27.32</td><td>86.96</td><td>29.76 86.22</td></tr><tr><td>12B</td><td>29.71 85.02</td><td>28.78</td><td>88.49</td><td>25.10</td><td>84.98</td><td>37.75</td><td>85.47</td><td>27.64</td><td>86.99 29.80</td><td>86.19</td></tr><tr><td>12B,beam size=5 31.37</td><td>85.45</td><td>31.12</td><td>89.42</td><td>26.67</td><td>85.85</td><td>39.05 →en)</td><td>85.76</td><td>28.76</td><td>87.50</td><td>31.39 86.80</td></tr><tr><td colspan="11">Translating to English(xx-</td></tr><tr><td>NLLB-54B</td><td>26.89</td><td>78.94</td><td>39.11</td><td>80.13 23.09</td><td>71.66</td><td>16.56</td><td>70.70</td><td>39.11</td><td>81.88</td><td>28.95</td><td>76.66</td></tr><tr><td>GPT-3.5-D</td><td>30.90</td><td>84.79</td><td>44.50</td><td>86.16</td><td>31.90</td><td>82.13 25.00</td><td>81.62</td><td>38.50</td><td>84.80</td><td>34.16</td><td>83.90</td></tr><tr><td>1B</td><td>30.66</td><td>84.36</td><td>43.71</td><td>86.06</td><td>34.96</td><td>85.54</td><td>23.22 79.88</td><td>38.87</td><td>84.88</td><td>34.28</td><td>84.14</td></tr><tr><td>2B</td><td>30.26</td><td>84.32</td><td>42.46</td><td>85.86</td><td>34.30</td><td>85.63</td><td>22.66 79.88</td><td>37.30</td><td>84.70</td><td>33.40</td><td>84.08</td></tr><tr><td>3B</td><td>30.14</td><td>84.27</td><td>42.22</td><td>85.98</td><td>34.55</td><td>85.79</td><td>22.56 79.64</td><td>38.31</td><td>84.77</td><td>33.56</td><td>84.09</td></tr><tr><td>4B</td><td>30.14</td><td>84.38</td><td>42.84</td><td>86.03</td><td>34.86</td><td>85.75</td><td>23.18 79.95</td><td>38.45</td><td>84.90</td><td>33.89</td><td>84.20</td></tr><tr><td>5B</td><td>30.20</td><td>84.42</td><td>42.89</td><td>86.14</td><td>34.52</td><td>85.87</td><td>23.32 80.07</td><td>38.07</td><td>85.02</td><td>33.80</td><td>84.30</td></tr><tr><td>6B</td><td>30.22</td><td>84.35</td><td>42.85</td><td>86.22</td><td>34.75</td><td>85.96</td><td>23.40 79.94</td><td>38.25</td><td>84.90</td><td>33.89</td><td>84.27</td></tr><tr><td>7B</td><td>30.37</td><td>84.36</td><td>42.77</td><td>86.11</td><td>35.86</td><td>86.12</td><td>22.76 79.86</td><td>37.95</td><td>84.90</td><td>33.94</td><td>84.27</td></tr><tr><td>8B</td><td>30.16</td><td>84.33</td><td>43.25</td><td>85.98</td><td>34.85</td><td>85.83</td><td>22.90 79.82</td><td>37.42</td><td>84.84</td><td>33.72</td><td>84.16</td></tr><tr><td>9B</td><td>30.11</td><td>84.30</td><td>42.90</td><td>85.97</td><td>35.21</td><td>85.85</td><td>22.50 79.52</td><td>37.74</td><td>84.92</td><td>33.69</td><td>84.11</td></tr><tr><td>10B</td><td>29.93</td><td>84.32</td><td>43.02</td><td>86.10</td><td>35.98</td><td>86.09</td><td>22.54</td><td>79.77 37.86</td><td>84.88</td><td>33.87</td><td>84.23</td></tr><tr><td>11B</td><td>30.57</td><td>84.33</td><td>43.42</td><td>86.11</td><td>36.19</td><td>86.14</td><td>22.98</td><td>79.84 38.40</td><td>84.88</td><td>34.31</td><td>84.26</td></tr><tr><td>12B</td><td>30.40</td><td>84.30</td><td>43.16</td><td>86.17</td><td>35.73</td><td>86.19</td><td>23.89</td><td>80.17 38.49</td><td>84.89</td><td>34.33</td><td>84.34</td></tr><tr><td>12B,beamsize=5</td><td>30.73</td><td>84.42</td><td>44.68</td><td>86.29</td><td>36.46</td><td>86.30</td><td>24.65</td><td>79.90 40.37</td><td>85.09</td><td>35.38</td><td>84.40</td></tr></table></body></html>

Table 7: The comprehensive numeric results for LLaMA-2-13B fine-tuned by every 1B monolingual tokens followed by human-written data fine-tuning.  

# F NUMERIC RESULTS FOR MODELS FINE-TUNED WITH EVERY 1B TOKENS  

In Table 7 and 8, results for LLaMA-2-13B and LLaMA-2-7B are presented. Both models were fine-tuned at every 1B-token interval (comprising six languages) before subsequent fine-tuning with human-written parallel data. Full-weight fine-tuning was employed to ensure a consistent comparison. During inference, the 7B models utilized a beam search of size 5, while the 13B models adopted a greedy search strategy. For 13B models, we only utilize a beam size 5 for the final models we reported in the main manuscript (Table 1 and 2).  

The data from these tables highlight that fine-tuning only 1B tokens, followed by human-written data fine-tuning, is adequate to compete with or even outperform the state-of-the-art (SoTA) models.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">de</td><td colspan="2">CS</td><td colspan="2">is</td><td colspan="2">zh</td><td colspan="2">ru</td><td colspan="2">Avg.</td></tr><tr><td>BLEU</td><td>COMET</td><td>BLEU</td><td>COMET</td><td>BLEU</td><td></td><td>COMETBLEUCOMET</td><td></td><td>BLEU</td><td>COMET</td><td>BLEU COMET</td></tr><tr><td colspan="10">Translating fromEnglish(en→xx)</td></tr><tr><td>NLLB-54B</td><td>34.50</td><td>86.45</td><td>37.60</td><td>90.15 24.15</td><td></td><td>81.76</td><td>27.38 78.91</td><td>30.96</td><td>87.92</td><td>30.92</td><td>85.04</td></tr><tr><td>GPT-3.5-D</td><td>31.80</td><td>85.61</td><td>31.30</td><td>88.57</td><td>15.90</td><td>76.28</td><td>38.30 85.76</td><td>27.50</td><td>86.74</td><td>28.96</td><td>84.59</td></tr><tr><td>1B</td><td>28.40</td><td>84.45</td><td>26.99 87.91</td><td>20.64</td><td>83.22</td><td></td><td>35.09 84.41</td><td>25.10</td><td>86.33</td><td>27.24</td><td>85.26</td></tr><tr><td>2B</td><td>28.96</td><td>84.62</td><td>28.05 88.34</td><td>22.23</td><td></td><td>84.44 34.39</td><td>84.08</td><td>26.02</td><td>86.37</td><td>27.93</td><td>85.57</td></tr><tr><td>3B</td><td>29.10</td><td>84.66</td><td>28.68</td><td>88.46 23.23</td><td></td><td>84.74</td><td>35.50 84.40</td><td>26.35</td><td>86.75</td><td>28.57</td><td>85.80</td></tr><tr><td>4B</td><td>29.02</td><td>84.75</td><td>28.14</td><td>88.53 23.78</td><td></td><td>84.94</td><td>35.51 84.65</td><td>26.22</td><td>86.68</td><td>28.53</td><td>85.91</td></tr><tr><td>5B</td><td>29.34</td><td>84.89</td><td>29.00</td><td>88.82 24.16</td><td></td><td>84.76</td><td>35.82 84.71</td><td>26.21</td><td>86.74</td><td>28.91</td><td>85.98</td></tr><tr><td>6B</td><td>28.78</td><td>84.61</td><td>28.31</td><td>88.56</td><td>23.85</td><td>84.82</td><td>34.96 84.43</td><td>26.03</td><td>86.67</td><td>28.39</td><td>85.82</td></tr><tr><td>7B</td><td>28.72</td><td>84.83</td><td>27.72</td><td>88.49</td><td>23.88</td><td>84.86</td><td>35.18 84.33</td><td>26.17</td><td>86.54</td><td>28.33</td><td>85.81</td></tr><tr><td>8B</td><td>29.03</td><td>84.78</td><td>28.76</td><td>88.64</td><td>23.49</td><td>84.94</td><td>35.38 84.66</td><td>26.42</td><td>86.45</td><td>28.62</td><td>85.89</td></tr><tr><td>9B</td><td>28.97</td><td>84.79</td><td>28.06</td><td>88.39</td><td>23.57</td><td>85.04</td><td>35.11 84.49</td><td>26.20</td><td>86.70</td><td>28.38</td><td>85.88</td></tr><tr><td>10B</td><td>29.25</td><td>84.81</td><td>27.97</td><td>88.52</td><td>23.55</td><td>85.08</td><td>35.60</td><td>84.66 26.18</td><td>86.58</td><td>28.51</td><td>85.93</td></tr><tr><td>11B</td><td>29.62</td><td>85.23</td><td>28.77</td><td>88.68</td><td>24.27</td><td>85.08</td><td>35.75</td><td>84.73 26.55</td><td>86.91</td><td>28.99</td><td>86.13</td></tr><tr><td>12B</td><td>29.85</td><td>85.15</td><td>28.90</td><td>88.67</td><td>24.68</td><td>85.27</td><td>36.31</td><td>84.78 26.95</td><td>87.00</td><td>29.34</td><td>86.17</td></tr><tr><td>13B</td><td>29.88</td><td>85.20</td><td>29.30</td><td>88.80</td><td>24.78</td><td>85.24</td><td>36.35</td><td>84.77 26.98</td><td>87.05</td><td>29.46</td><td>86.21</td></tr><tr><td>14B</td><td>29.95</td><td>85.23</td><td>29.59</td><td>89.09</td><td>25.02</td><td>85.20</td><td>36.37 84.83</td><td>27.00</td><td>87.10</td><td>29.59</td><td>86.29</td></tr><tr><td>15B</td><td>30.10</td><td>85.22</td><td>29.79</td><td>89.09</td><td>25.21</td><td>85.40</td><td>36.27</td><td>84.78 27.37</td><td>86.94</td><td>29.75</td><td>86.29</td></tr><tr><td>16B</td><td>30.12</td><td>85.32</td><td>29.65</td><td>89.14</td><td>24.87</td><td>85.34</td><td>36.58</td><td>84.93 26.97</td><td>86.98</td><td>29.64</td><td>86.34</td></tr><tr><td>17B</td><td>30.07</td><td>85.32</td><td>29.32</td><td>88.71</td><td>25.28</td><td>85.13</td><td>36.24</td><td>84.89 27.43</td><td>87.05</td><td>29.67</td><td>86.22</td></tr><tr><td>18B</td><td>29.63</td><td>85.40</td><td>29.14</td><td>89.02</td><td>25.11</td><td>85.33</td><td>36.64</td><td>84.96 26.96</td><td>87.02</td><td>29.50</td><td>86.35</td></tr><tr><td>19B</td><td>30.01</td><td>85.25</td><td>29.75</td><td>89.06</td><td>25.66</td><td>85.37</td><td>36.87 85.11</td><td>27.13</td><td>86.98</td><td>29.88</td><td>86.35</td></tr><tr><td>20B</td><td>30.31</td><td>85.59</td><td>29.88</td><td>89.10 25.71</td><td></td><td>85.52 36.48</td><td>85.05</td><td>27.09</td><td>87.17</td><td>29.89</td><td>86.49</td></tr><tr><td colspan="10">Translating English (xx→en)</td></tr><tr><td>NLLB-54B</td><td>26.89</td><td>78.94</td><td>39.11</td><td>80.13</td><td>23.09</td><td>71.66</td><td>16.56 70.70</td><td>39.11</td><td>81.88</td><td>28.95</td><td>76.66</td></tr><tr><td>GPT-3.5-D</td><td>30.90</td><td>84.79</td><td>44.50</td><td>86.16</td><td>31.90</td><td>82.13</td><td>25.00</td><td>81.62 38.50</td><td>84.80</td><td>34.16</td><td>83.90</td></tr><tr><td>1B</td><td>29.40</td><td>83.99</td><td>41.64</td><td>85.54</td><td>33.35</td><td>84.76</td><td>22.45 79.12</td><td>38.15</td><td>84.34</td><td>33.00</td><td>83.55</td></tr><tr><td>2B</td><td>29.53</td><td>84.00</td><td>43.32</td><td>85.66</td><td>33.79</td><td>85.17</td><td>22.19 78.98</td><td>38.82</td><td>84.59</td><td>33.53</td><td>83.68</td></tr><tr><td>3B</td><td>30.15</td><td>84.00</td><td>43.08</td><td>85.79</td><td>34.43</td><td>85.47</td><td>22.70 79.29</td><td>39.32</td><td>84.61</td><td>33.94</td><td>83.83</td></tr><tr><td>4B</td><td>29.82</td><td>83.98</td><td>43.26</td><td>85.92</td><td>34.55</td><td>85.59</td><td>23.27 79.84</td><td>39.00</td><td>84.62</td><td>33.98</td><td>83.99</td></tr><tr><td>5B</td><td>30.09</td><td>84.15</td><td>43.39</td><td>85.97</td><td>35.26</td><td>85.77</td><td>23.65 80.05</td><td>38.81</td><td>84.65</td><td>34.24</td><td>84.12</td></tr><tr><td>6B</td><td>30.26</td><td>84.00</td><td>43.91</td><td>85.86</td><td>35.46</td><td>85.82</td><td>23.75 79.85</td><td>39.37</td><td>84.58</td><td>34.55</td><td>84.02</td></tr><tr><td>7B</td><td>29.44</td><td>83.87</td><td>42.53</td><td>85.90</td><td>34.33</td><td>85.71</td><td>23.23 79.76</td><td>38.60</td><td>84.58</td><td>33.63</td><td>83.96</td></tr><tr><td>8B</td><td>29.69 29.76</td><td>84.00 83.94</td><td>42.85 42.89 85.90</td><td>85.68 34.47</td><td>34.38</td><td>85.69 23.03</td><td>22.92 79.31</td><td>38.54 79.57</td><td>84.47 84.50</td></table></body></html>

Table 8: The comprehensive numeric results for LLaMA-2-7B fine-tuned by every 1B monolingual tokens followed by human-written data fine-tuning.  

# G DETAILED RESULTS IN ABLATION STUDY  

We show the detailed results of the ablation study on the effect of monolingual data and the quality of the data in Table 9.  

# H IS MORE HUMAN-WRITTEN PARALLEL DATA BETTER?  

The composition of our human-written data consists of the prior-year WMT test sets (approximately 10K parallel sentences per pair) and Flores data (around 2K per pair). In this analysis, we assess the impact of additional human-written parallel data. Specifically, we compare models (LLaMa-2-7B after stage 1) fine-tuned exclusively on Flores against those fine-tuned on both Flores and WMT data. Results can be found in Table 10. Notably, upon integrating WMT data into the training set, we discern a modest improvement in COMET scores. However, there's an uptick in BLEU scores, particularly for translations into English. We attribue the increase in lexical match (BLEU) to the domain alignment of WMT data. Consequently, our hypothesis is that while an augmented volume of human-written data might marginally enhance segment-level human judgment correlation (COMET), in-domain data can significantly enhance lexical matching.  

Table 9: Detailed results of ablation study on the effect of monolingual data and parallel data quality. The backbone model is LLaMA-2-7B. A red cross $({\pmb x})$ in the table denotes the omission of monolingual data fine-tuning or parallel data (indicative of zero-shot translation). A green check $(\nu)$ signifies that the model undergoes fine-tuning with monolingual data.   


<html><body><table><tr><td rowspan="2" colspan="2">Usemono.ParallelDataQuality</td><td colspan="2">de</td><td colspan="2">CS</td><td colspan="2">is</td><td colspan="2">zh</td><td colspan="2">ru</td><td colspan="2">Avg.</td></tr><tr><td colspan="2">BLEU COMET</td><td colspan="2">BLEU COMET</td><td colspan="2">BLEU COMET</td><td colspan="2">BLEU COMET</td><td colspan="2">BLEU COMET</td><td colspan="2">BLEUCOMET</td></tr><tr><td colspan="10">TranslatingfromEnglish(en-→xx)</td><td colspan="7"></td></tr><tr><td></td><td>x</td><td>19.00</td><td>76.39</td><td>16.02</td><td>79.13</td><td>1.33</td><td>43.83</td><td>16.97 27.66</td><td>71.80 79.70</td><td>16.00</td><td></td><td>73.24</td><td>13.86</td><td>68.88</td></tr><tr><td></td><td>Random</td><td>22.74</td><td>78.06</td><td>19.38</td><td>79.59</td><td>6.20</td><td>50.45</td><td></td><td></td><td></td><td>22.40</td><td>81.64</td><td>19.68</td><td>73.89</td></tr><tr><td></td><td>Filtered</td><td>21.92</td><td>77.59</td><td>19.93</td><td>80.24</td><td>6.91</td><td>51.42</td><td>27.15</td><td>80.09</td><td></td><td>21.90</td><td>82.42</td><td>19.56</td><td>74.35</td></tr><tr><td></td><td>HW</td><td>27.30</td><td>83.46</td><td>22.59</td><td>84.59</td><td>3.61</td><td>45.81</td><td>33.74</td><td>83.81</td><td></td><td>23.63</td><td>84.94</td><td>22.17</td><td>76.52</td></tr><tr><td></td><td>x</td><td>27.44</td><td>84.17</td><td>28.61</td><td>88.57</td><td>21.55</td><td>83.69</td><td>28.51</td><td></td><td>81.56</td><td>25.65</td><td>85.67</td><td>26.35</td><td>84.73</td></tr><tr><td></td><td>Random</td><td>27.38</td><td>82.12</td><td>28.43</td><td>86.82</td><td>21.65</td><td>80.53</td><td>31.68</td><td></td><td>81.73</td><td>25.74</td><td>84.53</td><td>26.98</td><td>83.15</td></tr><tr><td></td><td>Filtered</td><td>27.97</td><td>83.16</td><td>28.45</td><td>87.26</td><td>23.03</td><td>82.40</td><td>31.55</td><td></td><td>82.26</td><td>25.92</td><td>84.84</td><td>27.38</td><td>83.98</td></tr><tr><td></td><td>HW</td><td>30.31</td><td>85.59</td><td>29.88</td><td>89.10</td><td>25.71</td><td>85.52</td><td>36.48</td><td>85.05</td><td></td><td>27.09</td><td>87.17</td><td>29.89</td><td>86.49</td></tr><tr><td colspan="10">Translatingto English(xx-→en)</td><td colspan="7"></td></tr><tr><td>x</td><td>x</td><td>30.42</td><td>82.74</td><td>36.56</td><td>82.42</td><td>10.98</td><td>62.79</td><td></td><td>18.19</td><td>75.00</td><td>36.02</td><td>82.84</td><td>26.43</td><td>77.16</td></tr><tr><td></td><td>Random</td><td>29.15</td><td>82.33</td><td>38.61</td><td>82.67</td><td>17.14</td><td>68.25</td><td>19.32</td><td></td><td>77.24</td><td>36.98</td><td>82.97</td><td>28.24</td><td>78.69</td></tr><tr><td></td><td>Filtered</td><td>29.29</td><td>82.42</td><td>38.41</td><td>82.80</td><td>17.89</td><td>69.05</td><td>19.22</td><td></td><td>77.41</td><td>37.12</td><td>83.04</td><td>28.39</td><td>78.94</td></tr><tr><td></td><td>HW</td><td>29.95</td><td>83.93</td><td>40.32</td><td>84.31</td><td>15.61</td><td>69.13</td><td>22.51</td><td></td><td>78.77</td><td>38.56</td><td>83.88</td><td>29.39</td><td>80.00</td></tr><tr><td></td><td>x</td><td>28.28</td><td>82.48</td><td>38.05</td><td>84.18</td><td>32.79</td><td>84.07</td><td>9.44</td><td></td><td>69.71</td><td>33.88</td><td>81.18</td><td>28.49</td><td>80.32</td></tr><tr><td></td><td>Random</td><td>28.89</td><td>82.74</td><td>40.64</td><td>85.01</td><td>35.11</td><td>85.67</td><td>19.50</td><td></td><td>77.67</td><td>38.19</td><td>84.01</td><td>32.47</td><td>83.02</td></tr><tr><td></td><td>Filtered</td><td>28.63</td><td>82.85</td><td>40.93</td><td>84.85</td><td>35.12</td><td>85.49</td><td>19.04</td><td></td><td>77.92</td><td>37.90</td><td>84.02</td><td>32.32</td><td>83.03</td></tr><tr><td></td><td>HW</td><td>29.49</td><td>83.98</td><td>42.91</td><td>85.90</td><td>35.26</td><td>85.97</td><td>23.52</td><td></td><td>79.73</td><td>38.93</td><td>84.81</td><td>34.02</td><td>84.08</td></tr></table></body></html>  

Table 10: The performance of LLaMa-2-7B (post stage 1 fine-tuning) when fine-tuned exclusively on Flores versus when fine-tuned on both WMT and Flores.   


<html><body><table><tr><td rowspan="2">Parallel Data Used</td><td colspan="2">Avg.xx→en</td><td colspan="2">Avg.en→xx</td></tr><tr><td>BLEU</td><td>COMET</td><td>BLEU</td><td>COMET</td></tr><tr><td>Backbone:LLaMA-2-7BAfterStage1</td><td></td><td></td><td></td><td></td></tr><tr><td>Flores</td><td>30.50</td><td>83.24</td><td>29.28</td><td>86.52</td></tr><tr><td>Flores+WMT</td><td>34.02</td><td>84.08</td><td>29.89</td><td>86.49</td></tr></table></body></html>  

<html><body><table><tr><td rowspan="2">Methods</td><td rowspan="2">BLEU</td><td rowspan="2">Avg. xx→en COMET</td><td rowspan="2">Avg. en→xx</td><td rowspan="2">COMET</td></tr><tr><td>BLEU</td></tr><tr><td>Backbone:LLaMA-2-13BAfterStage1</td><td></td><td></td><td></td><td></td></tr><tr><td>Zero-Shot</td><td>33.07</td><td>83.07</td><td>26.76</td><td>84.03</td></tr><tr><td>Filtered5-shot</td><td>33.12</td><td>83.13</td><td>27.50</td><td>83.78</td></tr><tr><td>HW5-shot</td><td>33.75</td><td>83.91</td><td>27.59</td><td>85.24</td></tr><tr><td>OurStage 2</td><td>34.31</td><td>84.12</td><td>29.78</td><td>86.37</td></tr><tr><td>OurStage2+HW5-shot</td><td>34.14</td><td>84.27</td><td>28.56</td><td>85.87</td></tr></table></body></html>  

Table 11: The performance between 5-shot ICL and stage 2 fine-tuning using the LLaMA-2-13B model post stage 1 as the backbone. Our findings indicate that the quality of shots affects ICL performance. Notably, stage 2 fine-tuning markedly surpasses the 5-shot ICL and ICL does not help moreonstage 2.  

# I  PARALLEL DATA FINE-TUNING VS. IN-CONTEXT LEARNING  

An alternative way to instruct the model to have better translation is in-context learning (ICL) (Brown et al., 2020), as opposed to additional fine-tuning on parallel data. However, ICL is limited to only a few shots given the length of translation examples, while fine-tuning can leverage entirely available data. For ICL, we consider 5-shot evaluations. 5 examples are randomly selected from Filtered data (the Quality-Random examples used by Hendy et al. (2023)). We also consider another 5 examples randomly from the human-written data to examine the impact of example quality. We here compare the performance of our fine-tuning method and 5-shot ICL.9 We assess the LLaMA-2-13B after stage 1 (12B token fine-tuning) and present results in Table 11.  

Interestingly, ICL also holds the same property that higher quality data leads to better performance (Filtered 5-shot vs. HW 5-shot). Moreover, as expected, ICL substantially underperforms our stage 2 fine-tuning possibly due to the small examples provided, which aligns with the findings in the previous work (Liu et al., 2022; Mosbach et al., 2023). This could also clarify why implementing ICL subsequent to stage 2 yields no additional benefits, as all high-quality data has already been incorporated during stage 2 fine-tuning (the last row in the Table).  

# J CROSS-LINGUAL PROFICIENCY OF FINE-TUNED MODELS  

We explore the cross-lingual competencies of our models derived from LLaMA-2 after fine-tuning them on monolingual data. Our aim is to discern whether augmenting monolingual data enhances performance in cross-lingual tasks. Experiments were conducted on zero-shot cross-lingual tasks encompassing three benchmarks: Cross-lingual language understanding (XNLI) Conneau et al. (2018), XStoryCloze—a translation of the English StoryCloze dataset into ten languages (Mostafazadeh et al., 2017), and XWinograd—a multilingual compilation of Winograd Schemas Tikhonov & Ryabinin (2021). Evaluations were restricted to languages overlapping with our fine-tuned languages, namely, German (with only XNLI being inclusive), Chinese, Russian, and English. Unfortunately, none of these datasets covers Icelandic. We first consider baselines for some widely used models: XLM-R large (Conneau et al., 2020), XGLM-7.5B (Lin et al., 2021), BLOOM-7B (Sca0 et al., 2022), and MPT-7B (MosaicML, 2023). In these comparisons, LLaMA-2 demonstrates the top performance for the tested languages. Subsequent fine-tuning with either 1B or 20B monolingual tokens on both LLaMA-2-7B and 13B models yields substantial enhancements for non-English languages across all tasks. A consistent trend observed was that increased monolingual data corresponds to greater performance boosts. Only English is observed for a negligible difference after fine-tuning monolingual data, which is an anticipated outcome given LLaMA-2's proficient grasp of English. The tool we utilize for LLM evaluation is lm-evaluat ion-harness (Gao et al., 2021).10  

<html><body><table><tr><td rowspan="2">Models</td><td colspan="5">XNLI</td><td colspan="4">Xstorycloze</td><td colspan="4">XWinograd</td></tr><tr><td>de</td><td>en</td><td>ru</td><td>zh</td><td>Avg.</td><td>en</td><td>ru</td><td>zh</td><td>Avg.</td><td>en</td><td>ru</td><td>zh</td><td>Avg.</td></tr><tr><td>XLMR-Large</td><td>32.29</td><td>27.51</td><td>31.20</td><td>33.41</td><td>31.10</td><td>49.83</td><td>48.25</td><td>46.46</td><td>48.18</td><td>47.31</td><td>48.89</td><td>44.05</td><td>46.75</td></tr><tr><td>XGLM-7.5B</td><td>48.31</td><td>54.06</td><td>46.55</td><td>34.66</td><td>45.90</td><td>69.82</td><td>63.34</td><td>58.90</td><td>64.02</td><td>79.35</td><td>63.17</td><td>72.82</td><td>71.78</td></tr><tr><td>BLOOM-7B</td><td>39.04</td><td>53.37</td><td>42.61</td><td>35.50</td><td>42.63</td><td>70.48</td><td>52.68</td><td>61.88</td><td>61.68</td><td>82.06</td><td>56.83</td><td>74.21</td><td>71.03</td></tr><tr><td>MPT-7B</td><td>47.87</td><td>55.62</td><td>46.10</td><td>36.71</td><td>46.58</td><td>78.09</td><td>57.71</td><td>59.50</td><td>65.10</td><td>86.58</td><td>68.89</td><td>73.21</td><td>76.23</td></tr><tr><td>LLaMA-2-7B</td><td>45.90</td><td>56.51</td><td>41.33</td><td>34.82</td><td>44.64</td><td>77.04</td><td>63.07</td><td>59.56</td><td>66.56</td><td>87.91</td><td>68.89</td><td>70.63</td><td>75.81</td></tr><tr><td>LLaMA-2-7B,1Bmono.</td><td>47.63</td><td>57.87</td><td>44.02</td><td>34.70</td><td>46.06</td><td>75.84</td><td>64.59</td><td>60.03</td><td>66.82</td><td>85.63</td><td>66.98</td><td>71.83</td><td>74.81</td></tr><tr><td>LLaMA-2-7B,20B mono.</td><td>50.48</td><td>57.35</td><td>46.47</td><td>33.82</td><td>47.03</td><td>75.71</td><td>67.57</td><td>62.81</td><td>68.70</td><td>86.06</td><td>66.67</td><td>75.20</td><td>75.98</td></tr><tr><td>LLaMA-2-13B</td><td>49.08</td><td>53.21</td><td>44.74</td><td>36.14</td><td>45.79</td><td>78.36</td><td>66.18</td><td>63.40</td><td>69.31</td><td>88.99</td><td>68.25</td><td>77.98</td><td>78.41</td></tr><tr><td>LLaMA-2-13B,1Bmono.</td><td>47.27</td><td>49.56</td><td>44.14</td><td>38.35</td><td>44.83</td><td>78.09</td><td>68.63</td><td>62.01</td><td>69.58</td><td>88.47</td><td>69.21</td><td>78.17</td><td>78.62</td></tr><tr><td>LLaMA-2-13B,12Bm0no.</td><td>49.56</td><td>54.02</td><td>47.55</td><td>40.72</td><td>47.96</td><td>78.29</td><td>69.82</td><td>63.67</td><td>70.59</td><td>88.82</td><td>70.16</td><td>78.17</td><td>79.05</td></tr></table></body></html>  

Table 12: We evaluate the zero-shot cross-lingual efficacy on three multilingual datasets. Our findings indicate that fine-tuning LLaMA-2 with more monolingual data results in enhanced performance for non-English languages.  