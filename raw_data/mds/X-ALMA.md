# X-ALMA: PLUG & PLAY MODULES AND ADAPTIVE REJECTION FOR QUALITY TRANSLATION AT SCALE  

Haoran Xu, Kenton Murray, Philipp Koehn {hxu64, kenton, phi}@jhu.edu Johns Hopkins University  

Hieu Hoang, Akiko Eriguchi, Huda Khayrallah {hihoan, akikoe,hkhayrallah}@microsoft.com Microsoft  

## ABSTRACT  

Large language models (LLMs) have achieved remarkable success across various NLP tasks, yet their focus has predominantly been on English due to Englishcentric pre-training and limited multilingual data. While some multilingual LLMs claim to support for hundreds of languages, models often fail to provide highquality response for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages like English and Chinese. We prioritize quality over scaling number of languages, with a focus on multilingual machine translation task, and introduce X-ALMA, a model designed with to ensuring top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 (Ustuin et al., 2024) and Aya-23 (Aryabumi et al., 2024), in every single translation direction on the FLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play languagespecific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimization methods to maximize the translation performance. After the final stage of training regimen, our proposed Adaptive-Rejection Preference Optimization (ARPO) surpasses existing preference optimization methods in translation tasks.1  

# 1 INTRODUCTION  

Large language models (LLMs) such as the GPT series (Brown et al., 2020; OpenAI, 2023), Mistral (Jiang et al., 2023), LLaMA series (Touvron et al., 2023a;b; Dubey et al., 2024), Gemma series (Team et al., 2024a;b), inter alia, among others, have demonstrated impressive performance across various NLP tasks. However, the efficacy of LLMs has primarily been evaluated on English tasks, with their multilingual capabilities receiving less attention due to the models being predominantly pre-trained on English and the scarcity of multilingual data. Recently, there has been a shift towards multilingual studies in LLMs. For instance, LLaMA 3 and 3.1 (Dubey et al., 2024) expand the vocabulary from 32K to 128K and pre-train on multilingual texts; Ustuin et al. (2024) have introduced Aya-101, a multilingual generative model supporting 101 languages; and BigTranslate (Yang et al., 2023) and LLaMAX (Lu et al., 2024) scale LLM-based multilingual translation models to over 100 languages.  

Despite the increased language support in LLMs, their performance across most languages falls short of practical application expectations, especially for mid- and low-resource languages (weakness I). Furthermore, the performance of high-resource languages tends to be inferior compared to LLMs trained on fewer languages, a phenomenon known as the \*curse of multilinguality' (Conneau et al., 2020) (weakness 2). The weaknesses are prevalent in most current state-of-the-art (SoTA) massively multilingual models: overall quality decreases as the number of supported languages increases. Although methods such as building models by focusing on a smaller number of high-resource languages like German and Chinese can achieve satisfactory performance for these languages and mitigate these weaknesses (Aryabumi et al., 2024; Xu et al., 2024a;b; Alves et al., 2024), they neglect the needs of mid- and low-resource languages. In this work, we address these weaknesses and build a multilingual model that achieves consistently high performance across 50 diverse languages, regardless of resource level, with a focus on multilingual machine translation.  

![](https://cdn-mineru.openxlab.org.cn/extract/6b671780-c664-4afa-b106-02815bb70a44/c99a33bd5d0b482a9bc7b80273e779278daf19bccdca396630ff3f69aef5ca33.jpg)  
Figure 1: Relationship between the number of supported languages and their average translation performance. While many state-of-the-art multilingual models claim to support hundreds of languages, the translation quality is not as high as in models trained on fewer languages, particularly for mid- and low-resource languages. This can be seen in the trend of decreasing average scores as more languages are supported. In contrast, we propose X-ALMA, which extends ALMA-R (Xu et al., 2024a;b) by supporting 44 additional diverse languages with even higher average performance, offering top performance across all supported languages, regardless of resource level.  

To visualize these weaknesses, let us closely examine current SoTA models in the context of multilingual translation. We evaluate each model on the overlapping set of languages that are supported by the model and the 50 languages we focus on in this paper. As shown in Figure 1, there is a clear trend: as the number of supported languages increases, the average translation performance decreases. This is intuitively understandable, as it is difficult for mid- and low-resource languages to reach the same level of performance as high-resource languages, thus lowering the overall average. For instance, ALMA-R (Xu et al., 2024b) achieves the highest average translation performance across the 6 languages it supports, while NLLB-200 (NLLB Team et al., 2022) exhibits the lowest average performance on 50 languages, largely due to poorer results in low-resource languages. Although this comparison is not entirely fair due to the varying number of languages tested, it provides a general indication of above-mentioned weaknesses in multilingual models.2  

Despite the ability of current multilingual models to support hundreds of languages, the hollow purplestar $\leftrightsquigarrow$ in the figure represents our ideal model, where the inclusion of more languages does not diminish the average performance. In this work, we introduce our multilingual translation model, X-ALMA, represented by the solid golden star \*★' in Figure 1, which extends ALMA(-R) (Xu et al., 2024a;b) from 6 languages to 50 languages. ALMA-R is one of top-performing translation models built on LLMs, comparable to WMT winners and GPT-4-turbo. Despite the addition of 44 more languages, X-ALMA even achieves slightly higher average performance compared to ALMA-R.  

We summarize our main contributions as follows, including our model architecture design and training methodology.  

Plug-and-Play Architecture: We design X-ALMA with language-specific modules to mitigate negative language interference, with each module serving a group of similar languages. These modules can either be plugged into the base model individually for the inference of target languagesreducing the necessity of loading all expert parameters and saving memory--or all modules can be loaded together in a mixture-of-experts (MoE) way (Shazeer et al., 2017; Lepikhin et al., 2021).  

Effective Training Recipe: The training regimen for X-ALMA consists of three pre-training stages and two post-training stages, each crucial for achieving optimal performance. Furthermore, in the final stage, we introduce Adaptive-Rejection Preference Optimization (ARPO), designed to maximize performance and address the ^over-rejection’ issue (detailed explanation in Section 4) in translation preference learning, which current optimization methods struggle to resolve.  

State-of-the-Art Performance and Data Release: X-ALMA outperforms existing_ SoTA opensource multilingual translation models across 50 diverse languages for every single direction only training on publicly available data, as measured by COMET-22. Given the limited availability of multilingual preference learning data, we also release the preference learning data for 50 languages and the model checkpoints.  

# 2 BACKGROUND  

## 2.1 PROBLEM DEFINITION  

We consider a decoder-only LLM, denoted as $\underline{{\pi}}\theta$ , parameterized by. ${\underline{{\theta,}}}$ for multilingual machine translation tasks. Let $\mathcal{D}$ represent the multilingual dataset, consisting of pairs of a source sentence $\scriptstyle{\frac{x}{\min{\frac{\partial{\big/}}{\partial{\big/}}}}}$ and the corresponding perfect translation $y$ represented as $\mathcal{D}=\{x,y\}$ . Given a prompt $\mathcal{T}$ that instructs the model to perform the translation, our goal is to maximize the log-likelihood of the multilingual parallel dataset $\mathcal{D}$  

$$
\operatorname*{max}_{\theta}\mathbb{E}_{(x,y)\sim\mathcal{D}}[\log\pi_{\theta}(y|x,\mathcal{T})].
$$  

## 2.2  RELATED WORKS  

LLM-Based Translation  Initially, decoder-only LLMs struggle to match the performance of conventional encoder-decoder models for MT. For example, GPT-3.5 slightly under-performed compared to the concurrent WMT winners (Hendy et al., 2023), and large open-source models like OPT175B (Zhang et al., 2022) performed worse than the 1.3B parameter NLLB model (NLLB Team et al., 2022), even on high-resource languages, as demonstrated by Zhu et al. (2024b). This lead to an increased interest in smaller LLMs, such as 7B or 13B models, because even smaller models like NLLB-1.3B showed strong translation capabilities. However, first generation LLM-based MT models such as TIM (Zeng et al., 2023), SWIE (Chen et al., 2023), and BayLing (Zhang et al., 2023) still lag behind encoder-decoder models in performance. Recently, GPT-4 (OpenAI, 2023) has been reported to have achieved top performance in the WMT competition (Kocmi et al., 2023), and smaller LLM-based models, like ALMA(-R) (Xu et al., 2024a;b) and Tower (Alves et al., 2024), have demonstrated comparable performance to GPT-4 by employing their specialized training methods. However, the high performance of SoTA LLM-based translation models is limited to a small subset of languages.  

Massively Multilingual LLM The limited scope of languages in LLM-based MT models stems primarily from their focus on English during pre-training and the use of restricted vocabularies. However, this limitation has driven interest in expanding these models to support a broader range of languages. The simplest approach to extending current LLM-based MT models involves expanding the vocabulary and training on large amounts of parallel data across additional languages (Yang et al., 2023), but this approach has been shown to degrade model performance. Aya-101 (Ustun et al., 2024) revisits the encoder-decoder architecture, building a multilingual model based on the largest MT5 (Xue et al., 2020), designed not only for translation but also for general multilingual QA. Similarly, LLaMAX (Lu et al., 2024) extends LLaMA-2 and LLaMA-3 to over 100 languages. However, multilingual models often suffer from reduced performance on mid- and low-resource languages, which can also negatively impact high-resource language performance. To mitigate this, the decoder-only model Aya-23 (Aryabumi et al., 2024) focuses exclusively on 23 high-resource languages to maximize their performance and avoid the ^curse of multilinguality'. While limiting the number of supported languages can indeed alleviate some challenges, it reverses the goal of building truly multilingual models and neglects the needs of mid- and low-resource languages. In this paper, we expand ALMA-R from 6 languages to 50, ensuring robust performance across all languages.  

![](https://cdn-mineru.openxlab.org.cn/extract/6b671780-c664-4afa-b106-02815bb70a44/26f9438488e16e48f9eaad9539deecc39bb3b69be5fa11af083855f5b3f4d761.jpg)  
Figure 2: High-level architecture design of the plug-and-play multilingual model. Each language group (depicted as fags) is assigned a specific module that works alongside the base model. These language-specific modules handle inputs exclusively from their respective language groups, enabling the model to effectively adapt to different linguistic characteristics while leveraging the shared base model for comprehensive multilingual learning.  

# 3 METHODS  

## 3.1 MODEL ARCHITECTURE  

Our model architecture consists of two main components: a dense base model and multiple language-specific (LS) modules. The core concept of LS modules is to prevent conflicts between languages during training, such as gradient conflicts (Wang et al., 2021). This design is alike the mixture-of-experts (MoE) approach (Shazeer et al., 2017; Lepikhin et al., 2021), but diverges by not using a neural-based gate to assign tokens to experts (LS modules). Instead, similar to Xu et al. (2023), the assignment is hard-gated, i.e., input data is assigned exclusively to the module designated for its language. Consequently, only the base model and the corresponding LS module are activated, depending on the input language. Languages are categorized into distinct groups, with each group sharing a common LS module. An overview of the model architecture is illustrated in Figure 2.  

In detail, the base model architecture is built upon the LLaMA-2 architecture (Touvron et al., 2023b). Each LS module comprises low-rank adaptations (LoRAs) (Hu et al., 2021) integrated into all linear layers within the attention and multi-layer perceptron (MLP) layers. The total number of parameters for each LS module is approximately $15\%$ of the base model.  

Why This Design? While model architectures such as MoE activate only one expert per example, all experts must still reside in GPU memory during training and inference, necessitating high-end GPUs. Moreover, MoE has been reported for its parameter ineffciency in multilingual settings, e.g., hard-gated language assignment can achieve similar performance to MoE while using 4 times fewer parameters( $\mathrm{{Xu}}$ et al., 2023). Compared to MoE, our design offers three distinct modelloading strategies for both training and inference: (1) selectively loading a single, on-demand LS module, which alleviates GPU memory constraints; (2) merging LS modules with the base model to generate a new LS LLM model that retains the same parameter count as the base model, facilitating subsequent use; and (3) loading both the base model and all LS modules as a larger, combined model, similar to the approach employed by MoE.  

Table 1: Language grouping based on linguistic features and balanced number of languages.   


<html><body><table><tr><td>Group ID</td><td>LinguisticFeature</td><td>Languages</td></tr><tr><td>1</td><td>Germaniclanguages</td><td>af,da,de,is,nl,no,sv,(en)</td></tr><tr><td>2</td><td>RomanceLanguages</td><td>ca,es,gl,it,pt,ro,(en)</td></tr><tr><td>3</td><td>Eastern and Southern Slavic Languages</td><td>bg,mk, ru,sr,uk, (en)</td></tr><tr><td>4</td><td>Southeast AsianLanguages</td><td>fr,id, mg,ms,th,vi, (en)</td></tr><tr><td>5</td><td>CentralandEasternEuropeanLanguages</td><td>cs,el,hu,lt,lv,pl,(en)</td></tr><tr><td>6</td><td>Eurasian Language Mix</td><td>et，fi，ja,ka,ko,zh,(en)</td></tr><tr><td>7</td><td>Indo-AryanLanguages</td><td>gu,hi,mr,ne,ur,(en)</td></tr><tr><td>8</td><td>TurkicandSemiticLanguages</td><td>ar,az，fa,he,kk,ky,tr,uz,(en)</td></tr></table></body></html>  

## 3.2 LANGUAGE GROUPING  

In this paper, we consider a total of 50 languages, encompassing 14 scripts and 18 language families, to capture the linguistic diversity. The languages are categorized into 8 groups based on two criteria: (1) each group should consist of languages that are as similar as possible, and (2) the number of languages in each group should be balanced. We opted not to use automated tools like Lang2Vec (Littell et al., 2017) for grouping, as we found that manual grouping based on human linguistic knowledge yields more accurate classification in line with our criteria. The specific languages within each group are presented in Table 1 with their $\operatorname{ISO}-639-1$ code. Note that English (en) is included in all groups to ensure that each group can perform English-centric translation. More detailed information on these languages is provided in Appendix A.  

## 3.3 TRAINING RECIPE  

We provide a comprehensive description of the training recipe for the X-ALMA model, including three pre-training stages and two post-training stages. An overview of this training recipe is depicted in the workflow diagram in Appendix B. The specifics of each stage are elaborated upon as follows.  

Pre-Training Stage 1: Monolingual Fine-Tuning Base Model The first stage of pre-training is dedicated exclusively to the base model. During this phase, we fine-tune the base model using 20B monolingual tokens from all 50 languages, with a sampling ratio proportional to the size of the available monolingual data for each language, as suggested by Xu et al. (2024a). This stage aims to facilitate the model's acquisition of fundamental knowledge across all languages.  

Pre-Training Stage 2: Monolingual Fine-Tuning Language-Specific Modules In all subsequent stages, the base model remains frozen, and the focus shifts to fine-tuning LS modules. During the second stage of pre-training, each LS module is fine-tuned 10B monolingual tokens exclusively from the languages within its respective group. This stage is designed to enable each LS module to emphasize on learning general knowledge across the specific languages.  

Pre-Training Stage 3: Pseudo-Monolingual Fine-tuning In this stage, we continue to fine-tune the LS modules using pseudo-monolingual data from each module's language group to enhance the language knowledge alignment. This pseudo-monolingual data is constructed from parallel sentences. While previous studies have indicated that simple instruction tuning with a large volume of parallel sentences for instruction tuning can degrade model performance ( $\mathrm{Xu}$ et al., $2024\mathrm{a}$ ; Zhu et al., 2024a), recent research demonstrates that utilizing parallel data in the pre-training stage can enhance multilingual alignment (Alves et al., 2024; Kondo et al., 2024; Lu et al., 2024). Similar to these approaches, we combine each available translation pair to create a new sentence in either a <sourcesentence><targetsentence $>$ or $<$ target sentence><source sentence $>$ manner, with the order of the source and target sentence in each pair determined randomly. We then concatenate all the combined translations to construct the pseudo-monolingual data. Each LS module is fine-tuned on 1.25B tokens.  

Post-Training Stage 1: Supervised Fine-tuning Building on the insights from prior research that small but high-quality multilingual datasets are sufficient to yield impressive performance (Maillard et al., 2023; Xu et al., 2024a), we supervised fine-tune (SFT) the model using a small, high-quality parallel dataset at this stage with the translation prompt suggested by $\mathrm{Xu}$ et al. (2024a). This finetuning is performed using a simple causal language modeling (CLM) loss.  

Post-Training Stage 2: Preference Optimization We also introduce Adaptive Rejection PreferenceOptimization $(A R P O)$ to further enhance translation quality across all languages. ARPO is designed to address the 'over-rejection’ issue in MT preference learning, a challenge that other preference optimization methods struggle to manage effectively. We will elaborate on our motivations, methodology, and preference data construction in the following section.  

# 4ADAPTIVE-REJECTION PREFERENCE OPTIMIZATION  

## 4.1  LIMITATIONS IN CURRENT PREFERENCE LEARNING  

When constructing preference data for MT, it is essential that the dis-preferred translation is also of high quality to ensure meaningful model improvement (Xu et al., 2024b). This results in a scenario where the preferred and dis-preferred translations are often very similar, differing by only a few words, which is quite different from the preference data used in open-ended question-answering (QA) tasks (A detailed example is shown in Appendix C). While many preference optimization methods have proven effective in various NLP tasks (Rafailov et al., 2024; Azar et al., 2024; Hong et al., 2024; Meng et al., 2024), we find that they are not well-suited for the MT task because they tend to reject the entire dis-preferred translation which is similar to the preferred one. This approach can inadvertently lead to the rejection of most tokens in the preferred translation as well, resulting in a phenomenon we term over-rejection, where the writing style of the translation outputs is forced away from the preferred data distribution (further analysis and examples in Section 6.1).  

Mathematically speaking, the preference optimization problem can be generally formulated given a dataset $\mathcal{D}=\left\{\dot{x}^{(i)},y_{w}^{(i)},\dot{y}_{l}^{(i)}\right\}_{i=1}^{\bar{N}}$ , where each data point consists of a prompt (source sentence) $x$ , a preferred response (translation) $y_{w}$ , and a dis-preferred response $y_{l}$ , for a total of $N$ data points:  

$$
\mathcal{L}=\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big[f\Big(r_{\theta}(y_{w}|x)-r_{\theta}(y_{l}|x)\Big)\Big],
$$  

where $f(\cdot):\mathbb{R}\to\mathbb{R}$ is a general non-linear function. In many instances, such as in the DPO method (Rafailov et al., 2024), $f$ is the negative log-likelihood of the Bradley-Terry objective, i.e., $f(\cdot)=-\log\sigma(\cdot)$ . Here, $r_{\theta}(y|x)$ represents the reward of $y$ , calculated according to log-probability of the policy model parameterized by $\theta$ : In the case of DPO, the reward function is defined as $r_{\theta}(y|x\bar{)}=\dot{\beta}\log(\pi_{\theta}(\bar{y}|x))-\beta\log(\pi_{\mathrm{ref}}(y|x))$ , where $\beta$ is a hyperparameter and $\pi_{\mathrm{ref}}$ is the reference model.  

As indicated by Equation 2, when $y_{w}$ and $y_{l}$ are too similar, the difference between $r_{\theta}(y_{w}|x)$ and $r_{\theta}(y_{l}|x)$ tends to be small and even near O. Consequently, the near-zero difference between rewards causes the preference loss to a constant value, such as $\dot{f}(0)=-\log\sigma(0)$ in the case of DPO. This makes it challenging for the optimization process to distinguish between the two options, hindering meaningful improvements.  

The challenge becomes even more pronounced in a finite data regime. While an infinite number of response pairs with small but precise preference differences could mitigate the optimization difficulties, translation preference data is often sparse and may contain noise (e.g., the AI-labeled preference datausedby $\mathrm{Xu}$ et al. (2024b) contains only 2K samples per direction). Consequently, the model is prone to overfitting to these minor differences, which poses a significant empirical challenge and can lead to suboptimal learning outcomes, particularly when dealing with a large response (translation) space, as is the case with LLMs.  

## 4.2 ADAPTIVE REJECTION  

To mitigate the over-rejection, we introduce an adaptive penalty, denoted as $\tau_{\theta}$ , which controls the strength of the dis  

$$
\begin{array}{r l}&{\cdot\mathrm{preferred~term~in~the~loss}\cdot\bigcirc}\ &{\mathcal{L}_{\mathrm{ARPO}}=\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\left[f\Big(r_{\theta}(y_{w}|x)-\frac{\eta_{\theta}(y_{w},y_{l})}{\eta_{\theta}(y_{w},y_{l})}\cdot r_{\theta}(y_{l}|x)\Big)\right]\cdot\rightarrow{\sf S f t}_{}}\end{array}
$$  

The value of $\tau_{\theta}$ is determined by the similarity between $y_{w}$ and $y_{l}$ , ranging from O to 1:  

$$
\tau_{\theta}(y_{w},y_{l})=\operatorname*{min}(e^{\eta\cdot z_{\theta}(y_{w},y_{l})}-1,1),
$$  

where $\eta$ is a hyperparameter, and $z_{\theta}(y_{w},y_{l})$ is a function that quantifies the distance between the the preferred and dis-preferred responses by measuring absolute difference of their average loglikelihoods:  

$$
z_{\theta}(y_{w},y_{l})=\mathsf{a b s}(\frac{\log(\pi_{\theta}(y_{w}|x))}{|y_{w}|}-\frac{\log(\pi_{\theta}(y_{l}|x))}{|y_{l}|}),
$$  

The idea is straightforward: when $y_{w}$ and $y_{l}$ are very similar, the absolute difference between their averaged log-likelihoods is small, resulting in $\tau_{\theta}$ close to $0$ , thereby reducing the impact of the dispreferred term on the loss and mitigate rejection on this translation. Conversely, when the difference between $y_{w}$ and $y_{l}$ is large, $\tau_{\theta}$ close to 1, turning the loss back to a standard preference optimization loss.  

In the multilingual MT task, we start with contrastive preference optimization (CPO) ( $\mathrm{Xu}$ et al., 2024b), which has proven to be one of the most effective optimization methods for translation.  

$$
\begin{array}{r}{\mathcal{L}_{\mathrm{CPO}}=-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\Big[\underbrace{\log\sigma\Big(\beta\log\pi_{\theta}\big(y_{w}|x\big)-\beta\log\pi_{\theta}\big(y_{l}|x\big)\Big)}_{\mathrm{preference~loss}}+\underbrace{\log\pi_{\theta}\big(y_{w}|x\big)}_{\mathrm{BC~loss}}\Big].}\end{array}
$$  

CPO consists of two components: preference loss and behavior cloning (BC) loss (Hejna et al., 2023). The BC loss helps prevent the model from drifting too far from the original task. Then, we incorporate adaptive rejection into the preference term in CPO, resulting in a new loss function:  

$$
\begin{array}{r}{\mathcal{L}_{\mathrm{ARP0}}=-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\left[\log\sigma\Big(\beta\log\pi_{\theta}(y_{w}|x)-\underline{{\tau_{\theta}(y_{w},y_{l})}}\cdot\beta\log\pi_{\theta}(y_{l}|x)\Big)+\log\pi_{\theta}(y_{w}|x)\right].}\end{array}
$$  

# 5 EXPERIMENTS  

## 5.1DATA  

Monolingual and Parallel Data Following the introduction of 50 languages in Section 3.2, we focus on 98 English-centric translation directions, both into and from English. We test on Flores-200 test data (NLLB Team et al., 2022) and WMT'23 (Kocmi et al., 2023). For pre-training stages 1 and 2, we use monolingual data sourced from OSCAR (Ortiz Su'arez et al., 2019; Kreutzer et al., 2022). In pre-training stage 3, we construct pseudo-monolingual data using NLLB (NLLB Team et al., 2022) and OPUS Tiedemann (2012); Zhang et al. (2020) parallel training data. In SFT step, building on the insights from Xu et al. (2024a) that a small amount of high-quality data can significantly enhance translation performance, we use the Flores-200 dev set and NTREX (Barrault et al., 2019; Federmann et al., 2022) test data as our training data to ensure the quality. Given that both Flores-200 and NTREX are multi-way-parallel datasets (all languages share the same English source sentences), we also incorporate the WMT'15-22 test data in training only if the language is available in WMT, to mitigate the risk of overfitting to English (Aharoni et al., 2019). Eventually, the data size in the SFT stage for each direction ranges from 3K to 7K, with an average of 4K per direction.  

Preference Data Construction Given the scarcity of preference datasets for multilingual MT, we describe our approach to constructing preference data for 50 languages. Starting with the parallel data used in SFT, for each source sentence $x$ , we generate a translation $y_{\mathrm{xalma}}$ using X-ALMA that has been fine-tuned through SFT. Then, the reference translation $y_{\mathrm{ref}}$ is designated as the preferred translation, and $y_{\mathrm{xalma}}$ as the dis-preferred one, forming our initial preference dataset, denoted as $\mathcal{D}_{1}=\{x,y_{\mathrm{ref}},y_{\mathrm{xalma}}\}$ . While $\mathrm{Xu}$ et al. (2024b) suggests that reference translations can sometimes be inferior to system-generated ones and employs reference-free models like XCOMET (Guerreiro et al., 2023) for ranking translations in preference data construction, we opted not to use this method to avoid potential metric bias, as the same metrics are used for evaluation.Although $\mathrm{Xu}$ et al. (2024b) demonstrates that using the same metric for both ranking and evaluation aligns with human preferences, we chose to avoid this approach to prevent unnecessary controversy. As a result, $\mathcal{D}1$ might contain some noise due to the assumption that reference translations are always preferred. To reduce this noise, for high-resource languages, we also employ GPT-4o to produce revised translations $y_{\mathrm{gpt}}$ conditioned on $(x,y_{\mathrm{xalma}})$ , drawing on studies that show post-editing by LLMs can improve translation quality (Ki & Carpuat, 2024; Feng et al., 2024; Raunak et al., 2023). We show the prompts in Appendix D. Thus, our second preference dataset is defined as $\mathcal{D}_{2}=\{x,y_{\mathrm{gpt}},y_{\mathrm{xalma}}\}$ .We then concatenate the two datasets to form the final preference dataset, denoted as $\mathcal{D}=\mathcal{D}_{1}\cup\mathcal{D}_{2}$  

Table 2:  The overall results of Flores test data across each language group in $\tt e n\to\tt x x$ Scores are reported using COMET-22. X-ALMA outperforms both massively multilingual models, such as Aya-101, and models focus specifically on high-resource languages, like Aya-23. ‘All’ represents the average performance across all languages in the group, while High’ refers to the average performance for high-resource languages in the group. Bold numbers represent the highest scores.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">Group1</td><td colspan="2">Group 2</td><td colspan="2">Group3</td><td colspan="2">Group 4</td><td colspan="2">Group5</td><td colspan="2">Group6</td><td colspan="2">Group7</td><td colspan="2">Group8</td></tr><tr><td>All</td><td>High</td><td>All</td><td>High</td><td>All</td><td>High</td><td>All</td><td>High</td><td>All</td><td>High</td><td>All</td><td>High</td><td>All</td><td>High</td><td>All</td><td>High</td></tr><tr><td>LLaMA-3.1-8B-Instruct</td><td>80.8</td><td>79.8</td><td>83.7</td><td>84.2</td><td>79.1</td><td>69.2</td><td>76.3</td><td>85.8</td><td>79.0</td><td>81.2</td><td>71.1</td><td>71.8</td><td>70.1</td><td>69.6</td><td>78.3</td><td>84.4</td></tr><tr><td>NLLB-3.3B</td><td>88.2</td><td>88.8</td><td>88.3</td><td>88.1</td><td>89.4</td><td>89.1</td><td>87.1</td><td>88.2</td><td>89.2</td><td>89.8</td><td>87.5</td><td>87.5</td><td>80.1</td><td>80.9</td><td>88.1</td><td>87.5</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>86.4</td><td>86.9</td><td>86.8</td><td>86.6</td><td>85.7</td><td>82.0</td><td>81.7</td><td>86.2</td><td>86.6</td><td>87.1</td><td>86.0</td><td>87.3</td><td>76.5</td><td>76.6</td><td>82.6</td><td>83.6</td></tr><tr><td>Aya-101</td><td>85.0</td><td>85.7</td><td>86.8</td><td>86.2</td><td>87.7</td><td>85.6</td><td>85.8</td><td>85.5</td><td>88.4</td><td>88.7</td><td>87.5</td><td>87.3</td><td>76.2</td><td>75.5</td><td>86.8</td><td>86.3</td></tr><tr><td>Aya-23-8B</td><td>75.1</td><td>84.7</td><td>86.6</td><td>86.6</td><td>74.4</td><td>75.7</td><td>74.6</td><td>88.7</td><td>70.6</td><td>77.3</td><td>67.1</td><td>79.8</td><td>68.9</td><td>79.3</td><td>76.0</td><td>87.9</td></tr><tr><td>Aya-23-35B</td><td>79.6</td><td>86.5</td><td>87.1</td><td>87.0</td><td>77.6</td><td>78.5</td><td>76.7</td><td>88.6</td><td>82.1</td><td>86.0</td><td>73.9</td><td>84.4</td><td>61.9</td><td>79.1</td><td>68.8</td><td>87.8</td></tr><tr><td>X-ALMA(only SFT)---8</td><td>89.589.7</td><td></td><td>89.2</td><td>88.9</td><td>90.7</td><td>90.2</td><td>88.1</td><td>89.1</td><td>90.6</td><td>90.7</td><td>90.1</td><td>90.48</td><td>82.6</td><td>81.4</td><td>89.2</td><td>88.9</td></tr><tr><td>X-ALMA</td><td>89.6</td><td>89.9</td><td>89.4</td><td>89.0</td><td>90.9</td><td>90.5</td><td>88.6</td><td>89.5</td><td>91.0</td><td>91.1</td><td>90.6</td><td>90.8</td><td>83.2</td><td>81.9</td><td>89.4</td><td>89.2</td></tr></table></body></html>  

Table 3: The overall COMET-22 scores of Flores test data across each language group in $\mathrm{xx}\mathrm{\longrightarrow}\mathrm{en}$ Similarly, X-ALMA outperforms all baselines.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">Group1</td><td colspan="2">Group 2</td><td colspan="2">Group3</td><td colspan="2">Group 4</td><td colspan="2">Group5</td><td colspan="2">Group6</td><td colspan="2">Group7</td><td colspan="2">Group8</td></tr><tr><td>All</td><td>High</td><td>Al1</td><td>High</td><td>All</td><td>High</td><td>Al1</td><td>High</td><td>All</td><td>High</td><td>All</td><td>High</td><td>Al1</td><td>High</td><td>All</td><td>High</td></tr><tr><td>LLaMA-3.1-8B-Instruct</td><td>68.8</td><td>77.6</td><td>70.9</td><td>76.9</td><td>51.2</td><td>53.8</td><td>65.6</td><td>76.4</td><td>54.8</td><td>60.2</td><td>58.8</td><td>66.9</td><td>47.6</td><td>53.7</td><td>53.7</td><td>67.5</td></tr><tr><td>NLLB-3.3B</td><td>79.1</td><td>81.8</td><td>84.5</td><td>85.0</td><td>84.3</td><td>83.8</td><td>81.1</td><td>85.4</td><td>74.9</td><td>76.0</td><td>76.1</td><td>77.3</td><td>88.3</td><td>88.9</td><td>79.5</td><td>81.6</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>88.3</td><td>88.5</td><td>88.1</td><td>87.9</td><td>87.0</td><td>86.8</td><td>86.2</td><td>87.9</td><td>87.0</td><td>87.2</td><td>81.7</td><td>87.7</td><td>83.6</td><td>88.9</td><td>84.7</td><td>87.7</td></tr><tr><td>Aya-101</td><td>87.2</td><td>88.2</td><td>87.6</td><td>87.6</td><td>85.4</td><td>85.5</td><td>86.2</td><td>87.6</td><td>86.3</td><td>86.5</td><td>86.5</td><td>86.7</td><td>84.8</td><td>87.5</td><td>85.9</td><td>87.1</td></tr><tr><td>Aya-23-8B</td><td>84.6</td><td>88.2</td><td>87.9</td><td>87.7</td><td>83.3</td><td>83.3</td><td>79.8</td><td>88.5</td><td>82.9</td><td>85.2</td><td>79.9</td><td>86.1</td><td>71.8</td><td>89.1</td><td>76.7</td><td>88.0</td></tr><tr><td>Aya-23-35B</td><td>87.4</td><td>88.9</td><td>88.8</td><td>88.6</td><td>86.3</td><td>86.2</td><td>82.3</td><td>88.7</td><td>86.4</td><td>87.2</td><td>85.9</td><td>88.0</td><td>79.4</td><td>89.6</td><td>82.7</td><td>88.6</td></tr><tr><td>X-ALMA (only SFT)</td><td>89.1</td><td>89.2</td><td>88.8</td><td>88.6</td><td>87.9</td><td>87.7</td><td>87.7</td><td>88.8</td><td>87.9</td><td>88.1</td><td>88.2</td><td>88.3</td><td>89.3</td><td>89.8</td><td>87.5</td><td>88.4</td></tr><tr><td>X-ALMA</td><td>89.4</td><td>89.5</td><td>89.2</td><td>89.0</td><td>88.1</td><td>87.8</td><td>88.0</td><td>88.9</td><td>88.2</td><td>88.4</td><td>88.7</td><td>88.8</td><td>89.6</td><td>90.1</td><td>88.0</td><td>88.9</td></tr></table></body></html>  

## 5.2  TRAINING SETUP  

We use ALMA-13B-Pretrain ( $\mathrm{{Xu}}$ et al., 2024a) as our backbone model, which is pre-trained on 6 languages and based on LLaMA-2 (Touvron et al., 2023b). Following Xu et al. (2024a), we pre-train the backbone model with a batch size of 256, a warm-up ratio of 0.01, and sequences containing up to 512 tokens. In the post-training stage, the model is fine-tuned for many-to-many multilingual translation manner using 1 epoch with a batch size of 128, and other settings remain unchanged. For preference learning,we set $\eta$ as1.5and $\beta$ as 0.1 for all experiments.  

## 5.3 BASELINES  

We use the strongest open-source massively multilingual translation models as our baselines, including NLLB-200 (NLLB Team et al., 2022), Aya-101 (Ustuin et al., 2024), and LLaMAX3-Alpaca (Lu et al., 2024). Additionally, we compare our model's translation performance with Aya-23-8B and Aya-35B (Aryabumi et al., 2024) to demonstrate that increasing the number of supported languages does not compromise the performance of high-resource languages, effectively mitigating the curse of multilinguality. We also include LLaMA-3.1-8B-Instruct as a baseline to assess the performance of one of the latest strong LLMs in multilingual translation.  

## 5.4 RESULTS  

We present the average performance for each language group in both $\tt e n\to\tt x x$ and $_{\mathrm{XX}\to\in\ n}$ directions on the Flores-200 test data in Tables 2 and 3. The results for WMT'23 in both directions are provided in Table 4. Due to space limitations, we report COMET-22 (Rei et al., 2022) in the main paper. However, in Appendix E, we also include the reference-free metric, XCOMET-XL (Guerreiro et al., 2023) as recommended by Xu et al. (2024b), and BLEU (Papineni et al., 2002; Post, 2018) for completeness even though it has been shown to be less accurate for strong translation models compared to other metrics (Freitag et al., 2022; Xu et al., 2024b; Kocmi et al., 2024). Detailed results for each translation direction can also be found in Appendix E.  

Compared with SoTA Multilingual Models: General instruction-tuned LLaMA-3.1 significantly lags behind models specifically designed for translation, so we primarily focus on other models. For Flores-200 and WMT'23, when compared to other massively multilingual models such as NLLB3.3B, LLaMAX3-Alpaca-8B, and Aya-101, X-ALMA consistently outperforms them on average across all language groups, both into and from English. Furthermore, X-ALMA surpasses Aya-23- 8B and Aya-23-35B—-both of which are tailored for high-resource languages—-on average across all high-resource languages in each group. In fact, as detailed in Appendix E, X-ALMA surpasses all baselines in every translation direction according to COMET-22 and outperforms in 97 out of 98 directions based on XCOMET-XL, offering top translation performance for all languages considered.  

Table 4: Results on WMT'23 dataset reported using COMET-22. The symbol $\rightarrow$ represents translations from English into the target language, while $\gets$ indicates translations into English.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="2">de</td><td colspan="2">zh</td><td colspan="2">ja</td><td colspan="2">ru</td><td colspan="2">uk</td><td colspan="2">he</td><td colspan="2">Avg.</td></tr><tr><td>→</td><td>←</td><td>←</td><td>←</td><td>←</td><td>←</td><td>一</td><td>←</td><td>→</td><td>←</td><td>←</td><td></td><td>←</td><td>←</td></tr><tr><td>ALMA-R-13B</td><td>84.0</td><td>85.5</td><td>85.0</td><td>80.6</td><td></td><td></td><td>85.5</td><td>83.3</td><td></td><td></td><td>1</td><td></td><td></td><td></td></tr><tr><td>TowerInstruct-7B-v0.2</td><td>83.1</td><td>84.6</td><td>85.6</td><td>80.5</td><td></td><td></td><td>85.3</td><td>83.1</td><td></td><td></td><td></td><td></td><td>一</td><td></td></tr><tr><td>NLLB-3.3B</td><td>79.7</td><td>66.6</td><td>79.6</td><td>67.8</td><td>81.6</td><td>65.8</td><td>83.8</td><td>76.7</td><td></td><td>82.879.083.6</td><td></td><td>79.98</td><td>81.872.6</td><td></td></tr><tr><td>LlamaX3-8B</td><td>73.3</td><td>79.4</td><td>81.5</td><td>79.3</td><td>81.8</td><td>80.1</td><td>81.6</td><td>81.3</td><td>80.6</td><td>84.9</td><td>82.5</td><td>83.0</td><td>80.2</td><td>81.3</td></tr><tr><td>Aya-101</td><td>75.1</td><td>81.6</td><td>78.6</td><td>73.7</td><td>84.6</td><td>77.3</td><td>83.1</td><td>81.4</td><td>82.7</td><td>84.5</td><td>82.0</td><td>82.9</td><td>81.0</td><td>80.2</td></tr><tr><td>Aya-23-8B</td><td>80.4</td><td>82.1</td><td>85.3</td><td>78.8</td><td>86.5</td><td>80.2</td><td>84.3</td><td>81.6</td><td>84.3</td><td>85.0</td><td>84.3</td><td>84.9</td><td>84.2</td><td>82.1</td></tr><tr><td>Aya-23-35B</td><td>80.7</td><td>82.3</td><td>84.6</td><td>79.7</td><td>86.4</td><td>81.6</td><td>84.7</td><td>82.2</td><td>84.0</td><td>85.7</td><td>84.1</td><td>85.9</td><td>84.1</td><td>82.9</td></tr><tr><td>X-ALMA (only SFT)</td><td>84.1</td><td>85.3</td><td>86.1</td><td>80.3</td><td>86.8</td><td>81.6</td><td>85.9</td><td>82.4</td><td>85.3</td><td>86.4</td><td>86.1</td><td>84.4</td><td>85.7</td><td>83.4</td></tr><tr><td>X-ALMA</td><td>84.4</td><td>85.7</td><td>86.7</td><td>80.9</td><td>87.5</td><td>82.4</td><td>86.3</td><td>83.3</td><td>85.5</td><td>86.8</td><td>86.2</td><td>85.6</td><td>86.1</td><td>84.1</td></tr></table></body></html>  

Table 5: Average performance comparison of various preference optimization methods for $\tt e n\to\tt x x$ and $\mathrm{xx}\mathrm{\longrightarrow}\mathrm{en}$ onGroup6.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="4">Avg.en→xx</td><td colspan="3">Avg.xx→en</td></tr><tr><td>BLEU</td><td>COMET-22</td><td></td><td>XCOMET-XL</td><td>BLEU</td><td>COMET-22</td><td>XCOMET-XL</td></tr><tr><td>XALMA(onlySFT)</td><td>26.5</td><td>90.1</td><td></td><td>80.3</td><td>32.1</td><td>88.2</td><td>77.4</td></tr><tr><td>+ DPO</td><td></td><td>53.6</td><td>51.1</td><td></td><td>-7.1----79.2</td><td></td><td>64.6</td></tr><tr><td>+BC</td><td>23.5</td><td>90.2</td><td>80.0</td><td>27.8</td><td>87.5</td><td></td><td>77.4</td></tr><tr><td>+SimPO</td><td>0.0</td><td>16.7</td><td>1.3</td><td></td><td>0.0 16.4</td><td></td><td>8.9</td></tr><tr><td>+ BC</td><td>23.3</td><td>89.7</td><td>78.7</td><td>26.6</td><td>87.1</td><td></td><td>76.5</td></tr><tr><td>+KTO</td><td>22.1</td><td>89.8</td><td></td><td>79.2</td><td>26.4</td><td>87.1</td><td>76.5</td></tr><tr><td>+BC</td><td>26.4</td><td>90.3</td><td></td><td>80.4</td><td>29.2</td><td>87.5</td><td>77.1</td></tr><tr><td>+ ORPO--</td><td></td><td></td><td></td><td></td><td></td><td></td><td>70.8</td></tr><tr><td>+ CPO-- -</td><td></td><td></td><td></td><td></td><td></td><td></td><td>77.0</td></tr><tr><td>+ ARPO (Fina1X-ALMA) - - 27.8 - - - 90.6</td><td></td><td></td><td></td><td></td><td></td><td></td><td>78.4</td></tr></table></body></html>  

Effectiveness of ARPO: ARPO delivers consistent improvements compared to SFT-only models in Flores-200 and WMT'23. Similarly, as shown in the full results in Appendix E, ARPO enhances performance in every translation direction as measured by COMET-22 and delivers improvements in 95 out of 98 directions according to XCOMET-XL. We also compare the effectiveness of ARPO against other preference optimization methods in Section 6.1.  

# 6 ANALYSES  

All analyses will be conducted on languages in Group 6, as it is the most challenging group to learn due to its mix of typologically diverse Asian and European languages.  

## 6.1 PREFERENCE OPTIMIZATION COMPARISON  

Here, we compare ARPO with other popular optimization methods, including DPO (Rafailov et al., 2024), KTO (Ethayarajh et al., 2024), ORPO (Hong et al., 2024), SimPO (Meng et al., 2024), and the original CPO (Xu et al., 2024b). As indicated by CPO findings, directly applying preference learning to the MT task can harm the model, but adding a behavior cloning (BC) regularizer can stabilize training and improve the performance (Xu et al., 2024b). Following them, we also incorporate a BC regularizer into optimization methods that do not originally include it to provide a fair comparison. Table 5 presents the comparison of preference optimization methods across all three metrics. As shown, ARPO clearly outperforms all baselines.  

Where is Over-Rejection?  Over-rejection manifests itself under the significant shift in writing style away from the preferred data distribution. We observe a clear and big BLEU scores drop across all other preference optimization methods, indicating a decline in lexical matching. However, for certain methods, such as $\mathrm{DPO}+\mathrm{BC}$ $\mathrm{KTO}+\mathrm{BC}$ , and CPO, both COMET scores do not decrease as drastically (and in some cases even improve slightly for $\tt e n\to\tt X X$ ), suggesting that the models still produce accurate translations that maintain the same semantic meaning, but with a different writing style. Some translation examples generated by CPO are provided in Appendix F. Unlike ARPO, methods such as CPO tend to produce a wider range of writing styles to convey the same meaning as the reference, many of which are accurate and non-detrimental. However, excessive shifts in writing style still can introduce translation errors that negatively impact overall quality. These small number of errors, concealed within a lot of stylistic deviations, are where over-rejection occurs. As hypothesized in Section 4, the significant style shift is caused by the model rejecting dis-preferred translations that are similar to preferred ones, leading to an excessive rejection of certain writing styles from the preferred data. However, ARPO addresses this issue by constraining the stylistic variation within a more controlled range, thereby mitigating errors caused by over-rejection.  

For other methods such as naive DPO and SimPO, which even though work well in other NLP tasks, the over-rejection severely impairs the model's ability to generate meaningful translations. The introduction of ARPO significantly mitigate the over-rejection issue (stable BLEU scores) and maximize the translation qualities (the highest scores in two COMET metrics).  

## 6.2ABLATION STUDY  

Training Recipe We investigate the impact of each step in the training recipe on model performance. The average results for Group 6, both into and from English, are presented in the left part of Figure 3. The results show a clear trend of consistent performance improvement with each step in the training process. Note that 'None′ in the figure represents the initial checkpoint in our recipe, ALMA-13B-Pretrain.  

Parallel Data for SFT For SFT, we use high-quality parallel data from three sources: NTREX, WMT, and the Flores-200 dev set. Here, we investigate how combining parallel datasets affects performance during the SFT stage. As shown on the right of Figure 3, using only NTREX data already achieves impressive average translation performance for Group 6. Adding high-quality WMT data further boosts average performance, particularly for translations into English data. We hypothesize that this improvement stems from the increased diversity of English data, which mitigates overfitting to the NTREX English domain—a known issue with multi-way-parallel data, as observed by Aharoni et al. (2019). Conversely, incorporating more Flores-200 dev data (also multi-way-parallel) into training does not result in significant gains, also suggesting that the strong translation performance is not driven by in-domain Flores-200 data.  

![](https://cdn-mineru.openxlab.org.cn/extract/6b671780-c664-4afa-b106-02815bb70a44/4cecac801bc2cefae7d6149b1512b503f0d2bd07b02416774f05314d821c4364.jpg)  
Figure 3: Left: ablation study on each stage of the training recipe, demonstrating that adding each stage leads to consistent performance improvements. Right: ablation study on the impact of parallel data composition during the SFT stage. Adding WMT data to NTREX significantly enhances model performance, while adding Flores-200 data provides no noticeable improvement.  

# 7 CONCLUSION  

we tackled the challenge of achieving high translation quality while scaling to a large number of languages, a limitation seen in many state-of-the-art multilingual models. We have introduced X  

ALMA, an LLM-based multilingual translation system that prioritizes translation quality across all supported 50 languages, regardless of resource level. X-ALMA surpasses SoTA models such as Aya-101 and Aya-23 in all translation directions on the FLORES-200 and WMT'23 test datasets, as measured by COMET-22. X-ALMA is built on a plug-and-play architecture with language-specific modules, complemented by a carefully designed training recipe. In particular, the final stage of the recipe, ARPO, achieves further performance gains and outperforms existing preference optimization methods in translation tasks, while successfully mitigating the over-rejection issue.  

## ACKNOWLEDGEMENTS  

We express our profound appreciation to HyoJung Han, Jack Zhang, Tianjian Li, Thamme Gowda, Tom Kocmi, Young Jin Kim, Hany Hassan Awadalla, Marcin Junczys-Dowmunt, Vikas Raunak, Matt Post, Anoop Kunchukuttan, Roman Grundkiewicz, Arul Menezes, and Vishal Chowdhary for their engaging and valuable discussions that greatly enriched our work.  

REFERENCES   
Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 3874-3884, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1388. URL https : / /aclanthology.0rg/N19-1388.   
Duarte M Alves, José Pombal, Nuno M Guerreiro, Pedro H Martins, Joao Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, et al. Tower: An open multilingual large language model for translation-related tasks. arXiv preprint arXiv:2402.17733, 2024.   
Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bhara1 Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, et al. Aya 23: Open weight releases to further multilingual progress. arXiv preprint arXiv:2405.15032, 2024.   
Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, Pp. 4447-4455. PMLR, 2024.   
Loic Barrault, Ondrej Bojar, Marta R. Costa-jussa, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Muiller, Santanu Pal, Matt Post, and Marcos Zampieri. Findings of the 2019 conference on machine translation (WMT19). In Ondrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Mathias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), Pp. 1-61, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5301. URL https: //aclanthology.0rg/W19-5301.   
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.   
Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou.  Improving translation faithfulness of large language models via augmenting instructions. arXiv preprint arXiv:2308.12674,2023.   
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL ht tps : //aclanthology.0rg/2020.acl-main.747.   
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.   
Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 - news test references for MT evaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pp. 21-24, Online, nov 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.sumeval-1.4.   
Zhaopeng Feng, Ruizhe Chen, Yan Zhang, Zijie Meng, and Zuozhu Liu. Ladder: A modelagnostic framework boosting llm-based machine translation to the next level. arXiv preprint arXiv:2406.15741,2024.   
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 46-68, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https : //aclanthology.0rg/2022.wmt-1.2.   
Nuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT Martins. xcomet: Transparent machine translation evaluation through fine-grained eror detection. arXiv preprint arXiv:2310.10482, 2023.   
Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W Bradley Knox, and Dorsa Sadigh. Contrastive prefence learning: Learning from human feedback without rl. arXiv preprint arXiv:2310.13639, 2023.   
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210, 2023.   
Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691, 2(4):5, 2024.   
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.   
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   
Dayeon Ki and Marine Carpuat. Guiding large language models to post-edit machine translation with error annotations. arXiv preprint arXiv:2404.07851, 2024.   
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic, and Mariya Shmatova. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on Machine Translation, pp. 1-42, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.wmt-1.1.   
Tom Kocmi, Vilm Zouhar, Christian Federmann, and Matt Post. Navigating the metrics maze: Reconciling score magnitudes and accuracies. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1999-2014, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long. 110.  

Minato Kondo, Takehito Utsuro, and Masaaki Nagata. Enhancing translation accuracy of large language models through continual pre-training on parallel data. arXiv preprint arXiv:2407.03145, 2024.  

Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoit Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias Muiller, Andre Muller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Cabuk Ball, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50-72, 2022. doi: 10.1162/ tacla-00447. URL https://aclanthology.org/2022.tacl-1.4.   
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang. Maxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. URL https: / /openreview.net /forum?id $\equiv$ qrwe7XHTmYb.   
Patrick Littell, David R. Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors. In Mirella Lapata, Phil Blunsom, and Alexander Koller (eds.), Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 8-14, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://aclanthology.org/E17-2002.   
Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei Yuan. Llamax: Scaling linguistic horizons ofllm by enhancing translation capabilities beyond 100 languages. arXiv preprint arXiv:2407.05975, 2024.   
Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj Goswami, Philipp Koehn, Angela Fan, and Francisco Guzman. Small data, big impact: Leveraging minimal data for effective machine translation. In Proceedings of the 6lst Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2740-2756, 2023.   
Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. arXiv preprint arXiv:2405.14734, 2024.   
Marta R NLLB Team, Costa-jussa, James Cross, Onur Celebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. Nolanguage lft behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.   
OpenAI. Gpt-4 technical report, 2023.   
Pedro Javier Ortiz Su'arez, Benoit Sagot, and Laurent Romary. Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pp. 9 - 16, Mannheim, 2019. Leibniz-Institut f'ur Deutsche Sprache. doi: 10.14618/ids-pub-9021. URL http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215.   
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 4oth annual meeting of the Association for Computational Linguistics, pp. 311-318, 2002.   
Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186-191, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https: //aclanthology.org/W18-6319.  

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.  

Vikas Raunak, Amr Sharaf, Yiren Wang, Hany Awadalla, and Arul Menezes. Leveraging GPT-4 for automatic translation post-editing. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 12009-12024, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-emnlp.804. URL https://aclanthology.org/2023.findings-emnlp. 804.  

Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578-585, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https : / /aclanthology.org/2022. wmt-1.5 2.  

Noam Shazeer, \*Azalia Mirhoseini, \*Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.  Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In International Conference on Learning Representations, 2017. URL https : / /openreview.net /forum?id $\equiv{}_{-}$ B1ckMDqlg.  

Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje F Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al. Aya dataset: An open-access collection for multilingual instruction tuning. arXiv preprint arXiv:2402.06619, 2024.  

Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024a.  

Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118, 2024b.  

Jorg Tiedemann. Parallel data, tools and interfaces in OPUS. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pp. 2214-2218, Istanbul, Turkey, May 2012. European Language Resources Association (ELRA). URL http: / /www . 1rec- Conf . org / proceedings/1rec2012/pdf/463_Paper.pdf.  

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.  

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.  

Ahmet Ustin, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827, 2024.  

Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao. Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models. In International Conference on Learning Representations, 2021. URL https: / /openreview.net /forum?id= F1vEjWK-1H_.  

Haoran Xu, Weiting Tan, Shuyue Li, Yunmo Chen, Benjamin Van Durme, Philipp Koehn, and Kenton Murray. Condensing multilingual knowledge with lightweight language-specific modules. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1575-1587, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.97. URL https://aclanthology.org/2023.emnlp-main.97.   
Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. A paradigm shift in machine translation: Boosting translation performance of large language models. In The Twelfth International Conference on Learning Representations, 2024a. URL https : / /openreview . net / forum?id $=$ farT6XXntP.   
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of lm performance in machine translation. In Forty-first International Conference on Machine Learning, 2024b.   
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Sidhant, Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020.   
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. Bigtrans: Augmenting large language models with multilingual translation capability over 100 languages. arXiv preprint arXiv:2305.18098, 2023.   
Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. Tim: Teaching large language models to translate with comparison. arXiv preprint arXiv:2307.04408, 2023.   
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. _ Improving massively multilingual neural machine translation and zero-shot translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1628-1639, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.148. URL https://aclanthology.org/2020.acl-main.148.   
Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, et al. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. arXiv preprint arXiv:2306.10968, 2023.   
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.   
Dawei Zhu, Sony Trenous, Xiaoyu Shen, Dietrich Klakow, Bill Byrne, and Eva Hasler. A preference-driven paradigm for enhanced translation with large language models. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 3385-3403, Mexico City, Mexico, June 2024a. Ass0ociation for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.186. URL https : //aclanthology.0rg/2024.naacl-long.186.   
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language models: Empirical results and analysis. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 2765-2781, Mexico City, Mexico, June 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.176. URL https://aclanthology.org/2024.findings-naacl.176.  

# APPENDIX CONTENTS  

<html><body><table><tr><td>AppendixSections</td><td>Contents</td></tr><tr><td>AppendixA</td><td>LanguageInformation</td></tr><tr><td>AppendixB</td><td>Illustration of Training Recipe</td></tr><tr><td>Appendix C</td><td>Reward Differences of MT and QA</td></tr><tr><td>AppendixD</td><td>Prompts</td></tr><tr><td>Appendix E</td><td>FullResultsofAllDirections</td></tr><tr><td>Appendix F</td><td>Examples of Over-Rejection</td></tr></table></body></html>  

## A LANGUAGE INFORMATION  

We provide detailed information on the eight language groups, including their scripts, language families, and resource levels, in Table 6. Each group includes English to ensure that each languagespecific module supports English-centric translation and to prevent catastrophic forgetting of English. While we primarily grouped languages based on linguistic similarity, the grouping is not perfect. This is due to the need to balance the number of languages in each group and the inherent nature of language resources. For example, Group 6 is a mix of Asian and European languages, and although most languages in Group 4 are Southeast Asian languages, we include French as an additional bonus language to facilitate cross-lingual transfer, especially since most languages in this group are low- and mid-resource.  

Table 6: Detailed information of all langauges.   


<html><body><table><tr><td>Language</td><td>ISO-639-1</td><td>Script</td><td>Family</td><td>Subgroup</td><td>Resource</td></tr><tr><td>English</td><td>en</td><td>Latin</td><td>Indo-European</td><td>Germanic</td><td>High</td></tr><tr><td colspan="6">GroupI:Germaniclanguages</td></tr><tr><td>Afrikaans</td><td>af</td><td>Latin</td><td>Indo-European</td><td>Germanic</td><td>Mid</td></tr><tr><td>Danish</td><td>da</td><td>Latin</td><td>Indo-European</td><td>Germanic</td><td>Mid</td></tr><tr><td>Dutch</td><td>nl</td><td>Latin</td><td>Indo-European</td><td>Germanic</td><td>High</td></tr><tr><td>German</td><td>de</td><td>Latin</td><td>Indo-European</td><td>Germanic</td><td>High</td></tr><tr><td>Icelandic</td><td>is</td><td>Latin</td><td>Indo-European</td><td>Germanic</td><td>Low</td></tr><tr><td>Norwegian</td><td>no</td><td>Latin</td><td>Indo-European</td><td>Germanic</td><td>Low</td></tr><tr><td>Swedish</td><td>SV</td><td>Latin</td><td>Indo-European</td><td>Germanic</td><td>High</td></tr><tr><td colspan="6">Group2:RomanceLanguages</td></tr><tr><td>Catalan Galician</td><td>ca</td><td>Latin Latin</td><td>Indo-European</td><td>Italic</td><td>High</td></tr><tr><td>Italian</td><td>gl it</td><td>Latin</td><td>Indo-European Indo-European</td><td>Italic Italic</td><td>Mid High</td></tr><tr><td>Portuguese</td><td>pt</td><td>Latin</td><td>Indo-European</td><td>Italic</td><td>High</td></tr><tr><td>Romanian</td><td></td><td>Latin</td><td>Indo-European</td><td>Italic</td><td>Mid</td></tr><tr><td>Spanish</td><td>ro es</td><td>Latin</td><td>Indo-European</td><td>Italic</td><td>High</td></tr><tr><td colspan="6">Group3:Eastern andSouthernSlavicLanguages</td></tr><tr><td>Bulgarian</td><td>bg</td><td>Cyrillic</td><td>Indo-European</td><td>Balto-Slavic</td><td>Mid</td></tr><tr><td>Macedonian</td><td>mk</td><td>Cyrillic</td><td>Indo-European</td><td>Balto-Slavic</td><td>Low</td></tr><tr><td>Russian</td><td>ru</td><td>Cyrillic</td><td>Indo-European</td><td>Balto-Slavic</td><td>High</td></tr><tr><td>Serbian</td><td>sr</td><td>Cyrillic</td><td>Indo-European</td><td>Balto-Slavic</td><td>High</td></tr><tr><td>Ukrainian</td><td>uk</td><td>Cyrillic</td><td>Indo-European</td><td>Balto-Slavic</td><td>Mid</td></tr><tr><td colspan="6">Group4:SoutheastAsianLanguages</td></tr><tr><td>French</td><td>fr</td><td>Latin</td><td></td><td></td><td></td></tr><tr><td>Indonesian</td><td></td><td></td><td>Indo-European</td><td>Italic</td><td>High</td></tr><tr><td></td><td>id</td><td>Latin</td><td>Austronesian</td><td>Malayo-Polynesian</td><td>Mid</td></tr><tr><td>Malagasy</td><td>mg</td><td>Latin</td><td>Austronesian</td><td>Malayo-Polynesian</td><td>Low</td></tr><tr><td>Malay</td><td>ms</td><td>Latin</td><td>Austronesian</td><td>Malayo-Polynesian</td><td>Mid</td></tr><tr><td>Thai</td><td>th</td><td>Thai</td><td>Tai-Kadai</td><td>Kam-Tai</td><td>Mid</td></tr><tr><td>Vietnamese</td><td>vi</td><td>Latin</td><td>Austronesian</td><td>Vietic</td><td>High</td></tr></table></body></html>

Continued on next page  

<html><body><table><tr><td>Language</td><td>ISO-639-1</td><td>Script</td><td>Family</td><td>Subgroup</td><td>Resource</td></tr><tr><td colspan="6">Group 5: Central and Eastern EuropeanLanguages</td></tr><tr><td>Czech</td><td>CS</td><td>Latin</td><td>Indo-European</td><td>Balto-Slavic</td><td>High</td></tr><tr><td>Greek</td><td>el</td><td>Greek</td><td>Indo-European</td><td>Graeco-Phrygian</td><td>Mid</td></tr><tr><td>Hungarian</td><td>hu</td><td>Latin</td><td>Uralic</td><td>Finnic</td><td>High</td></tr><tr><td>Latvian</td><td>1v</td><td>Latin</td><td>Indo-European</td><td>Balto-Slavic</td><td>Mid</td></tr><tr><td>Lithuanian</td><td>1t</td><td>Latin</td><td>Indo-European</td><td>Balto-Slavic</td><td>Mid</td></tr><tr><td>Polish</td><td>pl</td><td>Latin</td><td>Indo-European</td><td>Balto-Slavic</td><td>High</td></tr><tr><td colspan="6">Group6:EurasianLanguageMix</td></tr><tr><td>Chinese</td><td>zh</td><td>Han</td><td>Sino-Tibetan</td><td>Sinitic</td><td>High</td></tr><tr><td>Estonian</td><td>et</td><td>Latin</td><td>Uralic</td><td>Finnic</td><td>Mid</td></tr><tr><td>Finnish</td><td>fi</td><td>Latin</td><td>Uralic</td><td>Finnic</td><td>High</td></tr><tr><td>Georgian</td><td>ka</td><td>Georgian</td><td>Kartvelian</td><td>Georgian-Zan</td><td>Mid</td></tr><tr><td>Japanese</td><td>ja</td><td>Japanese</td><td>Japonic</td><td>Japanesic</td><td>High</td></tr><tr><td>Korean</td><td>ko</td><td>Hangul</td><td>Koreanic</td><td>Korean</td><td>High</td></tr><tr><td colspan="6">Group 7: Indo-AryanLanguages</td></tr><tr><td>Gujarati</td><td>gu</td><td>Gujarati</td><td>Indo-European</td><td>Indo-Aryan</td><td>Low</td></tr><tr><td>Hindi</td><td>hi</td><td>Devanagari</td><td>Indo-European</td><td>Indo-Aryan</td><td>High</td></tr><tr><td>Marathi</td><td>mr</td><td>Devanagari</td><td>Indo-European</td><td>Indo-Aryan</td><td>Low</td></tr><tr><td>Nepali</td><td>ne</td><td>Devanagari</td><td>Indo-European</td><td>Indo-Aryan</td><td>Low</td></tr><tr><td>Urdu</td><td>ur</td><td>Arabic</td><td>Indo-European</td><td>Indo-Aryan</td><td>Mid</td></tr><tr><td colspan="6">Group8:TurkicandSemiticLanguages</td></tr><tr><td>Arabic</td><td>ar</td><td>Arabic</td><td>Afro-Asiatic</td><td>Semitic</td><td>High</td></tr><tr><td>Azerbaijani</td><td>az</td><td>Arabic/Latin</td><td>Turkic</td><td>Common Turkic</td><td>Low</td></tr><tr><td>Hebrew</td><td>he</td><td>Hebrew</td><td>Afro-Asiatic</td><td>Semitic</td><td>Mid</td></tr><tr><td>Kazakh</td><td>kk</td><td>Cyrillic</td><td>Turkic</td><td>Common Turkic</td><td>Mid</td></tr><tr><td>Kyrgyz</td><td>ky</td><td>Cyrillic</td><td>Turkic</td><td>Common Turkic</td><td>Low</td></tr><tr><td>Persian</td><td>fa</td><td>Arabic</td><td>Indo-European</td><td>Iranian</td><td>High</td></tr><tr><td>Turkish</td><td>tr</td><td>Latin</td><td>Turkic</td><td>Common Turkic</td><td>High</td></tr><tr><td>Uzbek</td><td>uz</td><td>Latin</td><td>Turkic</td><td>Common Turkic</td><td>Low</td></tr></table></body></html>  

## B ILLUSTRATION OF TRAINING RECIPE  

Here, we illustrate an overview of our 5-step training recipe in Figure 4.  

![](https://cdn-mineru.openxlab.org.cn/extract/6b671780-c664-4afa-b106-02815bb70a44/fed576b01079c771b49b0c09b4629214d78c3c2e370bb80858d9a0f1deb34d88.jpg)  
Figure 4:  This diagram of the multi-stage process of fine-tuning a multilingual model. In PreTraining Stage 1, the base model is fine-tuned using 20B tokens of monolingual data from 50 languages. The process continues with Pre-Training Stage 2, where language-specific modules are fine-tuned with 10B monolingual tokens. Pre-Training Stage 3 introduces pseudo-monolingual fine-tuning, using randomly concatenated parallel sentences to improve multilingual alignment. The model then undergoes Post-Training Stage I, where SFT is performed on high-quality parallel data, followed by Post-Training Stage 2, which applies Adaptive Contrastive Preference Optimization to address over-rejection issues in translation preference learning.  

## C REWARD DIFFERENCES OF MT AND QA  

Here, we present a comparison of the reward difference between machine translation (MT) tasks and open-ended question answering (QA) tasks in preference learning. Figure 5 illustrates the cumulative distribution of reward differences for the MT preference dataset, as described in Section 5, alongside the multilingual preference data from the Aya open-ended QA dataset (Singh et al., 2024) for languages in Group 6. The reward differences are sorted in ascending order, and their cumulative probabilities are displayed. The reward difference is computed using the CPO loss function: $\log\pi_{\boldsymbol{\theta}}(y_{w}|x)-\log\pi_{\boldsymbol{\theta}}(y_{l}|\bar{\boldsymbol{x}})$ . The construction of the Aya preference dataset follows the same methodology as the MT preference data, where we fine-tune the Aya QA dataset via SFT and use the fine-tuned model to generate answers for the training data. System-generated responses are treated as dis-preferred, while original references are considered preferred. As shown in Figure 5, the open-ended QA task exhibits significantly larger reward differences compared to machine translation. For instance, the maximum reward difference for the smallest $80\%$ of MT preference data is 20, whereas it is approximately 300 for Aya QA. Similarly, the maximum reward difference for the MT preference data is 131, while that for Aya QA is nearly tenfold larger.  

![](https://cdn-mineru.openxlab.org.cn/extract/6b671780-c664-4afa-b106-02815bb70a44/61be29de97934b3402c320dd136e57cbdf4027203d055cb8cacf4ddd4b0e24bb.jpg)  
Figure 5: Cumulative distribution of reward differences between machine translation and openended question answering tasks in contrastive preference optimization.  

## D PROMPTS  

In Figure 6, we present the prompt used for GPT-4o post-editing during the construction of the preference dataset, as well as the prompt used for X-ALMA in generating translations.  

## E FULL RESULTS  

Tables 7 to 14 present the results for each translation direction across language groups in the Flores200 dataset, while Table 15 shows the full results for the WMT'23 dataset. On the Flores-200 dataset, X-ALMA surpasses all other open-source multilingual models in every translation direction according to COMET-22, and in 97 out of 98 directions according to XCOMET-XL. Additionally, ARPO, when compared to SFT, demonstrates superior performance in all translation directions reported by COMET-22 and in 95 out of 98 directions according to XCOMET-XL.  

# GPT-4o Post-Edit Prompt  

# System:  

You are a nativespeakerofboth<sourcelanguage $>$ and<targetlanguage $>$ .You are an expert post editor of translationsfrom<sourcelanguage $>$ into $<$ targetlanguage> and a helpful assistant dedicated to improving translation quality. You will be provided with a source sentence in <source language> and its translation in <target language>. Your task is to carefully analyze provided source sentence and translation, and suggest improvements to the translation. Note that you only need to generate a refined translation in<target sentence $>$ and do not generate anything else.  

# User:  

The source sentence in<source language $>$ is: <source sentence> The translation in <target language $>$ is: ${<}X$ -ALMA translatedsentence> Note that you only need to generate a refined translation in $<$ targetlanguage $\mathrm{\textperthousand}$ and do not generate anything else.  

# X-ALMA Translation Chat Template  

$\mathrm{<s>}$ [INST] Translate this from $<$ sourcelanguage $>$ to $<$ target language>:   
<source language>: <source sentence>   
<targetlanguage $>$ : [/INST]  

Table 7: Full results for Group 1 in the Flores test data.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">en→af</td><td colspan="3">en→da</td><td colspan="3">en→de</td><td colspan="3">en→is</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>LLaMA-3.1-8B-Instruct</td><td>34.4</td><td>84.8</td><td>74.5</td><td>37.3</td><td>86.6</td><td>71.7</td><td>30.2</td><td>77.5</td><td>68.8</td><td>11.9</td><td>69.4</td><td>53.6</td></tr><tr><td>NLLB-3.3B</td><td>38.9</td><td>87.4</td><td>74.8</td><td>44.5</td><td>90.0</td><td>76.8</td><td>40.0</td><td>88.1</td><td>76.2</td><td>24.5</td><td>84.6</td><td>74.1</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>38.1</td><td>86.2</td><td>73.2</td><td>40.3</td><td>89.5</td><td>76.2</td><td>32.2</td><td>86.6</td><td>75.0</td><td>20.4</td><td>82.9</td><td>72.3</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>38.5</td><td>86.0</td><td>72.7</td><td>38.2</td><td>88.6</td><td>73.6</td><td>31.4</td><td>85.4</td><td>72.4</td><td>18.3</td><td>81.2</td><td>69.5</td></tr><tr><td>Aya-101</td><td>22.5</td><td>78.8</td><td>40.8</td><td>34.2</td><td>87.6</td><td>62.9</td><td>29.3</td><td>84.3</td><td>67.5</td><td>20.9</td><td>84.3</td><td>74.5</td></tr><tr><td>Aya-23-8B</td><td>17.6</td><td>79.6</td><td>68.2</td><td>19.3</td><td>76.4</td><td>56.6</td><td>36.8</td><td>88.1</td><td>77.0</td><td>1.6</td><td>38.4</td><td>9.6</td></tr><tr><td>Aya-23-35B</td><td>26.7</td><td>81.2</td><td>67.9</td><td>29.0</td><td>82.9</td><td></td><td>37.0</td><td>88.1</td><td>77.2</td><td>5.9</td><td>51.0</td><td>28.6</td></tr><tr><td>X-ALMA (only SFT)</td><td>44.2</td><td>87.5</td><td>75.0</td><td>48.6</td><td>91.8</td><td>79.2</td><td>41.2</td><td>88.7</td><td>77.9</td><td>28.0</td><td>87.2</td><td>78.5</td></tr><tr><td>X-ALMA (Ours)</td><td>43.0</td><td>87.6</td><td>75.8</td><td>48.9</td><td>92.0</td><td>79.7</td><td>41.1</td><td>88.8</td><td>78.0</td><td>27.4</td><td>87.2</td><td>78.5</td></tr><tr><td></td><td></td><td>en→n1</td><td></td><td></td><td>en→no</td><td></td><td></td><td>en→sv</td><td></td><td></td><td>Avg. en→xx</td><td></td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>LLaMA-3.1-8B-Instruct</td><td>22.1</td><td>81.1</td><td>72.5</td><td>27.0</td><td>85.6</td><td>74.0</td><td>34.4</td><td>80.8</td><td>67.3</td><td>28.2</td><td>80.8</td><td>68.9</td></tr><tr><td>NLLB-3.3B</td><td>27.5</td><td>87.5</td><td>76.7</td><td>33.0</td><td>88.9</td><td>76.6</td><td>44.3</td><td>90.7</td><td>78.0</td><td>36.1</td><td>88.2</td><td>76.2</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>23.4</td><td>86.4</td><td>76.2</td><td>30.1</td><td>88.9</td><td>78.1</td><td>39.3</td><td>89.6</td><td>77.7</td><td>32.0</td><td>87.1</td><td>75.5</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>23.3</td><td>86.3</td><td>75.9</td><td>28.0</td><td>87.8</td><td>74.2</td><td>38.7</td><td>89.1</td><td>75.6</td><td>30.9</td><td>86.4</td><td>73.4</td></tr><tr><td>Aya-101</td><td>22.1</td><td>85.8</td><td>72.2</td><td>26.9</td><td>87.5</td><td>69.0</td><td>31.3</td><td>86.9</td><td>61.2</td><td>26.7</td><td>85.0</td><td>64.0</td></tr><tr><td>Aya-23-8B</td><td>26.0</td><td>87.9</td><td>78.8</td><td>15.7</td><td>77.3</td><td>60.0</td><td>20.8</td><td>78.3</td><td>59.7</td><td>19.7</td><td>75.1</td><td>58.5</td></tr><tr><td>Aya-23-35B</td><td>26.6</td><td>87.7</td><td>78.2</td><td>22.1</td><td>82.4</td><td>67.6</td><td>28.8</td><td>83.7</td><td>67.9</td><td>25.2</td><td>79.6</td><td>64.7</td></tr><tr><td>X-ALMA (only SFT)</td><td>29.3</td><td>88.8</td><td>80.2</td><td>35.0</td><td>90.6</td><td>80.8</td><td>47.0</td><td>91.7</td><td>80.8</td><td>39.1</td><td>89.5</td><td>78.9</td></tr><tr><td>X-ALMA (Ours)</td><td>29.5</td><td>89.0</td><td>80.4</td><td>34.2</td><td>90.8</td><td>81.5</td><td>47.2</td><td>91.8</td><td>81.0</td><td>38.7</td><td>89.6</td><td>79.3</td></tr><tr><td colspan="2"></td><td>af→en</td><td>XCOMET</td><td>BLEU</td><td></td><td>da→en</td><td>XCOMET</td><td>de→en</td><td></td><td>BLEU</td><td>is→en</td><td></td></tr><tr><td colspan="14">BLEU COMET-22 BLEU</td></tr><tr><td>LLaMA-3.1-8B-Instruct</td><td>14.5</td><td>66.2</td><td>33.9</td><td>21.0</td><td>COMET-22 66.5</td><td>52.5</td><td>36.0</td><td>COMET-22 78.6</td><td>XCOMET 72.3</td><td>3.2</td><td>COMET-22 43.4</td><td>XCOMET 42.0</td></tr><tr><td>NLLB-3.3B</td><td>40.6</td><td>80.3</td><td>62.7</td><td>34.4</td><td>83.0</td><td>66.0</td><td>28.6</td><td>81.3</td><td>64.5</td><td>16.2</td><td>64.2</td><td>42.9</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>53.5</td><td>88.9</td><td>76.1</td><td>46.0</td><td>89.7</td><td>79.8</td><td>41.4</td><td>88.9</td><td>78.5</td><td>31.2</td><td>84.8</td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>53.1</td><td>89.0</td><td>76.0</td><td>45.3</td><td>89.6</td><td>79.6</td><td>40.5</td><td>88.8</td><td>78.4</td><td>32.5</td><td>85.6</td><td>75.0</td></tr><tr><td>Aya-101</td><td>43.2</td><td>86.1</td><td>65.4</td><td>42.4</td><td>89.2</td><td>75.9</td><td>39.7</td><td>88.5</td><td>77.9</td><td>27.2</td><td>82.3</td><td>75.8</td></tr><tr><td>Aya-23-8B</td><td>46.9</td><td>85.3</td><td>70.6</td><td>42.6</td><td>87.7</td><td>76.8</td><td>43.9</td><td>89.3</td><td>78.9</td><td>13.0</td><td></td><td>68.4</td></tr><tr><td>Aya-23-35B</td><td>54.3</td><td>88.3</td><td>74.9</td><td>47.3</td><td>89.7</td><td>79.4</td><td>45.1</td><td>89.5</td><td></td><td></td><td>68.0</td><td>46.5</td></tr><tr><td>X-ALMA (only SFT)</td><td>58.8</td><td>89.9</td><td>76.2</td><td></td><td></td><td></td><td></td><td></td><td>78.6</td><td>24.5</td><td>78.5</td><td>66.1</td></tr><tr><td>X-ALMA (Ours)</td><td>58.6</td><td>90.0</td><td>76.6</td><td>49.6 49.7</td><td>90.2 90.7</td><td>79.5 80.4</td><td>45.7 45.3</td><td>89.6 89.8</td><td>78.7 79.2</td><td>37.7 37.4</td><td>87.1 87.2</td><td>76.3 76.6</td></tr><tr><td></td><td colspan="8">nl→en no→en</td></table></body></html>  

Table 8: Full results for Group 2 in the Flores test data.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">en→ca</td><td colspan="3">en→es</td><td colspan="3">en→g1</td><td colspan="3">en→it</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>37.5</td><td>86.8</td><td>77.8</td><td>25.0</td><td>83.9</td><td>77.2</td><td>30.2</td><td>84.3</td><td>74.0</td><td>24.9</td><td>81.7</td><td>72.8</td></tr><tr><td>NLLB-3.3B</td><td>43.1</td><td>87.8</td><td>77.6</td><td>28.6</td><td>86.5</td><td>80.0</td><td>35.7</td><td>87.3</td><td>76.8</td><td>31.3</td><td>88.5</td><td>80.5</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>37.7</td><td>87.2</td><td>78.1</td><td>25.1</td><td>85.5</td><td>79.0</td><td>31.5</td><td>86.5</td><td>77.2</td><td>26.0</td><td>87.0</td><td>78.9</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>36.3</td><td>86.5</td><td>76.5</td><td>24.1</td><td>85.0</td><td>76.1</td><td>31.2</td><td>86.4</td><td>76.6</td><td>26.5</td><td>86.9</td><td>77.7</td></tr><tr><td>Aya-101</td><td>37.8</td><td>87.1</td><td>77.9</td><td>24.2</td><td>85.3</td><td>78.2</td><td>32.7</td><td>86.7</td><td>78.0</td><td>25.6</td><td>87.0</td><td>78.5</td></tr><tr><td>Aya-23-8B</td><td>25.1</td><td>81.7</td><td>71.9</td><td>27.8</td><td>86.4</td><td>80.6</td><td>17.2</td><td>82.7</td><td>77.3</td><td>30.2</td><td>88.4</td><td>81.0</td></tr><tr><td>Aya-23-35B</td><td>33.1</td><td>83.9</td><td>73.8</td><td>27.7</td><td>86.2</td><td>80.1</td><td>25.3</td><td>84.2</td><td>76.4</td><td>30.5</td><td>88.2</td><td>80.3</td></tr><tr><td>X-ALMA (only SFT)</td><td>45.7</td><td>89.0</td><td>80.6</td><td>29.5</td><td>87.2</td><td>81.8</td><td>39.0</td><td>88.4</td><td>80.1</td><td>32.5</td><td>89.1</td><td>82.1</td></tr><tr><td>X-ALMA (Ours)</td><td>45.3</td><td>89.0</td><td>80.6</td><td>29.5</td><td>87.3</td><td>81.0</td><td>38.8</td><td>88.7</td><td>80.6</td><td>32.7</td><td>89.3</td><td>82.3</td></tr><tr><td></td><td colspan="4">en→pt</td><td colspan="8">en→ro</td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>Avg. en→xx COMET-22</td><td>XCOMET</td><td></td><td></td><td></td></tr><tr><td>Llama-3.1</td><td>42.4</td><td>84.2</td><td>75.7</td><td>30.5</td><td>81.2</td><td>75.1</td><td>31.7</td><td>83.7</td><td>75.4</td><td></td><td></td><td></td></tr><tr><td>NLLB-3.3B</td><td>49.6</td><td>89.6</td><td>80.4</td><td>37.6</td><td>90.2</td><td>87.1</td><td>37.6</td><td>88.3</td><td>80.4</td><td></td><td></td><td></td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>41.9</td><td>88.6</td><td>79.7</td><td>31.8</td><td>88.6</td><td>85.9</td><td>32.3</td><td>87.2</td><td>79.8</td><td></td><td></td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>41.5</td><td>88.1</td><td>77.0</td><td>32.7</td><td>88.1</td><td>84.0</td><td>32.0</td><td>86.8</td><td>78.0</td><td></td><td></td><td></td></tr><tr><td>Aya-101</td><td>32.5</td><td>85.3</td><td>60.5</td><td>34.9</td><td>89.4</td><td>86.9</td><td>31.3</td><td>86.8</td><td>76.6</td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>48.4</td><td>89.9</td><td>81.7</td><td>37.9</td><td>90.6</td><td>89.3</td><td>31.1</td><td>86.6</td><td>80.3</td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>48.6</td><td>89.7</td><td>81.0</td><td>38.4</td><td>90.7</td><td>88.9</td><td>33.9</td><td>87.1</td><td>80.1</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (only SFT)</td><td>49.9</td><td>90.2</td><td>82.4</td><td>42.2</td><td>91.5</td><td>90.6</td><td>39.8</td><td>89.2</td><td>82.9</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (Ours)</td><td>50.2</td><td>90.4</td><td>82.8</td><td>43.3</td><td>91.6</td><td>90.8</td><td>40.0</td><td>89.4</td><td>83.0</td><td></td><td></td><td></td></tr><tr><td colspan="2"></td><td colspan="8">ca→en</td><td colspan="2">it→en</td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>38.7</td><td>78.1</td><td>72.7</td><td>26.6</td><td>76.2</td><td>71.4</td><td>9.2</td><td>53.2</td><td>48.5</td><td>24.1</td><td>71.6</td><td>67.3</td></tr><tr><td>NLLB-3.3B</td><td>37.9</td><td>83.7</td><td>70.6</td><td>27.1</td><td>85.3</td><td>76.3</td><td>34.7</td><td>84.0</td><td>71.7</td><td>28.8</td><td>84.4</td><td>73.1</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>43.6</td><td>88.4</td><td>78.4</td><td>29.3</td><td>86.8</td><td>78.4</td><td>38.7</td><td>88.0</td><td>78.2</td><td>31.9</td><td>87.6</td><td>78.9</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>42.9</td><td>88.3</td><td>78.1</td><td>29.0</td><td>86.7</td><td>78.1</td><td>38.6</td><td>88.0</td><td>78.0</td><td>31.3</td><td>87.5</td><td>78.5</td></tr><tr><td>Aya-101</td><td>41.1</td><td>87.6</td><td>75.6</td><td>28.8</td><td>86.8</td><td>78.0</td><td>35.5</td><td>86.9</td><td>72.7</td><td>31.2</td><td>87.4</td><td>78.1</td></tr><tr><td>Aya-23-8B</td><td>39.5</td><td>85.8</td><td>75.9</td><td>31.3</td><td>87.4</td><td>78.6</td><td>37.3</td><td>87.0</td><td>76.5</td><td>34.1</td><td>88.1</td><td>79.0</td></tr><tr><td>Aya-23-35B</td><td>46.3</td><td>88.4</td><td>77.8</td><td>33.1</td><td>87.7</td><td>78.5</td><td>41.7</td><td>88.5</td><td>78.0</td><td>36.0</td><td>88.3</td><td>78.9</td></tr><tr><td>X-ALMA (only SFT)</td><td>48.6 48.7</td><td>89.2</td><td>77.8 78.7</td><td>34.9</td><td>87.7</td><td>77.8</td><td>44.9</td><td>89.0</td><td>77.9</td><td>36.9</td><td>88.3</td><td>78.5 79.3</td></tr><tr><td>X-ALMA (Ours)</td><td colspan="4">89.6</td><td colspan="8">87.9 79.0</td></tr><tr><td>BLEU</td><td></td><td>pt→en COMET-22</td><td>XCOMET</td><td></td></table></body></html>  

Table 9: Full results for Group 3 in the Flores test data.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">en→bg</td><td colspan="3">en→mk</td><td colspan="3">en→ru</td><td colspan="3">en→sr</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>29.6</td><td>87.8</td><td>74.2</td><td>24.9</td><td>85.7</td><td>74.3</td><td>14.4</td><td>63.0</td><td>45.1</td><td>1.4</td><td>75.3</td><td>75.4</td></tr><tr><td>NLLB-3.3B</td><td>40.5</td><td>90.9</td><td>77.5</td><td>34.4</td><td>88.8</td><td>77.4</td><td>32.2</td><td>89.2</td><td>77.5</td><td>33.8</td><td>89.0</td><td>77.1</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>33.0</td><td>89.6</td><td>77.1</td><td>29.6</td><td>87.9</td><td>77.5</td><td>25.4</td><td>87.9</td><td>76.3</td><td>8.1</td><td>79.3</td><td>77.4</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>32.2</td><td>89.0</td><td>76.8</td><td>29.3</td><td>87.4</td><td>77.0</td><td>26.4</td><td>87.7</td><td>76.4</td><td>5.8</td><td>76.2</td><td>73.8</td></tr><tr><td>Aya-101</td><td>34.3</td><td>90.0</td><td>78.4</td><td>30.7</td><td>88.7</td><td>79.0</td><td>27.2</td><td>88.3</td><td>77.5</td><td>23.3</td><td>82.9</td><td>73.2</td></tr><tr><td>Aya-23-8B</td><td>6.7</td><td>73.3</td><td>60.4</td><td>2.9</td><td>57.1</td><td>42.3</td><td>29.9</td><td>89.6</td><td>79.4</td><td>0.9</td><td>61.7</td><td>55.0</td></tr><tr><td>Aya-23-35B</td><td>17.0</td><td>75.7</td><td>56.3</td><td>9.6</td><td>65.4</td><td>51.3</td><td>31.2</td><td>89.6</td><td>79.1</td><td>1.1</td><td>67.4</td><td>65.1</td></tr><tr><td>X-ALMA (only SFT)</td><td>42.1</td><td>91.7</td><td>80.9</td><td>37.3</td><td>90.4</td><td>80.9</td><td>32.3</td><td>90.1</td><td>80.2</td><td>36.4</td><td>90.2</td><td>81.4</td></tr><tr><td>X-ALMA (Ours)</td><td>41.7</td><td>91.8</td><td>81.1</td><td>37.6</td><td>90.6</td><td>81.4</td><td>32.9</td><td>90.3</td><td>80.5</td><td>36.8</td><td>90.7</td><td>81.6</td></tr><tr><td>en→uk</td><td colspan="3">Avg. en→xx</td><td colspan="3"></td><td colspan="3"></td><td colspan="3"></td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-3.1</td><td>22.9</td><td>83.6</td><td>69.6</td><td>18.6</td><td>79.1</td><td>67.7</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NLLB-3.3B</td><td>30.3</td><td>89.1</td><td>74.4</td><td>34.2</td><td>89.4</td><td>76.8</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>24.3</td><td>88.0</td><td>74.8</td><td>24.1</td><td>86.5</td><td>76.6</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>25.5</td><td>87.9</td><td>74.3</td><td>23.8</td><td>85.7</td><td>75.6</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-101</td><td>25.1</td><td>88.7</td><td>75.7</td><td>28.1</td><td>87.7</td><td>76.8</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>29.4</td><td>90.2</td><td>78.1</td><td>13.9</td><td>74.4</td><td>63.0</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>30.3</td><td>90.0</td><td>77.3</td><td>17.8</td><td>77.6</td><td>65.8</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>X-ALMA (only SFT)</td><td>31.8 32.0</td><td>90.8 90.9</td><td>78.8 78.9</td><td>36.0</td><td>90.7</td><td>80.4 80.7</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>X-ALMA (Ours)</td><td colspan="3"></td><td colspan="3">36.2 90.9</td><td colspan="3"></td><td colspan="3"></td></tr><tr><td></td><td></td><td>bg→en COMET-22</td><td></td><td></td><td>mk→en</td><td></td><td></td><td>ru→en</td><td></td><td></td><td>sr→en</td><td></td></tr><tr><td></td><td>BLEU 10.8</td><td>53.7</td><td>XCOMET 46.7</td><td>BLEU 1.5</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1 NLLB-3.3B</td><td>37.6</td><td>86.0</td><td>74.1</td><td>37.1</td><td>39.7 84.3</td><td>43.1 71.6</td><td>14.4 30.7</td><td>61.0 84.2</td><td>48.2</td><td>6.0 35.8</td><td>46.6</td><td>48.8</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>38.1</td><td>87.6</td><td>77.9</td><td>39.6</td><td>87.2</td><td>77.2</td><td>33.1</td><td>86.2</td><td>73.1 76.7</td><td>40.5</td><td>83.4</td><td>71.1</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>38.2</td><td>87.5</td><td>77.6</td><td>39.8</td><td>87.2</td><td>77.2</td><td>33.1</td><td>86.4</td><td>76.8</td><td>40.6</td><td>87.2</td><td>78.8</td></tr><tr><td>Aya-101</td><td>32.9</td><td>85.4</td><td>70.7</td><td>33.7</td><td>84.3</td><td>70.1</td><td>32.7</td><td>86.1</td><td>76.7</td><td></td><td>87.3</td><td>78.6</td></tr><tr><td>Aya-23-8B</td><td>32.6</td><td>84.4</td><td>71.7</td><td>25.0</td><td>78.4</td><td>63.3</td><td>36.1</td><td>86.7</td><td></td><td>35.0</td><td>85.0</td><td>72.6</td></tr><tr><td>Aya-23-35B</td><td>38.2</td><td>86.7</td><td>75.5</td><td>36.2</td><td>84.6</td><td></td><td></td><td></td><td>76.8</td><td>27.9</td><td>79.9</td><td>66.0</td></tr><tr><td>X-ALMA (only SFT)</td><td>43.4</td><td>88.4</td><td>77.9</td><td>45.6</td><td>88.2</td><td>72.4 77.1</td><td>38.6</td><td>87.1 87.0</td><td>76.7 76.6</td><td>37.8</td><td>85.3</td><td>75.0 78.8</td></tr><tr><td>X-ALMA (Ours)</td><td colspan="3">42.9 88.6 78.5</td><td colspan="3">45.6 88.4 77.6 Avg. xx→en</td><td colspan="3">38.7 36.7 87.2</td><td colspan="3">46.2 88.4 44.7 88.4</td></tr><tr><td></td></table></body></html>  

Table 10: Full results for Group 4 in the Flores test data.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">en→fr</td><td colspan="3">en→id</td><td colspan="3">en→mg</td><td colspan="3">en→ms</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>43.5</td><td>84.5</td><td>73.7</td><td>32.7</td><td>78.8</td><td>64.6</td><td>1.6</td><td>47.0</td><td>11.2</td><td>34.0</td><td>86.9</td><td>75.9</td></tr><tr><td>NLLB-3.3B</td><td>51.1</td><td>88.3</td><td>76.9</td><td>46.4</td><td>91.2</td><td>77.9</td><td>17.7</td><td>81.6</td><td>59.9</td><td>41.6</td><td>89.1</td><td>76.8</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>42.0</td><td>86.8</td><td>75.7</td><td>38.0</td><td>89.7</td><td>77.7</td><td>4.4</td><td>64.9</td><td>33.3</td><td>35.0</td><td>88.3</td><td>76.7</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>41.2</td><td>86.4</td><td>74.6</td><td>35.6</td><td>89.0</td><td>74.1</td><td>2.4</td><td>56.8</td><td>24.4</td><td>32.5</td><td>87.4</td><td>73.7</td></tr><tr><td>Aya-101</td><td>38.3</td><td>85.3</td><td>69.5</td><td>38.7</td><td>90.0</td><td>77.7</td><td>16.1</td><td>81.1</td><td>60.8</td><td>30.7</td><td>86.3</td><td>68.3</td></tr><tr><td>Aya-23-8B</td><td>48.9</td><td>88.3</td><td>77.8</td><td>42.9</td><td>91.2</td><td>80.0</td><td>0.3</td><td>31.0</td><td>4.4</td><td>22.2</td><td>87.3</td><td>79.7</td></tr><tr><td>Aya-23-35B</td><td>49.0</td><td>88.0</td><td>77.1</td><td>43.5</td><td>91.1</td><td>79.4</td><td>0.8</td><td>41.4</td><td>16.4</td><td>26.7</td><td>87.2</td><td>77.4</td></tr><tr><td>X-ALMA (only SFT)</td><td>51.8</td><td>88.7</td><td>78.5</td><td>48.0</td><td>91.8</td><td>80.2</td><td>16.8</td><td>81.8</td><td>61.7</td><td>42.0</td><td>89.7</td><td>78.4</td></tr><tr><td>X-ALMA (Ours)</td><td>51.9</td><td>89.0</td><td>78.9</td><td>48.2</td><td>92.3</td><td>81.2</td><td>16.1</td><td>82.1</td><td>62.4</td><td>40.9</td><td>90.2</td><td>79.7</td></tr><tr><td></td><td colspan="4">en→th</td><td colspan="2">en→vi</td><td colspan="3">Avg. en→xx</td><td colspan="3"></td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td></td><td></td><td></td></tr><tr><td>Llama-3.1</td><td>3.4</td><td>73.6</td><td>53.2</td><td>37.7</td><td>87.1</td><td>74.5</td><td>25.5</td><td>76.3</td><td>58.9</td><td></td><td></td><td></td></tr><tr><td>NLLB-3.3B</td><td>5.3</td><td>84.3</td><td>71.5</td><td>41.8</td><td>88.0</td><td>75.4</td><td>34.0</td><td>87.1</td><td>73.1</td><td></td><td></td><td></td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>6.0</td><td>82.5</td><td>69.7</td><td>34.9</td><td>86.7</td><td>74.6</td><td>26.7</td><td>83.1</td><td>67.9</td><td></td><td></td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>3.7</td><td>84.8</td><td>72.2</td><td>34.9</td><td>86.0</td><td>71.7</td><td>25.0</td><td>81.7</td><td>65.1</td><td></td><td></td><td></td></tr><tr><td>Aya-101</td><td>9.8</td><td>86.5</td><td>74.9</td><td>31.9</td><td>85.6</td><td>71.2</td><td>27.6</td><td>85.8</td><td>70.4</td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>0.7</td><td>61.0</td><td>54.0</td><td>40.3</td><td>89.0</td><td>78.1</td><td>25.9</td><td>74.6</td><td>62.3</td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>6.1</td><td>63.2</td><td>39.9</td><td>40.4</td><td>89.2</td><td>77.9</td><td>27.7</td><td>76.7</td><td>61.3</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (only SFT)</td><td>11.6 12.0</td><td>87.4</td><td>76.1</td><td>43.9</td><td>89.4</td><td>78.5</td><td>36.1</td><td>88.2</td><td>75.8</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (Ours)</td><td></td><td>88.2 fr→en</td><td>77.4</td><td>44.1</td><td>89.9</td><td>79.3</td><td>35.6</td><td>88.6</td><td>76.5</td><td></td><td></td><td></td></tr><tr><td colspan="3"></td><td></td><td colspan="3">id→en</td><td></td><td colspan="2">mg→en</td><td></td><td colspan="2">ms→→en</td></tr><tr><td>Llama-3.1</td><td>BLEU 40.1</td><td>COMET-22 81.6</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>NLLB-3.3B</td><td>38.1</td><td>86.6</td><td>71.6</td><td>21.9</td><td>70.6</td><td>53.2</td><td>1.8</td><td>41.6</td><td>17.1</td><td>10.8</td><td>63.4</td><td>35.7</td></tr><tr><td></td><td>42.1</td><td>88.8</td><td>72.7</td><td>34.3</td><td>84.5</td><td>68.5</td><td>13.5 15.4</td><td>63.3</td><td>43.5</td><td>31.4</td><td>82.1</td><td>65.3</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>41.6</td><td>88.7</td><td>77.3 76.8</td><td>40.4 40.8</td><td>88.9</td><td>78.2</td><td>19.6</td><td>71.8</td><td>56.7</td><td>40.2</td><td>88.3</td><td>77.1</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>41.2</td><td>88.6</td><td>77.0</td><td></td><td>89.0</td><td>78.2</td><td>27.7</td><td>76.0</td><td>60.6</td><td>41.3</td><td>88.6</td><td>77.0</td></tr><tr><td>Aya-101</td><td>45.3</td><td>89.4</td><td>77.4</td><td>38.8</td><td>88.4</td><td>75.3</td><td></td><td>79.8</td><td>61.5</td><td>39.0</td><td>87.8</td><td>73.8</td></tr><tr><td>Aya-23-8B</td><td>47.0</td><td>89.5</td><td></td><td>44.1</td><td>89.5</td><td>78.5</td><td>1.5</td><td>47.0</td><td>18.8</td><td>40.0</td><td>87.3</td><td>75.9</td></tr><tr><td>Aya-23-35B</td><td>47.8</td><td>89.6</td><td>77.0 77.3</td><td>45.7 47.3</td><td>89.8 89.6</td><td>78.4</td><td>5.3</td><td>54.1</td><td>33.0 63.3</td><td>43.9</td><td>88.7</td><td>77.0 77.0</td></tr><tr><td>X-ALMA (only SFT) X-ALMA (Ours)</td><td colspan="4">46.0 89.6</td><td colspan="2">78.2 29.2</td><td colspan="3">30.1 81.9</td><td colspan="3">46.9 89.1</td></tr></table></body></html>  

Table 11: Full results for Group 5 in the Flores test data.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">en→cs</td><td colspan="3">en→el</td><td colspan="3">en→hu</td><td colspan="3">en→1t</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>27.1</td><td>88.0</td><td>73.5</td><td>19.7</td><td>82.3</td><td>70.4</td><td>19.8</td><td>82.2</td><td>70.4</td><td>13.0</td><td>77.2</td><td>60.6</td></tr><tr><td>NLLB-3.3B</td><td>32.2</td><td>91.0</td><td>77.7</td><td>27.4</td><td>89.0</td><td>76.6</td><td>26.4</td><td>89.3</td><td>78.7</td><td>25.2</td><td>89.3</td><td>77.3</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>26.0</td><td>89.1</td><td>75.3</td><td>19.9</td><td>86.4</td><td>74.5</td><td>19.1</td><td>87.0</td><td>74.9</td><td>19.0</td><td>87.0</td><td>74.0</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>24.6</td><td>88.1</td><td>73.4</td><td>20.4</td><td>86.2</td><td>74.2</td><td>18.2</td><td>86.6</td><td>73.7</td><td>17.0</td><td>86.1</td><td>72.8</td></tr><tr><td>Aya-101</td><td>26.7</td><td>90.0</td><td>77.2</td><td>21.4</td><td>86.6</td><td>74.3</td><td>21.4</td><td>88.4</td><td>78.6</td><td>22.5</td><td>89.2</td><td>78.6</td></tr><tr><td>Aya-23-8B</td><td>30.5</td><td>91.1</td><td>79.1</td><td>26.1</td><td>89.5</td><td>80.1</td><td>3.6</td><td>51.7</td><td>21.4</td><td>5.4</td><td>65.4</td><td>42.0</td></tr><tr><td>Aya-23-35B</td><td>32.2</td><td>91.4</td><td>79.4</td><td>27.0</td><td>89.6</td><td>80.2</td><td>10.8</td><td>77.0</td><td>57.2</td><td>14.0</td><td>82.5</td><td>68.0</td></tr><tr><td>X-ALMA (only SFT)</td><td>33.8</td><td>91.5</td><td>79.4</td><td>27.9</td><td>89.8</td><td>80.3</td><td>27.0</td><td>90.4</td><td>82.2</td><td>28.4</td><td>91.3</td><td>81.9</td></tr><tr><td>X-ALMA (Ours)</td><td>34.4</td><td>92.1</td><td>80.3</td><td>28.7</td><td>90.1</td><td>80.6</td><td>27.3</td><td>90.7</td><td>82.7</td><td>28.3</td><td>91.5</td><td>78.9</td></tr><tr><td></td><td colspan="3">en→1v en→p1</td><td colspan="3"></td><td colspan="3"></td><td colspan="3"></td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>Avg. en→xx COMET-22</td><td>XCOMET</td><td></td><td></td><td></td></tr><tr><td>Llama-3.1</td><td>11</td><td>70.7</td><td></td><td>14.2</td><td>73.5</td><td>57.1</td><td>17.5</td><td>79.0</td><td>62.2</td><td></td><td></td><td></td></tr><tr><td>NLLB-3.3B</td><td>25.0</td><td>87.4</td><td>70.9</td><td>21.6</td><td>88.9</td><td>75.5</td><td>26.3</td><td>89.2</td><td>76.1</td><td></td><td></td><td></td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>22.3</td><td>86.8</td><td>70.6</td><td>18.3</td><td>87.5</td><td>73.9</td><td>20.8</td><td>87.3</td><td>73.9</td><td></td><td></td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>21.1</td><td>85.8</td><td>68.9</td><td>17.2</td><td>86.7</td><td>71.9</td><td>19.8</td><td>86.6</td><td>72.5</td><td></td><td></td><td></td></tr><tr><td>Aya-101</td><td>25.0</td><td>88.6</td><td>74.8</td><td>18.3</td><td>87.6</td><td>75.0</td><td>22.6</td><td>88.4</td><td>76.4</td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>1.5</td><td>36.5</td><td>7.3</td><td>20.7</td><td>89.2</td><td>77.2</td><td>14.6</td><td>70.6</td><td>51.2</td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>7.9</td><td>62.7</td><td>38.1</td><td>22.4</td><td>89.8</td><td>78.1</td><td>19.1</td><td>82.1</td><td>66.8</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (only SFT)</td><td>29.3 30.8</td><td>90.7 91.1</td><td>78.5 79.3</td><td>23.3</td><td>90.1</td><td>78.9 79.3</td><td>28.3</td><td>90.6</td><td>80.2 80.2</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (Ours)</td><td colspan="3"></td><td colspan="3">23.2 90.4</td><td colspan="3">28.8 91.0</td><td colspan="3"></td></tr><tr><td></td><td></td><td>cs→en COMET-22</td><td></td><td></td><td>el→en</td><td></td><td></td><td>hu→en</td><td></td><td></td><td>1t→en</td><td></td></tr><tr><td></td><td>BLEU 22.1</td><td>64.9</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>29.4</td><td>80.1</td><td>64.9 59.9</td><td>10.3 33.0</td><td>57.4</td><td>43.0 75.3</td><td>8.6 14.0</td><td>54.8 70.1</td><td>52.9 43.7</td><td>3.3 12.6</td><td>49.2</td><td>41.8</td></tr><tr><td>NLLB-3.3B</td><td>37.6</td><td>87.9</td><td>77.8</td><td>24.5</td><td>86.1 77.2</td><td>71.5</td><td>32.2</td><td>87.5</td><td>78.2</td><td>29.7</td><td>67.1 85.1</td><td>38.9 73.6</td></tr><tr><td>LLaMAX2-Alpaca-7B LLaMAX3-Alpaca-8B</td><td>37.5</td><td>88.1</td><td>77.8</td><td>34.2</td><td>87.5</td><td>77.5</td><td>32.5</td><td>87.8</td><td>78.6</td><td>31.0</td><td>86.0</td><td>74.4</td></tr><tr><td></td><td>35.6</td><td>87.6</td><td>76.4</td><td>32.1</td><td>86.5</td><td>75.3</td><td>29.9</td><td>86.4</td><td>74.6</td><td>30.2</td><td>85.8</td><td></td></tr><tr><td>Aya-101 Aya-23-8B</td><td>40.7</td><td>88.5</td><td>78.1</td><td>36.1</td><td></td><td>77.8</td><td>23.0</td><td></td><td>67.2</td><td>24.6</td><td></td><td>74.2</td></tr><tr><td>Aya-23-35B</td><td>42.3</td><td>88.5</td><td>77.9</td><td>39.0</td><td>87.8</td><td></td><td></td><td>81.1</td><td></td><td></td><td>80.6</td><td>65.6</td></tr><tr><td>X-ALMA (only SFT)</td><td>43.3</td><td>89.0</td><td>78.4</td><td>38.0</td><td>88.3 87.9</td><td>78.0 77.0</td><td>32.2</td><td>86.5</td><td>76.6 78.9</td><td>32.9</td><td>85.4</td><td>73.5 75.1</td></tr><tr><td>X-ALMA (Ours)</td><td colspan="3">42.6 89.2 78.8</td><td colspan="3">37.4 88.3 78.3</td><td colspan="3">37.3 88.7 37.6 89.1</td><td colspan="3">35.9 87.1</td></tr><tr><td>1v→en</td></table></body></html>  

Table 12: Full results for Group 6 in the Flores test data.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="4">en→et</td><td colspan="2">en→fi</td><td colspan="3">en→ja</td><td colspan="3">en→ka</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>9.3</td><td>66.3</td><td>48.1</td><td>16.0</td><td>82.0</td><td>70.4</td><td>10.8</td><td>66.0</td><td>25.0</td><td>7.0</td><td>73.1</td><td>48.1</td></tr><tr><td>NLLB-3.3B</td><td>25.0</td><td>90.5</td><td>79.7</td><td>24.1</td><td>91.7</td><td>81.1</td><td>22.6</td><td>87.9</td><td>75.1</td><td>14.8</td><td>84.6</td><td>70.2</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>19.0</td><td>88.4</td><td>76.1</td><td>18.4</td><td>90.2</td><td>79.6</td><td>28.1</td><td>88.9</td><td>78.1</td><td>10.7</td><td>83.0</td><td>68.2</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>18.1</td><td>87.7</td><td>75.1</td><td>17.5</td><td>89.3</td><td>77.6</td><td>27.5</td><td>89.0</td><td>77.3</td><td>9.6</td><td>78.6</td><td>55.9</td></tr><tr><td>Aya-101</td><td>21.9</td><td>90.7</td><td>81.3</td><td>18.9</td><td>90.3</td><td>78.9</td><td>27.3</td><td>89.0</td><td>77.4</td><td>11.3</td><td>85.3</td><td>72.1</td></tr><tr><td>Aya-23-8B</td><td>1.5</td><td>40.5</td><td>12.9</td><td>2.4</td><td>51.9</td><td>21.8</td><td>30.7</td><td>90.8</td><td>80.3</td><td>0.4</td><td>43.3</td><td>32.8</td></tr><tr><td>Aya-23-35B</td><td>6.1</td><td>57.8</td><td>33.0</td><td>8.1</td><td>70.0</td><td>46.3</td><td>30.9</td><td>91.0</td><td>80.3</td><td>2.0</td><td>47.6</td><td>19.0</td></tr><tr><td>X-ALMA (only SFT)</td><td>26.4</td><td>91.6</td><td>82.6</td><td>25.3</td><td>92.7</td><td>84.6</td><td>34.6</td><td>91.2</td><td>81.0</td><td>14.0</td><td>87.6</td><td>75.7</td></tr><tr><td>X-ALMA(Ours)</td><td>27.9</td><td>92.2</td><td>84.0</td><td>26.4</td><td>92.9</td><td>85.2</td><td>36.6</td><td>91.7</td><td>81.9</td><td>15.2</td><td>88.5</td><td></td></tr><tr><td></td><td></td><td>en→ko</td><td></td><td></td><td>en→zh</td><td></td><td></td><td>Avg. en→xx</td><td></td><td></td><td></td><td>76.9</td></tr><tr><td colspan="3">BLEU</td><td>XCOMET BLEU COMET-22 XCOMET</td><td colspan="8">BLEU COMET-22</td></tr><tr><td>Llama-3.1</td><td>6.8</td><td>COMET-22 71.5</td><td>41.7</td><td>14.0</td><td>67.7</td><td>37.6</td><td>10.6</td><td>71.1</td><td>XCOMET 45.1</td><td></td><td></td><td></td></tr><tr><td>NLLB-3.3B</td><td>12.5</td><td>88.4</td><td>79.1</td><td>32.4</td><td>82.0</td><td>64.1</td><td>21.9</td><td>87.5</td><td>74.9</td><td></td><td></td><td></td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>10.3</td><td>86.8</td><td>76.7</td><td>35.2</td><td>85.5</td><td>74.0</td><td>20.3</td><td>87.1</td><td>75.4</td><td></td><td></td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>8.8</td><td>85.6</td><td>74.0</td><td>36.3</td><td>85.6</td><td>73.3</td><td>19.6</td><td>86.0</td><td>72.2</td><td></td><td></td><td></td></tr><tr><td>Aya-101</td><td>10.2</td><td>87.4</td><td>77.2</td><td>27.3</td><td>82.4</td><td>64.7</td><td>19.5</td><td>87.5</td><td>75.3</td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>13.1</td><td>89.0</td><td>79.5</td><td>40.2</td><td>87.3</td><td>76.8</td><td>14.7</td><td>67.1</td><td>50.7</td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>12.8</td><td>89.4</td><td>80.3</td><td>37.3</td><td>87.5</td><td>77.0</td><td>16.2</td><td>73.9</td><td>56.0</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (only SFT)</td><td>15.0</td><td>89.3</td><td>80.7</td><td>43.6</td><td>88.2</td><td>77.4</td><td>26.5</td><td>90.1</td><td>80.3</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (Ours)</td><td>15.8</td><td>89.9</td><td>81.6</td><td>44.9</td><td>88.7</td><td>78.4</td><td>27.8</td><td>90.6</td><td>81.3</td><td></td><td></td><td></td></tr><tr><td colspan="2"></td><td>et→en</td><td></td><td></td><td>fi→en</td><td></td><td></td><td>ja→en</td><td></td><td></td><td>ka→en</td><td></td></tr><tr><td colspan="2"></td><td>BLEU COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>4.1</td><td>45.9</td><td>51.6</td><td>5.8</td><td>53.3</td><td>52.8</td><td>17.7</td><td>69.4</td><td>67.6</td><td>0.3</td><td>39.2</td><td>34.3</td></tr><tr><td>NLLB-3.3B</td><td>7.2</td><td>62.5</td><td>29.9</td><td>10.2</td><td>67.7</td><td>39.2</td><td>17.2</td><td>79.5</td><td>61.0</td><td>25.6</td><td>84.8</td><td>70.3</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>32.1</td><td>87.4</td><td>80.7</td><td>31.7</td><td>89.0</td><td>78.9</td><td>23.4</td><td>87.1</td><td>76.4</td><td>18.1</td><td>76.7</td><td>66.8</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>33.6</td><td>88.3</td><td>82.2</td><td>31.6</td><td>89.3</td><td>79.4</td><td>24.6</td><td>87.5</td><td>76.9</td><td>1.2</td><td>50.7</td><td>51.5</td></tr><tr><td>Aya-101</td><td>32.5</td><td>87.7</td><td>80.2</td><td>29.7</td><td>88.6</td><td>77.8</td><td>23.5</td><td>86.5</td><td>75.5</td><td>25.6</td><td>84.5</td><td>69.7</td></tr><tr><td>Aya-23-8B</td><td>15.4</td><td>74.9</td><td>55.9</td><td>20.4</td><td>81.3</td><td>66.3</td><td>28.1</td><td>87.9</td><td>76.8</td><td>3.6</td><td>60.1</td><td>32.9</td></tr><tr><td>Aya-23-35B</td><td>28.9</td><td>84.2</td><td>74.5</td><td>29.8</td><td>87.3</td><td>76.2</td><td>30.4</td><td>88.4</td><td>77.1</td><td>19.4</td><td>79.4</td><td>65.0</td></tr><tr><td>X-ALMA (only SFT) X-ALMA(Ours)</td><td>38.2 38.8</td><td>89.2</td><td>82.5</td><td>36.0</td><td>90.0</td><td>79.6</td><td>28.9</td><td>88.1</td><td>77.0</td><td>28.4</td><td>86.8 87.1</td><td>71.6 72.8</td></tr><tr><td colspan="2"></td><td>89.6 ko→en</td><td>83.5</td><td>36.2</td><td>90.5 zh→en</td><td>80.4</td><td>28.8</td><td>88.5 Avg. xx→en</td><td>77.7</td><td>29.3</td></table></body></html>  

Table 13: Full results for Group 7 in the Flores test data.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">en→gu</td><td colspan="3">en→hi</td><td colspan="3">en→mr</td><td colspan="3">en→ne</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>611</td><td>81.7</td><td>55.7</td><td>21.7</td><td>69.6</td><td>53.5</td><td>7.6</td><td>62.4</td><td>49.8</td><td>4.3</td><td>59.6</td><td>47.5</td></tr><tr><td>NLLB-3.3B</td><td>24.3</td><td>87.2</td><td>66.2</td><td>34.4</td><td>80.9</td><td>67.7</td><td>17.1</td><td>74.3</td><td>65.6</td><td>16.4</td><td>76.5</td><td>76.8</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>11.0</td><td>78.9</td><td>40.6</td><td>24.8</td><td>76.9</td><td>62.0</td><td>11.1</td><td>70.7</td><td>62.9</td><td>13.8</td><td>80.8</td><td>86.8</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>13.7</td><td>82.7</td><td>57.3</td><td>23.5</td><td>76.6</td><td>61.9</td><td>10.1</td><td>69.5</td><td>63.1</td><td>10.7</td><td>78.4</td><td>83.0</td></tr><tr><td>Aya-101</td><td>15.6</td><td>83.9</td><td>60.8</td><td>21.4</td><td>75.5</td><td>57.7</td><td>10.3</td><td>69.5</td><td>60.2</td><td>10.5</td><td>77.5</td><td>79.1</td></tr><tr><td>Aya-23-8B</td><td>0.4</td><td>65.7</td><td>64.2</td><td>25.0</td><td>79.3</td><td>64.9</td><td>0.9</td><td>66.7</td><td>64.5</td><td>1.5</td><td>69.2</td><td>64.4</td></tr><tr><td>Aya-23-35B</td><td>1.5</td><td>62.2</td><td>51.1</td><td>26.0</td><td>79.1</td><td>65.6</td><td>1.3</td><td>61.1</td><td>56.3</td><td>1.4</td><td>68.3</td><td>64.0</td></tr><tr><td>X-ALMA (only SFT)</td><td>25.0</td><td>88.2</td><td>67.9</td><td>34.3</td><td>81.4</td><td>67.6</td><td>18.0</td><td>75.9</td><td>68.3</td><td>21.5</td><td>84.0</td><td>89.5</td></tr><tr><td>X-ALMA (Ours)</td><td>24.7</td><td>88.9</td><td>68.9</td><td>34.1</td><td>81.9</td><td>68.4</td><td>17.9</td><td>76.5</td><td>69.3</td><td>21.5</td><td>84.7</td><td>90.7</td></tr><tr><td></td><td colspan="3">en→ur</td><td colspan="3">Avg. en→xx</td><td colspan="3"></td><td colspan="3"></td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-3.1</td><td></td><td>77.0</td><td>66.2</td><td>12.1</td><td>70.1</td><td>54.5</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NLLB-3.3B</td><td>22.9</td><td>81.3</td><td>71.9</td><td>23.0</td><td>80.1</td><td>69.6</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>16.5</td><td>77.8</td><td>68.3</td><td>15.5</td><td>77.0</td><td>64.1</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>13.4</td><td>75.6</td><td>64.8</td><td>14.3</td><td>76.5</td><td>66.0</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-101</td><td>13.9</td><td>74.6</td><td>58.5</td><td>14.3</td><td>76.2</td><td>63.3</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>0.3</td><td>63.6</td><td>64.6</td><td>5.6</td><td>68.9</td><td>64.5</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>2.4 23.8</td><td>39.1</td><td>21.2</td><td>6.5</td><td>61.9</td><td>51.6</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>X-ALMA (only SFT)</td><td>24.0</td><td>83.5 84.1</td><td>75.0 75.8</td><td>24.5 24.5</td><td>82.6</td><td>73.6 74.6</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>X-ALMA (Ours)</td><td colspan="3">gu→en</td><td colspan="3">83.2</td><td colspan="3"></td><td colspan="3"></td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td></td><td></td><td>hi→en</td><td></td><td></td><td>mr→en</td><td></td><td></td><td>ne→en</td><td></td></tr><tr><td></td><td>1.8</td><td>46.2</td><td>XCOMET 39.2</td><td>BLEU 9.2</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1 NLLB-3.3B</td><td>42.3</td><td>90.2</td><td>73.5</td><td>38.7</td><td>53.7 88.9</td><td>35.8 68.3</td><td>3.2 34.0</td><td>45.1 87.0</td><td>31.1 70.1</td><td>1.6 38.0</td><td>45.5</td><td>43.4</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>0.0</td><td>42.7</td><td>47.9</td><td>27.3</td><td>81.7</td><td>63.4</td><td>23.5</td><td>79.9</td><td>66.0</td><td>26.0</td><td>89.7</td><td>90.9 86.6</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>9.9</td><td>66.0</td><td>60.7</td><td>35.4</td><td>88.9</td><td>67.9</td><td>30.6</td><td>87.3</td><td>70.1</td><td>32.9</td><td>83.4</td><td></td></tr><tr><td>Aya-101</td><td>28.0</td><td>82.3</td><td>65.0</td><td>34.6</td><td>87.5</td><td>66.6</td><td>30.1</td><td>85.2</td><td></td><td>31.2</td><td>89.3</td><td>90.3</td></tr><tr><td>Aya-23-8B</td><td>3.4</td><td>53.6</td><td>38.6</td><td>37.6</td><td>89.1</td><td>67.9</td><td>7.5</td><td></td><td>68.9</td><td></td><td>84.9</td><td>86.1</td></tr><tr><td>Aya-23-35B</td><td>8.8</td><td>63.1</td><td>53.0</td><td>40.1</td><td></td><td></td><td></td><td>68.9</td><td>46.2</td><td>10.0</td><td>77.0</td><td>68.1</td></tr><tr><td>X-ALMA (only SFT)</td><td>40.4</td><td>90.1</td><td>72.3</td><td>43.0</td><td>89.6 89.8</td><td>68.2 67.7</td><td>18.4</td><td>79.9 88.5</td><td>59.9 70.4</td><td>23.3</td><td>84.1</td><td>81.7 90.9</td></tr><tr><td>X-ALMA (Ours)</td><td colspan="3">40.6 90.3 72.7</td><td colspan="3">42.7 90.1 68.4</td><td colspan="3">37.7 37.7 88.6</td><td colspan="3">41.2 90.6 41.4 90.7</td></tr><tr><td></td></table></body></html>  

Table 14: Full results for Group 8 in the Flores test data   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">en→ar</td><td colspan="3">en→az</td><td colspan="3">en→fa</td><td colspan="3">en→he</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>18.9</td><td>82.9</td><td>70.0</td><td>6.6</td><td>69.6</td><td>50.7</td><td>21.4</td><td>86.1</td><td>77.3</td><td>20.8</td><td>85.3</td><td>73.5</td></tr><tr><td>NLLB-3.3B</td><td>27.5</td><td>86.3</td><td>75.2</td><td>14.0</td><td>86.9</td><td>76.6</td><td>22.6</td><td>86.5</td><td>77.5</td><td>30.4</td><td>87.8</td><td>76.1</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>19.7</td><td>84.9</td><td>73.7</td><td>9.4</td><td>82.0</td><td>68.2</td><td>17.8</td><td>83.7</td><td>73.4</td><td>22.0</td><td>85.1</td><td>72.7</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>14.1</td><td>82.2</td><td>69.1</td><td>7.3</td><td>80.0</td><td>65.9</td><td>17.7</td><td>84.5</td><td>74.0</td><td>23.3</td><td>86.2</td><td>74.7</td></tr><tr><td>Aya-101</td><td>17.2</td><td>84.1</td><td>72.8</td><td>11.5</td><td>85.6</td><td>75.4</td><td>19.1</td><td>86.4</td><td>77.6</td><td>20.5</td><td>85.4</td><td>73.2</td></tr><tr><td>Aya-23-8B</td><td>26.5</td><td>87.3</td><td>77.3</td><td>2.0</td><td>75.5</td><td>69.1</td><td>23.2</td><td>87.7</td><td>79.4</td><td>27.0</td><td>88.3</td><td>77.7</td></tr><tr><td>Aya-23-35B</td><td>27.4</td><td>87.1</td><td>76.6</td><td>3.0</td><td>67.2</td><td>54.8</td><td>23.8</td><td>87.6</td><td>79.1</td><td>28.9</td><td>88.2</td><td>77.0</td></tr><tr><td>X-ALMA (only SFT)</td><td>29.1</td><td>87.8</td><td>77.6</td><td>14.0</td><td>88.2</td><td>79.0</td><td>28.4</td><td>88.5</td><td>80.2</td><td>32.7</td><td>89.6</td><td>79.6</td></tr><tr><td>X-ALMA (Ours)</td><td>28.3</td><td>88.2</td><td>78.3</td><td>14.0</td><td>88.4</td><td>79.3</td><td>27.1</td><td>88.8</td><td>80.8</td><td>33.6</td><td>89.8</td><td>79.5</td></tr><tr><td></td><td colspan="4">en→kk</td><td colspan="4">en→ky</td><td colspan="4"></td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>en→uz COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>8.2</td><td>77.7</td><td>61.8</td><td>3.7</td><td>58.5</td><td>39.4</td><td>19.1</td><td>84.2</td><td>69.5</td><td>9.3</td><td>81.9</td><td>66.1</td></tr><tr><td>NLLB-3.3B</td><td>20.6</td><td>90.0</td><td>78.9</td><td>13.2</td><td>88.1</td><td>74.7</td><td>29.0</td><td>89.7</td><td>76.1</td><td>18.6</td><td>89.8</td><td>75.6</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>13.0</td><td>86.5</td><td>74.7</td><td>8.2</td><td>84.0</td><td>71.9</td><td>16.2</td><td>85.1</td><td>69.4</td><td>10.1</td><td>85.1</td><td>69.3</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>12.7</td><td>86.0</td><td>75.1</td><td>7.9</td><td>82.9</td><td>72.5</td><td>13.8</td><td>84.2</td><td>67.5</td><td>6.8</td><td>74.5</td><td>59.1</td></tr><tr><td>Aya-101</td><td>17.2</td><td>89.0</td><td>78.3</td><td>10.4</td><td>86.6</td><td>75.6</td><td>21.1</td><td>88.3</td><td>75.0</td><td>12.0</td><td>88.6</td><td>75.6</td></tr><tr><td>Aya-23-8B</td><td>1.2</td><td>71.0</td><td>77.0</td><td>1.2</td><td>62.9</td><td>68.9</td><td>23.7</td><td>88.9</td><td>75.8</td><td>0.5</td><td>46.5</td><td>26.6</td></tr><tr><td>Aya-23-35B</td><td>0.7</td><td>45.0</td><td>21.9</td><td>0.9</td><td>49.6</td><td>34.4</td><td>23.6</td><td>88.7</td><td>74.5</td><td>0.3</td><td>37.1</td><td>17.0</td></tr><tr><td>X-ALMA (only SFT) X-ALMA (Ours)</td><td>22.2</td><td>90.7</td><td>80.8</td><td>13.2</td><td>88.5</td><td>78.5</td><td>27.7</td><td>90.3</td><td>78.3</td><td>16.8</td><td>90.0</td><td>77.0</td></tr><tr><td></td><td>22.0</td><td>91.1</td><td>81.4</td><td>12.8</td><td>88.8</td><td>78.8</td><td>28.4</td><td>90.5</td><td>78.6</td><td>15.5</td><td>90.1</td><td>77.3</td></tr><tr><td colspan="9">Avg. en→xx</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>BLEU 13.5</td><td>COMET-22</td><td>XCOMET</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Llama-3.1</td><td></td><td>78.3</td><td>63.5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>NLLB-3.3B</td><td>22.0</td><td>88.1</td><td>76.3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>14.5</td><td>84.6</td><td>71.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>12.9</td><td>82.6</td><td>69.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-101</td><td>16.1</td><td>86.8</td><td>75.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>13.2</td><td>76.0</td><td>69.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>13.6</td><td>68.8</td><td>54.4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>X-ALMA (only SFT)</td><td>23.0</td><td>89.2</td><td>78.9 79.3</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>X-ALMA(Ours)</td><td>22.7</td><td colspan="2">89.4 ar→en</td><td colspan="4"></td><td colspan="2">fa→en</td><td colspan="3"</table></body></html>  

<html><body><table><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>Llama-3.1</td><td>9.9</td><td>53.7</td><td>41.4</td></tr><tr><td>NLLB-3.3B</td><td>24.3</td><td>79.5</td><td>63.3</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>27.5</td><td>85.1</td><td>73.9</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>28.3</td><td>84.7</td><td>71.4</td></tr><tr><td>Aya-101</td><td>29.7</td><td>85.9</td><td>74.2</td></tr><tr><td>Aya-23-8B</td><td>22.4</td><td>76.7</td><td>59.8</td></tr><tr><td>Aya-23-35B</td><td>28.4</td><td>82.7</td><td>69.6</td></tr><tr><td>X-ALMA (only SFT)</td><td>34.8</td><td>87.5</td><td>75.1</td></tr><tr><td>X-ALMA (Ours)</td><td>35.2</td><td>88.0</td><td>76.0</td></tr></table></body></html>  

Table 15: Full results for all languages in the WMT'23 test data.   


<html><body><table><tr><td rowspan="2">Models</td><td colspan="3">en→de</td><td colspan="3">en→zh</td><td colspan="3">en→ja</td><td colspan="3">en→ru</td></tr><tr><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td></tr><tr><td>ALMA-R-13B</td><td>30.4</td><td>84.0</td><td>68.8</td><td>32.3</td><td>85.0</td><td>71.3</td><td></td><td></td><td></td><td>22.8</td><td>85.5</td><td>74.4</td></tr><tr><td>Towerlnstruct-7B-v0.2</td><td>37.9</td><td>83.1</td><td>68.1</td><td>41.9</td><td>85.6</td><td>70.2</td><td></td><td></td><td>=</td><td>29.2</td><td>85.3</td><td>71.1</td></tr><tr><td>NLLB-3.3B</td><td>33.5</td><td>79.7</td><td>61.0</td><td>34.8</td><td>79.6</td><td>35.6</td><td>13.8</td><td>81.6</td><td>65.7</td><td>29.1</td><td>83.8</td><td>669</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>18.6</td><td>74.1</td><td>59.8</td><td>39.8</td><td>82.6</td><td>64.6</td><td>15.4</td><td>83.4</td><td>70.7</td><td>22.1</td><td>81.6</td><td>67.8</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>20.9</td><td>73.3</td><td>55.2</td><td>34.0</td><td>81.5</td><td>59.9</td><td>11.9</td><td>81.8</td><td>66.7</td><td>23.5</td><td>81.6</td><td>67.6</td></tr><tr><td>Aya-101</td><td>25.1</td><td>75.1</td><td>52.9</td><td>25.4</td><td>78.6</td><td>52.0</td><td>14.1</td><td>84.6</td><td>72.0</td><td>22.1</td><td>83.1</td><td>69.9</td></tr><tr><td>Aya-23-8B</td><td>29.3</td><td>80.4</td><td>66.2</td><td>44.5</td><td>85.3</td><td>68.8</td><td>19.3</td><td>86.5</td><td>75.1</td><td>24.3</td><td>84.3</td><td>71.8</td></tr><tr><td>Aya-23-35B</td><td>30.7</td><td>80.7</td><td>66.6 68.6</td><td>42.8 47.5</td><td>84.6</td><td>68.1</td><td>20.6</td><td>86.4</td><td>75.1</td><td>27.5</td><td>84.7</td><td>71.7</td></tr><tr><td>X-ALMA (only SFT)</td><td>40.9</td><td>84.1 84.4</td><td>69.4</td><td>47.9</td><td>86.1</td><td>69.6 71.3</td><td>22.3 22.7</td><td>86.8</td><td>75.4</td><td>31.5</td><td>85.9</td><td>73.4</td></tr><tr><td>X-ALMA (Ours)</td><td>39.4</td><td colspan="2">en→uk</td><td></td><td>86.7</td><td></td><td></td><td>87.5</td><td>77.1</td><td>31.5</td><td>86.3</td><td>74.0</td></tr><tr><td>BLEU</td><td colspan="3">COMET-22</td><td colspan="3">en→he</td><td colspan="3">Avg. en→xx</td><td colspan="3"></td></tr><tr><td>ALMA-R-13B Towerlnstruct-7B-v0.2</td><td></td><td></td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td></td><td></td><td></td></tr><tr><td>NLLB-3.3B</td><td>-</td><td>= 82.8</td><td>67.6</td><td>31.4</td><td>83.6 -</td><td>69.1</td><td>28.0</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>25.5</td><td>80.9</td><td>65.0</td><td>23.4</td><td>81.5</td><td>66.6</td><td>23.2</td><td>81.8 80.7</td><td>64.8 65.8</td><td></td><td></td><td></td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>20.0 19.8</td><td>80.6</td><td>64.9</td><td>24.4</td><td>82.5</td><td>68.5</td><td>22.4</td><td>80.2</td><td>63.8</td><td></td><td></td><td></td></tr><tr><td>Aya-101</td><td>19.7</td><td>82.7</td><td>67.8</td><td>19.8</td><td>82.0</td><td>67.2</td><td>21.0</td><td>81.0</td><td>63.6</td><td></td><td></td><td></td></tr><tr><td>Aya-23-8B</td><td>24.3</td><td>84.3</td><td>70.1</td><td>26.5</td><td>84.3</td><td>71.1</td><td>28.0</td><td>84.2</td><td>70.5</td><td></td><td></td><td></td></tr><tr><td>Aya-23-35B</td><td>24.9</td><td>84.0</td><td>69.6</td><td>29.3</td><td>84.1</td><td>70.5</td><td>29.3</td><td>84.1</td><td>70.3</td><td></td><td></td><td></td></tr><tr><td>X-ALMA (only SFT)</td><td>27.4</td><td>85.3</td><td>71.6</td><td>31.4</td><td>86.1</td><td>73.7</td><td>33.5</td><td>85.7</td><td>72.0</td><td></td><td></td><td></td></tr><tr><td>X-ALMA(Ours)</td><td>28.3</td><td>85.5</td><td>72.2</td><td>32.5</td><td>86.2</td><td>74.1</td><td>33.7</td><td>86.1</td><td>73.0</td><td></td><td></td><td></td></tr><tr><td></td><td colspan="4">de→en</td><td colspan="2">zh→en</td><td colspan="3"></td><td colspan="3"></td></tr><tr><td></td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>COMET-22</td><td>XCOMET</td><td>BLEU</td><td>ja→en COMET-22</td><td>XCOMET</td><td>BLEU</td><td>ru→en COMET-22</td><td>XCOMET</td></tr><tr><td>ALMA-R-13B</td><td>42.6</td><td>85.5</td><td>69.1</td><td>23.2</td><td>80.6</td><td>70.6</td><td></td><td></td><td></td><td>33.0</td><td>83.3</td><td>72.1</td></tr><tr><td>Towerlnstruct-7B-v0.2</td><td>39.8</td><td>84.6</td><td>69.3 一</td><td>23.9</td><td>80.5</td><td>70.4</td><td></td><td></td><td>-</td><td>34.1</td><td>83.1</td><td>72.1</td></tr><tr><td>NLLB-3.3B</td><td>20.1</td><td>66.6</td><td>20.1</td><td>11.4</td><td>67.8</td><td>41.6</td><td>6.8</td><td>65.8</td><td>41.5</td><td>24.4</td><td>76.7</td><td>62.2</td></tr><tr><td>LLaMAX2-Alpaca-7B</td><td>22.1</td><td>78.0</td><td>66.3</td><td>20.7</td><td>78.3</td><td>68.1</td><td>16.9</td><td>79.5</td><td>68.5</td><td>29.6</td><td>81.1</td><td>70.3</td></tr><tr><td>LLaMAX3-Alpaca-8B</td><td>25.6</td><td>79.4</td><td>66.7</td><td>22.3</td><td>79.3</td><td>69.2</td><td>17.6</td><td>80.1</td><td>69.3</td><td>29.4</td><td>81.3</td><td>70.5</td></tr><tr><td>Aya-101</td><td>34.9</td><td>81.6 82.1</td><td>65.1</td><td>13.8</td><td>73.7</td><td>55.5</td><td>13.9</td><td>77.3</td><td>63.7</td><td>28.4</td><td>81.4</td><td>70.5</td></tr><tr><td>Aya-23-8B</td><td>32.3</td><td>82.3</td><td>67.9</td><td>22.6</td><td>78.8</td><td>68.0</td><td>19.8</td><td>80.2</td><td>68.6</td><td>30.9</td><td>81.6</td><td>70.8</td></tr><tr><td>Aya-23-35B</td><td>32.7</td><td></td><td>68.3 68.9</td><td>23.5 23.8</td><td>79.7 80.3</td><td>69.5 69.9</td><td>21.3 20.4</td><td>81.6 81.6</td><td>69.8 70.2</td><td>31.7 32.8</td><td>82.2 82.4</td><td>71.1 71.5</td></tr><tr><td>X-ALMA (only SFT) X-ALMA (Ours)</td><td>42.5 41.7</td><td>85.3 85.7</td></table></body></html>  

## F EXAMPLES OF OVER-REJECTION  

Figure 7 presents examples of over-rejection in translations from Chinese to English. For each source sentence, we provide translations from the reference, ARPO (implemented on CPO), and CPO. The words where ARPO and CPO differ from the reference are color-highlighted: green indicates that the variation does not affect the meaning, while red indicates a potentially negative impact on translation quality. As shown in Figure 7, CPO exhibits more stylistic variations than ARPO across all translation examples. Although most of the stylistic changes introduced by CPO are accurate and do not impair meaning, a small number are detrimental. Excessive changes in style can result in sub-optimal translations, a phenomenon we refer to as ^over-rejection'.  

### Example 1  

Source:除了一艘英国巡航舰外，其余船只悉数沉没。近200名美国人和德国人丧失生命。   
Reference: All the ships were sunk, except for one British cruiser. Nearly 200 American and German lives were lost.   
ARPO: All but one of the British cruisers were sunk, and nearly 200 Americans and Germans lost their lives.   
CPO: With the exception of a British cruiser, all other ships were sunk, and nearly 200 Americans and Germans lost their lives.  

### Example 2  

Source:2017年年末，西米诺夫出现在QVC电视销售频道。   
Reference: In late 2017, Siminoff appeared on shopping television channel QVC.   
ARPO: In late 2017, Siminoff appeared on the QVC television shopping channel.   
CPO: At the end of 2017, Zhukovsky appeared on the QVC television shopping channel.  

### Example 3  

Source:罗斯贝数越小，恒星在磁场倒转方面就越不活跃。   
Reference: The smaller the Rossby number, the less active the star with respect to magnetic reversals.   
ARPO: The lower the Rossby number, the less active a star is in flipping its magnetic field.   
CPO: The Rossby number is lower, the less active the star is in flipping its magnetic field.  

### Example 4  

Source:马略卡岛的菜肴和地中海相似地区的菜肴一样，主要以面包、蔬菜和肉类（尤其是猪肉）为基础，而且完全使用 橄榄油。   
Reference: Majorcan cuisine,like that of similar zones in the Mediterranean, is based onbread, vegetables and meat (specially pork), and uses olive oil throughout.   
ARPO: Mallorca's cuisine,like that of similar Mediteranean regions, is based primarily on bread, vegetables, and meat (particularly pork), and uses exclusively olive oil.   
CPO: The cuisine of Mallorca is similar to that of other Mediterranean regions, with dishes primarily based on bread, vegetables, and meat (particularly pork), and entirely using olive oil.  

### Example 5  

source:结果，演员在舞台上吸食大麻，剧院本身也鼓励观众一起加入。 Reference: As a result, the performers smoke cannabis joints on stage, and the theatre itself is encouraging the audience to join in ARPO: As a result, the actor smoked marijuana on stage, and the theater itself encouraged the audience to join in. CPO: As a result, the actors smoked cannabis on stage, and the theater itself encouraged the audience to participate in the act.  

Figure 7:  Examples of over-rejection in Chinese-to-English translation, comparing translations from the reference, ARPO, and CPO. Green highlights indicate acceptable variations, while red highlights show the potentially harmful changes. CPO introduces more stylistic differences than ARPO, with most being correct but some leading to over-rejection. Although most of variations are correct, the phenomenon of excessive stylistic changes leading to non-optimal translations is referred to as ‘over-rejection'.  