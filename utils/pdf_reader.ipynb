{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-03T06:10:11.861133600Z",
     "start_time": "2025-03-03T06:10:11.853743600Z"
    }
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def filter_content(text):\n",
    "    # 过滤掉以 '![]' 开头的行\n",
    "    # ^ 表示行开头，\\Q 和 \\E 用来转义 '[]' 中的特殊字符\n",
    "    text = re.sub(r'\\n\\n!\\[\\][^\\n]*', '', text)\n",
    "\n",
    "    # 过滤掉被 <html> 标签包裹的内容\n",
    "    # 匹配从 <html> 到 </html> 的所有内容，包括多行内容\n",
    "    text = re.sub(r'<html>[\\s\\S]*?</html>', '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def load_md(file_path, is_omit_ref_apx=True):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        sections = re.split(r'\\n(?=#+ )', content)\n",
    "        meta_information = {}\n",
    "        title2content = {}\n",
    "        for idx, sec in enumerate(sections):\n",
    "\n",
    "                position = sec.find('\\n\\n')\n",
    "                if position != -1 :\n",
    "                        header = sec[:position]\n",
    "                else:\n",
    "                        match = re.search(r'^(#{1,6})[\\s\\S]*?(?=\\n)', sec, re.MULTILINE)\n",
    "                        header = match.group()\n",
    "                \n",
    "                if idx == 0:\n",
    "                        meta_information['title'] = header\n",
    "                else:\n",
    "                        filtered_content = filter_content(sec[position:].strip())\n",
    "                        if len(filtered_content)!=0:\n",
    "                                title2content[header] = filtered_content\n",
    "                \n",
    "                if is_omit_ref_apx:\n",
    "                        if 'conclusion' in header.lower():\n",
    "                                break\n",
    "        meta_information['body'] = title2content\n",
    "        return meta_information"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-03T06:13:13.227992400Z",
     "start_time": "2025-03-03T06:13:13.176881400Z"
    }
   },
   "id": "53fa6682210738a2",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "file_path = \"/mnt/d/PycharmCode/LLMscratch/essay_searcher/data/mds/lexmatcher.md\"\n",
    "meta_information = load_md(file_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-03T06:13:15.528434800Z",
     "start_time": "2025-03-03T06:13:15.519180600Z"
    }
   },
   "id": "9105805484e6bc26",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'title': '# LexMatcher: Dictionary-centric Data Collection for LLM-based Machine Translation  ',\n 'body': {'## Abstract  ': 'The fine-tuning of open-source large language models (LLMs) for machine translation has recently received considerable attention, marking a shift towards data-centric research from traditional neural machine translation. However, the area of data collection for instruction finetuning in machine translation remains relatively underexplored. In this paper, we present LexMatcher, a simple yet effective method for data collection that leverages bilingual dictionaries to generate a dataset, the design of which is driven by the coverage of senses found in these dictionaries. The dataset comprises a subset retrieved from an existing corpus and a smaller synthesized subset which supplements the infrequent senses of polysemous words. Utilizing LLaMA2 as our base model, our approach outperforms the established baselines on the WMT2022 test sets and also exhibits significant performance improvements in tasks related to word sense disambiguation and specialized terminology translation. These results underscore the effectiveness of LexMatcher in enhancing LLM-based machine translation.',\n  '## 1 Introduction  ': 'The emergence of large language models (LLMs) has brought about new opportunities for machine translation. The modeling paradigm has gradually shifted from training sequence-to-sequence models from scratch to utilizing commercial or opensourced LLMs (Hendy et al., 2023; Agrawal et al., 2023; Zhu et al., 2023; Jia0 et al., 2023; Xu et al., 2024). Unlike traditional neural machine translation methods, which rely on abundant parallel sentences (Vaswani et al., 2017; Gordon et al., 2021; Fernandes et al., 2023) and monolingual sentences (Sennrich et al., 2016; Edunov et al., 2018), it has been shown that LLMs do not require much supervised fine-tuning data to achieve competitive translation, and the quality of fine-tuning data is crucial (Zhang et al., 2023; Xu et al., 2024).  \\n\\nCurrent research primarily focuses on constructing fine-tuning data by leveraging human-written development sets, as well as creating refined instruction data to improve performance, such as contrastive translation pairs and interactive translation (Jiao et al., 2023; Zeng et al., 2024; Zhang et al., 2023). In addition, some studies have explored post-training LLMs using extensive bilingual data (Yang et al., 2023; Wei et al., 2023). However, what kind of fine-tuning data is important for machine translation has not been thoroughly investigated yet. It has been demonstrated that fine-tuning LLMs with extensive parallel data can harm their intrinsic translation capabilities (Xu et al., 2024). Furthermore, recent studies emphasize that the quality of data distributions has a more significant impact on pretraining outcomes than quantity alone (Gunasekar et al., 2023; Li et al., 2023), with more uniform data distributions contributing to improved generalization for unseen compositions (Patel et al., 2022).  \\n\\nMotivated by the above observations, we investigate a principled method, LexMatcher, for curating supervised fine-tuning data for LLM-based translation. The objective is to collect a small yet carefully selected dataset that follows a proper distribution for maximizing translation quality. In particular, we leverage a bilingual dictionary as a pivotal resource to ensure comprehensive coverage of word or phrase senses in bilingual contexts. The construction of the dataset involves two steps: sense retrieval and sense supplement. In the sense retrieval step, we traverse commonly-used corpora (e.g., WMT training data), and identify sentence pairs that contain at least a segment pair that has not yet reached a matching threshold in the retrieved subset. To prioritize the selection of high-quality sentence pairs, we employ a quality estimation model to sort the data. Inevitably, there may be uncovered senses of polysemous words after the retrieval, representing crucial long-tail knowledge essential for accurate translation. To address this, we employ commercial LLMs (e.g., ChatGPT) to generate precise and concise demonstrations for the uncovered senses. Finally, we fine-tune LLMs using a combination of the retrieved and synthesized subsets.  \\n\\nWe primarily utilize the training data from WMT22 and conduct extensive experiments on six language directions, including $\\\\mathrm{Zh}{\\\\Leftrightarrow}\\\\mathrm{En}$ ， $\\\\mathrm{En{\\\\Longleftrightarrow}D e}$ ， and $\\\\mathtt{E n}{\\\\Longleftrightarrow}\\\\mathtt{R u}$ . By employing LexMatcher, we extract $0.1\\\\%$ of the data from the original corpus and utilize a maximum of only 1 million samples across all six language directions. The results of fine-tuned LLMs on the WMT22 test sets deliver superiority over the baselines in both common and zero-shot settings. The fine-tuned models also achieve comparable or better performance in terminology translation and translation disambiguation compared to the dedicated or commercial systems. Furthermore, the analyses of different data collection methods and composition generalization underscore the significance of high-quality data distributions. Finally, we showcase the complementarity of the collected parallel data with large-scale monolingual post-training by the experiment of fine-tuning ALMA (Xu et al., 2024). The code, data, and models are available at https://github.com/ARIESLM/Lexmatcher-MT.git.',\n  '## 2 Related Work  ': 'Data Selection for NMT. For traditional sequence-to-sequence neural machine translation (NMT) models, augmenting the volume of parallel data often leads to improvements in performance (Sennrich et al., 2016; Edunov et al., 2018; Gordon et al., 2021; Fernandes et al., 2023). Conversely, there have also been studies exploring data selection to reduce the size of the training corpus. For instance, van der Wees et al. (2017) gradually reduces the training data to a cleaner subset, determined by external scorers. Wang et al. (2018) introduce curriculum-based data selection that employs a trusted clean dataset to assess the noise level of each sample. Kumar et al. (2019) employ reinforcement learning to simultaneously learn a denoising curriculum and improve the NMT model. Mohiuddin et al. (2022) initially train a base NMT model on the entire available data and subsequently fine-tune the base model using selected subsets of the data. Compared to traditional NMT, data collection is more critical for LLM-based MT, yet it remains unexplored. We have taken the initiative to investigate it and propose a simple and practical method.  \\n\\nLLMs for MT. The usage of LLM-based MT is significantly different from the conventional NMT. LLMs, particularly large ones like GPT-4, serve as interfaces that can perform translation with simple translation instructions or ICL (Lin et al., 2022; Hendy et al., 2023; Zhu et al., 2023; Agrawal et al., 2022). For ICL, the influence of data selection methods on model performance is not significantly noticeable (Zhu et al., 2023; Agrawal et al., 2022; Lin et al., 2022). Fine-tuning open-source LLMs such as LLaMA (Touvron et al., 2023) for translation has garnered increasing attention (Jiao et al., 2023; Zhang et al., 2023). TIM (Zeng et al., 2024) constructs translation pairs for comparison and introduces an additional preference loss during SFT. Bayling (Zhang et al., 2023) automatically generates interactive translation instructions for tuning. (Mao and Yu, 2024) construct an additional cross-lingual discrimination task using word alignment for instruction fine-tuning in low-resource languages. Yang et al. (2023) fine-tune LLMs using more than 300 million parallel instances while Xu et al. (2024) indicate that such strategy could potentially impair the translation capabilities of LLMs. Instead, they propose a two-stage process that involves further post-training LLMs using a substantial amount of mixed monolingual data, followed by a subsequent step of SFT with humanwritten parallel data.  \\n\\nIn contrast, we are the first to propose specific parallel data collection methods, following the principle of achieving uniform coverage of semantic units in the dictionary. Moreover, our approach achieves a better balance between efficiency and performance, i.e., a high-quality translation model can be obtained using less computational resources than monolingual or bilingual post-training.  \\n\\nBilingual Dictionary for NMT. Bilingual dictionaries have been employed to enhance translation quality, particularly for rare words or domainspecific entities. One approach involves augmenting the training data with pseudo-parallel sentences generated based on the dictionary. For example, Zhao et al. (2020) enhance the parallel corpus with the help of paired entities extracted from multilingual knowledge graphs. Hu et al. (2022) propose denoising entity pretraining for NMT using monolingual data and paired entities. These methods do not consult bilingual dictionaries for translation candidates during the inference stage. Another approach involves leveraging bilingual alignments as lexical constraints (Li et al., 2022; Wang et al., 2022; Zeng et al., 2023). For LLMs, bilingual dictionaries have been used as a part of prompts (Lu et al., 2023; Ghazvininejad et al., 2023) for the LLMs more than 100B. In comparison, we aim to improve LLMs’ fine-tuning performance on translation tasks. The dictionaries serve as a pivot for data collection and can also be added in prompts whenneeded.  \\nFigure 1: Illustration of our LexMatcher for instruction fine-tuning.',\n  '## 3 Method  ': 'The overview of LexMatcher is illustrated in Figure 1. In brief, LexMatcher can generate a compact parallel dataset for instruction fine-tuning based on the provided dictionaries and corpus.',\n  '### 3.1 Sense Retrieval  ': 'Given  a  dictionary $\\\\begin{array}{r c l}{\\\\Phi}&{=}&{(s,t)}\\\\end{array}$ ， where $\\\\Phi=$ $\\\\left\\\\{{{\\\\left({{s_{1}},{t_{1}}}\\\\right)},{\\\\left({{s_{2}},{t_{2}}}\\\\right)},\\\\ldots,{\\\\left({{s_{n}},{t_{n}}}\\\\right)}}\\\\right\\\\}$ and each $(s_{i},t_{i})$ represents a source-target segment pair, we aim to ground each pair in parallel contexts by retrieving data from a parallel dataset $D=(x,y)$ . The dictionary $\\\\Phi$ shares the same source and target languages with $D$ . In certain cases, the segments can be phrases (e.g., “take over\\') or named entities (e.g., \"World Trade Organization\") in the dictionary. Ideally, the objective is to find a subset $S_{r}\\\\subseteq D$ such that:  \\n\\n$$\\n\\\\forall(s,t)\\\\in\\\\Phi,\\\\exists(x,y)\\\\in S_{r}:s\\\\subseteq x\\\\land t\\\\subseteq y,\\n$$  \\n\\nwhere $\\\\begin{array}{r l r}{x}&{{}=}&{\\\\{x_{1},x_{2},...,x_{|x|}\\\\}}\\\\end{array}$ and $y\\\\quad=$ $\\\\{y_{1},y_{2},...,y_{|y|}\\\\}$ . In practice, we cannot guarantee that the existing bilingual corpora can cover all senses in the dictionary, and we extract a subset that satisfies this objective to the full.  \\n\\nWe traverse the corpus in sequential order and search for potential matches with paired words in the dictionary. To prioritize the extraction of high-quality sentence pairs, we rank the corpus with model-based translation quality metrics, e.g., COMET-KIWI (Rei et al., 2022). Specifically, for each segmentl in a source sentence, we perform a dictionary lookup for all the aligned target words. If one of the aligned target segments exists in the target sentence, we put the sentence pair into the translation candidate subset $S_{r}$ We lemmatize each word in the source and target sentence to alleviate the effect of textual variations. In addition, we introduce a threshold $K$ to skip the sentence if all the segment pairs in it have already been matched $K$ times. $K$ enables convenient control over the size of the subset and is used to encourage the even distribution of segment pairs to some extent. The matching procedure is illustrated in Algorithm 1.',\n  '### 3.2 Sense Supplement  ': \"Using a partial set of open-source corpora cannot cover all the senses in the dictionary, and some senses may be included in the filtered low-quality data. The unseen senses could be named entities or simply low-frequency occurrences. The translation of rare entities is generally unique and can be solved effectively by prompting LLMs during inference, and the lack of training data for these cases may have minimal impact. In contrast, the senses of polysemous words are context-sensitive and may require specific training data to strengthen the model's understanding and translation of these words. To compensate for the missing senses, we leverage ChatGPT2 to construct translation demonstrations for each sense, thus creating the subset $S_{c}$ . Concretely, we prompt ChatGPT with a sense expressed in source and target languages and the sense's definition. The prompt is shown in Figure 6 (Appendix B). Only nouns and verbs\",\n  '# Algorithm 1 Sense Retrieval in LexMatcher  ': '1:Input:Parallel dataset $D$ , dictionary $\\\\Phi$ , thresh  \\nold $K$   \\n2:Output:Subset $S_{r}\\\\subseteq D$   \\n3: Initialize $S_{r}=\\\\emptyset$ ,frequency count $C=\\\\{\\\\}$   \\n4: for each $(x,y)\\\\in\\\\bar{D}$ do   \\n5: InitializeFound=false   \\n6: for each segment $\\\\hat{x_{i}}$ in Lemmatize $(x)$ do   \\n7: for each $t_{n}$ in $\\\\Phi[\\\\hat{x_{i}}]$ do   \\n8: if $C[(\\\\hat{x_{i}},t_{n})]~<~K$ and $t_{n}$ in   \\nLemmatize $(y)$ then   \\n9:   \\n10: Set Found=true   \\n11: end if   \\n12: end for   \\n13: end for   \\n14: if Found then   \\n15: Add $(x,y)$ to $S_{r}$   \\n16: end if   \\n17: end for   \\n18:return $S_{r}$  \\n\\nwith more than three senses are considered due to their highly polysemous nature (Campolungo et al., 2022). Note that the subset $S_{c}$ onlytakes up a neglectable portion of the whole dataset, e.g.. 225 sentence pairs for English-Germen, and the specific numbers are reported in the experiment.  \\n\\nTable 1: The number of parallel sentences of different data sets.   \\n\\n\\n  \\n\\ndictionary or based on specific user requirements. To train the LLM in both scenarios - with and without specified word translations - we adopt a sampling strategy inspired by Li et al. (2022). We randomly select a small number of sentence pairs to incorporate specified word translations?. For each chosen instance, we sample at most 3 segment pairs that are matched in the dictionary and construct corresponding instructions with a template:  \\n\\n$$\\nc=\\\\mathrm{Template}(\\\\{(s_{i},t_{i})\\\\}_{i=1}^{N}).\\n$$  \\n\\nFor the template selection, we simply use “means\" to connect $s_{i}$ and $t_{i}$ , and prepend the constraint to the instruction. An example is shown in Figure 6 (Appendix B). In this way, we can choose whether to incorporate translations from the dictionary as auxiliary information during inference, depending on the situation.',\n  '### 3.3  Instruction Fine-tuning  ': 'Instruction fine-tuning has become standard practice in LLM-based translation (Jiao et al., 2023; Xu et al., 2024; Zhang et al., 2023). The instructionfollowing data is constructed based on $S=S_{r}\\\\cup S_{c}$ Generally, each instance comprises an “instruction\" $c$ describing the task the model should perform (e.g., “Translate the sentences from English to Chinese.\"), an “input\" $x$ indicating the source sentence, and a corresponding output $y$ indicating the answer to the instruction, i.e., the target sentence. The language models are optimized by minimizing the negative log-likelihood of the output $y$  \\n\\n$$\\nL=-\\\\sum_{(x,y)\\\\in S}\\\\frac{1}{|y|}\\\\sum_{i}^{|y|}\\\\log p(y_{i}|c,x;\\\\theta),\\n$$  \\n\\nwhere $\\\\theta$ is the trainable parameters. We use two kinds of translation instructions: 1) general translation instructions mainly used to indicate translation directions, and 2) constrained translation instructions that specify word translations from a given',\n  '### 4.1  Setting  ': \"For parallel training data, we use the opensource data from WMT22 in German $\\\\Leftrightarrow$ English, Chinese $\\\\Leftrightarrow$ English, and Russian $\\\\Leftrightarrow$ English. The detail of data preprocessing is shown in Appendix C. We use bilingual dictionaries provided by Open Multilingual WordNet (Bond et al., $2016)^{4}$ .In addition, we involve Wikititles? as an entity dictionary. Table 1 presents the number of sentence pairs for each language pair in different subsets, including the original training set, subsets extracted based on different $K$ , and the ChatGPT-generated data. It can be observed that our method achieves a high compression rate. The subset $K{=}3$ is used for the main experiment, and the extracted data for Chinese, German, and Russian accounts for only $0.57\\\\%$ ， $0.08\\\\%$ , and $0.11\\\\%$ of the original data, respectively. The development sets from the previous  \\n\\nTable 2: Evaluation results on WMT22 test sets. Higher scores (BLEU and COMET) denote better translation performance. Bold numbers indicate the best scores among models of the same sizes. The numbers with the dagger symbol represent the results from (Xu et al., 2024). LexMatcher-7B outperforms Parrot-7B and ALMA-7B with p-value ${<}0.01$ , and LexMatcher-13B outperforms ALMA-13B with p-value ${<}0.01$   \\n\\n\\n  \\n\\nWMT competitions are used by default (Jiao et al., 2023; Xu et al., 2024).  \\n\\nWe fine-tune LLaMA2-7B and LLaMA2-13B for 1 epoch with the collected multilingual instruction data. The batch size is 128 and the learning rate is 2e-5. The final checkpoint is used for evaluation, and we use beam search with a beam size of 4 during inference. For automatic evaluations, we use BLEU (Papineni et al., 2002) ° and COMET'.\",\n  '### 4.2 Main Results  ': 'Seen Language Directions. Table 2 presents the translation performance on the WMT22 test sets. The LLaMA2 models fine-tuned on the instruction data collected by LexMatcher significantly outperform their original zero-shot performance, especially for the $\\\\mathrm{En}{\\\\Rightarrow}\\\\mathbf{X}\\\\mathbf{X}$ .Concretely, LexMatcher-7B improves LLaMA2-7B by an average of 17.02 BLEU points and 12.68 COMET points in $\\\\mathrm{En}{\\\\Rightarrow}\\\\mathbf{X}\\\\mathbf{X}$ , and by 4.45 BLEU points and 2.42 COMET points in $\\\\scriptstyle\\\\mathbf{X}\\\\mathbf{X}{\\\\stackrel{\\\\mathrm{~\\\\tiny~\\\\left.~\\\\right.~}}{\\\\Rightarrow\\\\mathrm{En}}}$ . LLaMA2-13B performs significantly worse than its 7B counterpart in $\\\\mathrm{En}{\\\\Rightarrow}\\\\mathbf{X}\\\\mathbf{X}$ directions due to severe off-target issues, while LexMatcher-13B improves this performance significantly. We also consider an ICL method DictPrompt (Ghazvininejad et al., 2023) which provides dictionary translations for each source word? and the result shows that using dictionary translations as hints yields notable improvements in $\\\\mathrm{En}{\\\\Rightarrow}\\\\mathbf{X}\\\\mathbf{X}$ .In contrast, LexMatcher-13B achieves better performance and is more efficient due to a much shorter context during inference.  \\nFigure 2: Zero-shot translation.  \\n\\nLexMatcher demonstrates superior performance compared to other instruction fine-tuned baselines. Specifically, LexMatcher-7B outperforms Parrot-7B and TIM-7B, which construct additional translation pairs and utilize specialized instructions. In the $\\\\mathrm{En{=}D e}$ translation task, LexMatcher7B surpasses TIM-7B by more than 10 BLEU and COMET points.  Moreover, LexMatcher outperforms BigTrans and ALMA consistently across the $\\\\mathrm{En}{\\\\Rightarrow}\\\\mathbf{X}\\\\mathbf{X}$ tasks, which incorporate a large amount of data for continual pretraining. While LexMatcher-7B still underperforms GPT- $3.5^{9}$ and GPT- $4^{10}$ , the COMET scores for LexMatcher-7B are merely lower than GPT-3.5 within 2 points, and LexMatcher-13B further narrows the gap.  \\n\\nTable 3: Accuracies on the DiBiMT benchmark which is dedicated for evaluating word disambiguation in MT. The number following ICL denotes the number of translation demonstrations.   \\n\\n\\n  \\n\\nUnseen Language Directions. To evaluate performance in translation directions never seen previously, i.e., zero-shot multilingual capability, we further conduct experiments on Czechto-English $(\\\\mathbf{cs}\\\\Rightarrow\\\\mathbf{en}$ 0, Japanese-to-English $(\\\\mathrm{ja}{\\\\Rightarrow}\\\\mathrm{en})$ ， and Ukrainian-to-English $(\\\\mathbf{u}\\\\mathbf{k}{\\\\Rightarrow}\\\\mathbf{en}$ ). As depicted in Figure 2, LexMatcher- $({}^{*})$ exhibits superior zeroshot multilingual capability over the LLM baselines, highlighting that better aligning training languages strengthens the alignment of other languages as a by-product.  \\n\\nDisambiguation.  By comparing the different senses of a word and multilingual expressions of meaning, the model possibly learns more precise word usage in translation. To investigate it, we submit the models to a challenging disambiguation leaderboard, DiBiMT (Campolung0 et al., 2022). It compares the performance of NMT systems when translating sentences with ambiguous words and the performance is evaluated by accuracy. For comparison, we display the performance of top-ranked systems including DeepL11, Google Translate12, and NLLB-54B. The results of LLMs are from Iyer et al. (2023).  \\n\\nThe result is shown in Table 3. For the LLaMA models, increasing model size improves the performance, and LLaMA-65B matches Google Tranlate and NLLB-54B with few-shot prompting. Alpaca7B works well without demonstration (i.e., zeroshot prompting) and significantly outperforms the supervised NMT system OPUS, which indicates its potential for further improvement through finetuning on translation data. LexMatcher-7B significantly outperforms Alpaca- $^ Ḋ 7B Ḍ$ and surpasses Google Translate in Chinese and Russian disambiguation. With a scale of 13B, it also outperforms the best DEEPL system in Chinese and Russian, achieving accuracy rates of $59.09\\\\%$ and $69.93\\\\%$ ,respectively. This result demonstrates the advantage of our data construction principle.  \\n\\nTable 4: Performance on WMT23 terminology translation test sets. “Suc\" indicates Terminology Success Rate.   \\n\\n\\n  \\n\\nTerminology. During training, we introduce special instructions to train the model to use the provided segment pairs. In this experiment, we evaluate the effectiveness of the instructions on a terminology translation test set from $\\\\mathbf{WMT}23^{13}$ . The numbers of sentences on $Z\\\\mathrm{h}{\\\\Rightarrow}\\\\mathrm{En}$ and $\\\\mathrm{De}{\\\\Rightarrow}.$ En are 2640 and 2963, respectively. The average numbers of terms per segment on $Z\\\\mathrm{h}{\\\\Rightarrow}\\\\mathrm{En}$ and $\\\\mathrm{De}{\\\\Rightarrow}\\\\mathrm{En}$ are 3.8 and 1.1, respectively. The result is shown in Table 4, and we only present the systems achieving the best performance on a specific metric (Semenov et al., 2023). Lingua Custodia and VARCO are specialized Transformer architectures to ensure the appearance of given terminology in the translation, and $U E D I N_{\\\\mathrm{LLM}}$ uses ChatGPT with terminology translation prompts. Compared to them, our models achieve significantly higher terminology success rates, indicating a superior ability to accurately respond to the given domain-specific terminology. On the quality metrics, our models are inferior to $U E D I N_{\\\\mathrm{LLM}}$ on $Z\\\\mathrm{h}{\\\\Rightarrow}\\\\mathrm{En}$ , and achieve the best results on $\\\\mathrm{De}{\\\\Rightarrow}\\\\mathrm{En}$',\n  '### 5.1 Effect of $K$  ': \"The maximal number of bilingual contexts of each matched sense is influenced by $K$ .We show the performance of varying $K\\\\mathbf{s}$ across different model sizes on the WMT22 test sets (Figure 3). Regardless of the amount of training data used, the larger models perform better and require less data for fine-tuning. In addition, the model's performance improves as $K$ increases from 1 to 3. With the addition of more parallel data, the performance gains begin to plateau or even slightly decrease, which aligns with the conclusions of the previous study $\\\\mathrm{{{Xu}}}$ et al., 2024). Thanks to the strong fewshot learning capability of the backbones, we do not need to provide as many training examples as before when training the NMT model.  \\nFigure 3: BLEU and COMET on the WMT22 test sets with varying $K$ and model sizes.\",\n  '### 5.2   Effect of Selection Strategies  ': 'In this experiment, we investigate two intuitive data collection methods: 1) random selection (RAND), in which the training data are randomly sampled from the corpus; and 2) quality-based selection $(T O P)$ , in which the training samples are selected based on the COMET-KIWI scores in descending order. Specifically, we use these two methods to extract the same sample quantity as LexMatcher to mitigate the impact of sample quantity. We use LLaMA2-7B as the backbone, and the result on WMT test sets is shown in Figure 4. The performance of RAND is inferior to the other two methods. Random selection ensures a certain degree of diversity but the performance is uncontrollable and non-reproducible. TOP performs better than RAND, demonstrating the importance of data quality for instruction tuning. LexMatcher can simultaneously consider both quality and diversity and achieve the best performance.  \\nFigure 4: Performance of different data selection strategies.  \\nFigure 5: Word frequency distributions. The blue and gray curves denote the distributions calculated on the data selected by LexMatcher $(\\\\mathrm{K}{=}1)$ ) and randomly selected data, respectively.  \\n\\nWord Frequency Distribution   We are interested in whether the collected data has a different word frequency distribution from the general (randomly selected) one. We use the English data of the $\\\\mathrm{EN}{\\\\Rightarrow}Z\\\\mathrm{H}$ translation task with $K{=}1$ , and plot the word frequency distributions of the collected data (blue curve) and the corresponding random data (gray curve). As shown in Figure 5, the blue curve tends to be smoother than the gray one, and the blue curve has more flat segments. For words with higher frequency rankings, the word frequency of the data selected based on the dictionary is lower than that of the random data. This phenomenon indicates that the dictionary-based method has generated a less skewed data distribution, which could be the reason for better fine-tuning performance. Additionally, the dictionary-based data contains $98\\\\mathrm{k}$ unique words while the random data only includes $62\\\\mathrm{k}$ unique words, indicating that the dictionarybased data covers more semantic units, thus diluting the word frequency.  \\n\\nTable 5: Ablation study on different data subsets.',\n  '### 5.3Ablation Study  ': \"The ablation experiment of different data subsets is presented in Table 5. We use LLaMA2-7B as the backbone. Based on the development data, simply incorporating the small amount of synthesized data generated during the sense supplement phase does not have a significant impact on the performance. This is possibly because the data is predominantly focused on low-frequency senses, and the model is unable to effectively leverage this knowledge. In comparison, adding the retrieved data leads to a significant performance improvement, and further introducing the synthesized data helps the model learn word disambiguation better, increasing the disambiguation accuracy from 59.98 to 61.44.  \\n\\n5.4 Combination with ALMA   \\nTable 6: Combination with ALMA-7B.   \\n\\n\\n  \\n\\nALMA (Xu et al., 2024) is the post-trained LLaMA2 on a large amount of monolingual data mixed by different languages. We use ALMA-7B as the backbone and investigate whether the two methods can complement each other. As shown in Table 6, adding the parallel sentences constructed by LexMatcher further improves the performance of ALMA, indicating the compatibility of monolingual continual pretraining and bilingual supervised fine-tuning. Specifically, utilizing the training data With $K{=}1$ is sufficient to enhance ALMA's performance. Our findings indicate that the use of monolingual data during pretraining can significantly reduce the dependency on bilingual data. Conversely, the direct application of bilingual data for fine-tuning is more resource-efficient. The size of parallel data collected by LexMatcher is considerably smaller than that of mixed monolingual data, and the training process is only a single stage.  \\n\\nTable 7: Compound translation error rate (CTER) on CoGnition. Instance and Aggregate denote the instancelevel and aggregate-level compound translation error rates, respectively.\",\n  '### 5.5  Compositional Generalization  ': 'The data with balanced atom distribution can enhance the performance of compositional generalization, and we verify it on CoGnition (Li et al., 2021). The evaluation metrics include instancelevel CTER which denotes the translation accuracy of novel compounds, and aggregate-level CTER which measures the translation consistency across different contexts. We use the sense retrieval of LexMatcher to obtain 70,272 parallel sentences from the full training data (196,246) with $K{=}50$ For LLM, we apply ICL with 8 examples and finetune LLaMA2-7B on the randomly sampled training data, of which the size is similar to the retrieved data. The results are shown in Table 7. ICL does not yield good compositional generalization performance, while the fine-tuned LLaMA2 outperforms the previous NMT models significantly. LexMatcher achieves lower compound translation error rates than SFT with the same amount of training data, demonstrating the positive effect of the more balanced data distribution.',\n  '## 6 Conclusion  ': 'In this paper, we present LexMatcher, a dictionarycentric data collection method for supervised finetuning, and make open-source LLMs a better translation model. We use the bilingual dictionary as the pivot and try to collect limited parallel sentence pairs to cover the senses uniformly. Experiments and analyses validate the effectiveness of LexMatcher from multiple perspectives including zero-shot translation, disambiguation, and terminology translation. One potential avenue for future research involves extending LexMatcher to lowresource scenarios, where the utilization of monolingual data is crucial for achieving satisfactory translation performance.'}}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_information"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-03T06:13:16.163233600Z",
     "start_time": "2025-03-03T06:13:16.153054700Z"
    }
   },
   "id": "84df6a0f9b0c5fce",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T16:21:33.881191500Z",
     "start_time": "2025-03-02T16:21:33.874848900Z"
    }
   },
   "id": "8db9a86b5701a34d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "LocalEntryNotFoundError",
     "evalue": "An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/connection.py:203\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/util/connection.py:85\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 85\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "\u001B[0;31mOSError\u001B[0m: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mNewConnectionError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/connectionpool.py:790\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 790\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    803\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/connectionpool.py:491\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    490\u001B[0m         new_e \u001B[38;5;241m=\u001B[39m _wrap_proxy_error(new_e, conn\u001B[38;5;241m.\u001B[39mproxy\u001B[38;5;241m.\u001B[39mscheme)\n\u001B[0;32m--> 491\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m new_e\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001B[39;00m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/connectionpool.py:467\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 467\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/connectionpool.py:1096\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1095\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[0;32m-> 1096\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_verified:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/connection.py:611\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    610\u001B[0m sock: socket\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m|\u001B[39m ssl\u001B[38;5;241m.\u001B[39mSSLSocket\n\u001B[0;32m--> 611\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    612\u001B[0m server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/connection.py:218\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 218\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NewConnectionError(\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to establish a new connection: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    220\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001B[39;00m\n",
      "\u001B[0;31mNewConnectionError\u001B[0m: <urllib3.connection.HTTPSConnection object at 0x7f45879fdb20>: Failed to establish a new connection: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/connectionpool.py:844\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    842\u001B[0m     new_e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_e)\n\u001B[0;32m--> 844\u001B[0m retries \u001B[38;5;241m=\u001B[39m \u001B[43mretries\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mincrement\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnew_e\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    847\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/urllib3/util/retry.py:515\u001B[0m, in \u001B[0;36mRetry.increment\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    514\u001B[0m     reason \u001B[38;5;241m=\u001B[39m error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause)\n\u001B[0;32m--> 515\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, reason) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mreason\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    517\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncremented Retry for (url=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, url, new_retry)\n",
      "\u001B[0;31mMaxRetryError\u001B[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unstructuredio/yolo_x_layout/resolve/main/yolox_l0.05.onnx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f45879fdb20>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/file_download.py:1376\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[1;32m   1375\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1376\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1377\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\n\u001B[1;32m   1378\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1379\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/file_download.py:1296\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[1;32m   1295\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[0;32m-> 1296\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1297\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHEAD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1298\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1299\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1300\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1301\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1302\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1303\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1304\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1305\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/file_download.py:277\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[0;32m--> 277\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    278\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    279\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    280\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    282\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/file_download.py:300\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001B[39;00m\n\u001B[0;32m--> 300\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mget_session\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    301\u001B[0m hf_raise_for_status(response)\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:93\u001B[0m, in \u001B[0;36mUniqueRequestIdAdapter.send\u001B[0;34m(self, request, *args, **kwargs)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/requests/adapters.py:700\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    698\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m SSLError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[0;32m--> 700\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ClosedPoolError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mConnectionError\u001B[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unstructuredio/yolo_x_layout/resolve/main/yolox_l0.05.onnx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f45879fdb20>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 792ece9b-e4bb-47b0-9265-90b16082cc34)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mLocalEntryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/d/PycharmCode/LLMscratch/essay_searcher/pdfs/LexMatcher.pdf\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Returns a List[Element] present in the pages of the parsed pdf document\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m elements \u001B[38;5;241m=\u001B[39m \u001B[43mpartition_pdf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43mextract_images_in_pdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_table_structure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunking_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mby_title\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_characters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnew_after_n_chars\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3800\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcombine_text_under_n_chars\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured/documents/elements.py:581\u001B[0m, in \u001B[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    579\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Element]:\n\u001B[0;32m--> 581\u001B[0m     elements \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m     call_args \u001B[38;5;241m=\u001B[39m get_call_args_applying_defaults(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    584\u001B[0m     unique_element_ids: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m call_args\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munique_element_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured/file_utils/filetype.py:815\u001B[0m, in \u001B[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    813\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    814\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Element]:\n\u001B[0;32m--> 815\u001B[0m     elements \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    817\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m element \u001B[38;5;129;01min\u001B[39;00m elements:\n\u001B[1;32m    818\u001B[0m         \u001B[38;5;66;03m# NOTE(robinson) - Attached files have already run through this logic\u001B[39;00m\n\u001B[1;32m    819\u001B[0m         \u001B[38;5;66;03m# in their own partitioning function\u001B[39;00m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m element\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mattached_to_filename \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured/file_utils/filetype.py:773\u001B[0m, in \u001B[0;36madd_metadata.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    771\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    772\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Element]:\n\u001B[0;32m--> 773\u001B[0m     elements \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    774\u001B[0m     call_args \u001B[38;5;241m=\u001B[39m get_call_args_applying_defaults(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    776\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m call_args\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata_filename\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured/chunking/dispatch.py:74\u001B[0m, in \u001B[0;36madd_chunking_strategy.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# -- call the partitioning function to get the elements --\u001B[39;00m\n\u001B[0;32m---> 74\u001B[0m elements \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# -- look for a chunking-strategy argument --\u001B[39;00m\n\u001B[1;32m     77\u001B[0m call_args \u001B[38;5;241m=\u001B[39m get_call_args_applying_defaults(func, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured/partition/pdf.py:229\u001B[0m, in \u001B[0;36mpartition_pdf\u001B[0;34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, metadata_filename, metadata_last_modified, chunking_strategy, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, **kwargs)\u001B[0m\n\u001B[1;32m    226\u001B[0m exactly_one(filename\u001B[38;5;241m=\u001B[39mfilename, file\u001B[38;5;241m=\u001B[39mfile)\n\u001B[1;32m    228\u001B[0m languages \u001B[38;5;241m=\u001B[39m check_language_args(languages \u001B[38;5;129;01mor\u001B[39;00m [], ocr_languages)\n\u001B[0;32m--> 229\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpartition_pdf_or_image\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfile\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_page_breaks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_page_breaks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    233\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    234\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfer_table_structure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_table_structure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlanguages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlanguages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadata_last_modified\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata_last_modified\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhi_res_model_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhi_res_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextract_images_in_pdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_images_in_pdf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextract_image_block_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_image_block_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextract_image_block_output_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_image_block_output_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextract_image_block_to_payload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_image_block_to_payload\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstarting_page_number\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstarting_page_number\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextract_forms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_forms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43mform_extraction_skip_tables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mform_extraction_skip_tables\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpassword\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpdfminer_line_margin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpdfminer_line_margin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpdfminer_char_margin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpdfminer_char_margin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpdfminer_line_overlap\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpdfminer_line_overlap\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpdfminer_word_margin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpdfminer_word_margin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured/partition/pdf.py:342\u001B[0m, in \u001B[0;36mpartition_pdf_or_image\u001B[0;34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, languages, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, starting_page_number, extract_forms, form_extraction_skip_tables, password, pdfminer_line_margin, pdfminer_char_margin, pdfminer_line_overlap, pdfminer_word_margin, **kwargs)\u001B[0m\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m warnings\u001B[38;5;241m.\u001B[39mcatch_warnings():\n\u001B[1;32m    341\u001B[0m         warnings\u001B[38;5;241m.\u001B[39msimplefilter(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 342\u001B[0m         elements \u001B[38;5;241m=\u001B[39m \u001B[43m_partition_pdf_or_image_local\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfile\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspooled_to_bytes_io_if_needed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_image\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_image\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m            \u001B[49m\u001B[43minfer_table_structure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_table_structure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m            \u001B[49m\u001B[43minclude_page_breaks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_page_breaks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlanguages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlanguages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m            \u001B[49m\u001B[43mocr_languages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mocr_languages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata_last_modified\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata_last_modified\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mlast_modified\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhi_res_model_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhi_res_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpdf_text_extractable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpdf_text_extractable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextract_images_in_pdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_images_in_pdf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextract_image_block_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_image_block_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextract_image_block_output_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_image_block_output_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextract_image_block_to_payload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_image_block_to_payload\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstarting_page_number\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstarting_page_number\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextract_forms\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextract_forms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m            \u001B[49m\u001B[43mform_extraction_skip_tables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mform_extraction_skip_tables\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpassword\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpdfminer_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpdfminer_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    364\u001B[0m         out_elements \u001B[38;5;241m=\u001B[39m _process_uncategorized_text_elements(elements)\n\u001B[1;32m    366\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m strategy \u001B[38;5;241m==\u001B[39m PartitionStrategy\u001B[38;5;241m.\u001B[39mFAST:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured/utils.py:216\u001B[0m, in \u001B[0;36mrequires_dependencies.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: _P\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: _P\u001B[38;5;241m.\u001B[39mkwargs):\n\u001B[1;32m    215\u001B[0m     run_check()\n\u001B[0;32m--> 216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured/partition/pdf.py:643\u001B[0m, in \u001B[0;36m_partition_pdf_or_image_local\u001B[0;34m(filename, file, is_image, infer_table_structure, include_page_breaks, languages, ocr_languages, ocr_mode, model_name, hi_res_model_name, pdf_image_dpi, metadata_last_modified, pdf_text_extractable, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, analysis, analyzed_image_output_dir_path, starting_page_number, extract_forms, form_extraction_skip_tables, pdf_hi_res_max_pages, password, pdfminer_config, **kwargs)\u001B[0m\n\u001B[1;32m    640\u001B[0m skip_analysis_dump \u001B[38;5;241m=\u001B[39m env_config\u001B[38;5;241m.\u001B[39mANALYSIS_DUMP_OD_SKIP\n\u001B[1;32m    642\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 643\u001B[0m     inferred_document_layout \u001B[38;5;241m=\u001B[39m \u001B[43mprocess_file_with_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    644\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    645\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_image\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_image\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    646\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhi_res_model_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    647\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpdf_image_dpi\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpdf_image_dpi\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpassword\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    651\u001B[0m     extracted_layout, layouts_links \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    652\u001B[0m         process_file_with_pdfminer(\n\u001B[1;32m    653\u001B[0m             filename\u001B[38;5;241m=\u001B[39mfilename,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    659\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m ([], [])\n\u001B[1;32m    660\u001B[0m     )\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m analysis:\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured_inference/inference/layout.py:366\u001B[0m, in \u001B[0;36mprocess_file_with_model\u001B[0;34m(filename, model_name, is_image, fixed_layouts, pdf_image_dpi, password, **kwargs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_file_with_model\u001B[39m(\n\u001B[1;32m    355\u001B[0m     filename: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m    356\u001B[0m     model_name: Optional[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    362\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DocumentLayout:\n\u001B[1;32m    363\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Processes pdf file with name filename into a DocumentLayout by using a model identified by\u001B[39;00m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;124;03m    model_name.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 366\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mget_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, UnstructuredObjectDetectionModel):\n\u001B[1;32m    368\u001B[0m         detection_model \u001B[38;5;241m=\u001B[39m model\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured_inference/models/base.py:74\u001B[0m, in \u001B[0;36mget_model\u001B[0;34m(model_name)\u001B[0m\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m UnknownModelException(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnknown model type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     72\u001B[0m model: UnstructuredModel \u001B[38;5;241m=\u001B[39m model_class_map[model_name]()\n\u001B[0;32m---> 74\u001B[0m model\u001B[38;5;241m.\u001B[39minitialize(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minitialize_params)\n\u001B[1;32m     75\u001B[0m models[model_name] \u001B[38;5;241m=\u001B[39m model\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured_inference/utils.py:40\u001B[0m, in \u001B[0;36mLazyDict.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m     38\u001B[0m evaluate \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39mevaluate\n\u001B[1;32m     39\u001B[0m args, kwargs \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39minfo\n\u001B[0;32m---> 40\u001B[0m value \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache:\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raw_dict[key] \u001B[38;5;241m=\u001B[39m value\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/unstructured_inference/utils.py:115\u001B[0m, in \u001B[0;36mdownload_if_needed_and_get_local_path\u001B[0;34m(path_or_repo, filename, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m full_path\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath_or_repo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_use_auth_token:\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/file_download.py:862\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[1;32m    842\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_local_dir(\n\u001B[1;32m    843\u001B[0m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[1;32m    844\u001B[0m         local_dir\u001B[38;5;241m=\u001B[39mlocal_dir,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    859\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    860\u001B[0m     )\n\u001B[1;32m    861\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_hf_hub_download_to_cache_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Destination\u001B[39;49;00m\n\u001B[1;32m    864\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# File info\u001B[39;49;00m\n\u001B[1;32m    866\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    867\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# HTTP info\u001B[39;49;00m\n\u001B[1;32m    871\u001B[0m \u001B[43m        \u001B[49m\u001B[43mendpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mendpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[43m        \u001B[49m\u001B[43metag_timeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    873\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Additional options\u001B[39;49;00m\n\u001B[1;32m    877\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/file_download.py:969\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[1;32m    966\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m pointer_path\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[0;32m--> 969\u001B[0m     \u001B[43m_raise_on_head_call_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhead_call_error\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    971\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n\u001B[1;32m    972\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m etag \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124metag must have been retrieved from server\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/Ddcase/lib/python3.9/site-packages/huggingface_hub/file_download.py:1487\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[0;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[1;32m   1484\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1486\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n\u001B[0;32m-> 1487\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LocalEntryNotFoundError(\n\u001B[1;32m   1488\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is on.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1491\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhead_call_error\u001B[39;00m\n",
      "\u001B[0;31mLocalEntryNotFoundError\u001B[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "file_path = \"/mnt/d/PycharmCode/LLMscratch/essay_searcher/pdfs/LexMatcher.pdf\"\n",
    "# Returns a List[Element] present in the pages of the parsed pdf document\n",
    "elements = partition_pdf(file_path,  extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T16:21:54.329681600Z",
     "start_time": "2025-03-02T16:21:34.522221300Z"
    }
   },
   "id": "7b5acfa3dfd57a3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Title] n u J\n",
      "\n",
      "[Title] ] L C . s c [\n",
      "\n",
      "[Title] Abstract\n",
      "\n",
      "[Title] Introduction\n",
      "\n",
      "[Title] 2 Related Work\n",
      "\n",
      "[Title] Instructionfine-tuning\n",
      "\n",
      "[Title] Retrieve\n",
      "\n",
      "[Title] Corpus\n",
      "\n",
      "[Title] Sense retrievalSense supplement\n",
      "\n",
      "[Title] Figure 1: Illustration of our LexMatcher for instruction fine-tuning.\n",
      "\n",
      "[Title] 3 Method\n",
      "\n",
      "[Title] 3.1 Sense Retrieval\n",
      "\n",
      "[Title] 3.2 Sense Supplement\n",
      "\n",
      "[Title] 1We use unigram and bigram excluding stopwords. 2GPT-3.5-turbo-0314\n",
      "\n",
      "[Title] Algorithm 1 Sense Retrieval in LexMatcher\n",
      "\n",
      "[Title] 1: Input: Parallel dataset D, dictionary Φ, thresh-\n",
      "\n",
      "[Title] old K\n",
      "\n",
      "[Title] Lemmatize(y) then\n",
      "\n",
      "[Title] end if\n",
      "\n",
      "[Title] end for\n",
      "\n",
      "[Title] end for if Found then\n",
      "\n",
      "[Title] end if\n",
      "\n",
      "[Title] 16: 17: end for 18: return Sr\n",
      "\n",
      "[Title] Instruction Fine-tuning\n",
      "\n",
      "[Title] i\n",
      "\n",
      "[Title] Lang\n",
      "\n",
      "[Title] Raw\n",
      "\n",
      "[Title] Retrieval\n",
      "\n",
      "[Title] Supplement\n",
      "\n",
      "[Title] Zh De Ru\n",
      "\n",
      "[Title] Table 1: The number of parallel sentences of different data sets.\n",
      "\n",
      "[Title] 4 Experiments\n",
      "\n",
      "[Title] 4.1 Setting\n",
      "\n",
      "[Title] 4https://www.nltk.org/howto/wordnet.html 5https://data.statmt.org/wikititles/v3/\n",
      "\n",
      "[Title] Model\n",
      "\n",
      "[Title] Zh⇒En\n",
      "\n",
      "[Title] En⇒Zh\n",
      "\n",
      "[Title] BLEU/COMET BLEU/COMET\n",
      "\n",
      "[Title] De⇒En BLEU/COMET\n",
      "\n",
      "[Title] Ru⇒En BLEU/COMET BLEU/COMET BLEU/COMET\n",
      "\n",
      "[Title] En⇒De\n",
      "\n",
      "[Title] En⇒Ru\n",
      "\n",
      "[Title] LexMatcher-13B\n",
      "\n",
      "[Title] LexMatcher-7B\n",
      "\n",
      "[Title] GPT-3.5-turbo\n",
      "\n",
      "[Title] BayLing-13B\n",
      "\n",
      "[Title] GPT-4\n",
      "\n",
      "[Title] ALMA-7B\n",
      "\n",
      "[Title] ALMA-13B\n",
      "\n",
      "[Title] BayLing-7B\n",
      "\n",
      "[Title] 4.2 Main Results\n",
      "\n",
      "[Title] Figure 2: Zero-shot translation.\n",
      "\n",
      "[Title] implement the method on LLaMA2-13B.\n",
      "\n",
      "[Title] Model\n",
      "\n",
      "[Title] Zh\n",
      "\n",
      "[Title] De\n",
      "\n",
      "[Title] Ru\n",
      "\n",
      "[Title] DeepL Google-Translate OPUS NLLB-54B\n",
      "\n",
      "[Title] LLaMA-7B-ICL(1) LLaMA-7B-ICL(5) LLaMA-65B-ICL(1) LLaMA-65B-ICL(5) Alpaca-7B\n",
      "\n",
      "[Title] LexMatcher-7B LexMatcher-13B\n",
      "\n",
      "[Title] 11https://www.deepl.com/en/translator 12https://translate.google.com\n",
      "\n",
      "[Title] Model\n",
      "\n",
      "[Title] Zh⇒En\n",
      "\n",
      "[Title] De⇒En\n",
      "\n",
      "[Title] ChrF/COMET Suc ChrF/COMET Suc\n",
      "\n",
      "[Title] Lingua Custodia VARCO UEDINLLM\n",
      "\n",
      "[Title] LexMatcher-7B LexMatcher-13B\n",
      "\n",
      "[Title] 5 Analysis\n",
      "\n",
      "[Title] 5.1 Effect of K\n",
      "\n",
      "[Title] 13https://wmt-terminology-task.github.io/\n",
      "\n",
      "[Title] 5.2 Effect of Selection Strategies\n",
      "\n",
      "[Title] TOP\n",
      "\n",
      "[Title] LexMatcher\n",
      "\n",
      "[Title] RAND\n",
      "\n",
      "[Title] Figure 4: Performance of different data selection strate- gies.\n",
      "\n",
      "[Title] 0.004Frequency of token\n",
      "\n",
      "[Title] Frequency rank of token\n",
      "\n",
      "[Title] Model\n",
      "\n",
      "[Title] xx⇒En\n",
      "\n",
      "[Title] En⇒xx\n",
      "\n",
      "[Title] BLEU/COMET BLEU/COMET\n",
      "\n",
      "[Title] DiBi-Acc\n",
      "\n",
      "[Title] Dev +Supplement +Retrieval LexMatcher(3)\n",
      "\n",
      "[Title] Table 5: Ablation study on different data subsets.\n",
      "\n",
      "[Title] 5.3 Ablation Study\n",
      "\n",
      "[Title] 5.4 Combination with ALMA\n",
      "\n",
      "[Title] Model\n",
      "\n",
      "[Title] xx⇒En\n",
      "\n",
      "[Title] En⇒xx\n",
      "\n",
      "[Title] BLEU/COMET BLEU/COMET\n",
      "\n",
      "[Title] ALMA +LexMatcher(1) +LexMatcher(2)\n",
      "\n",
      "[Title] Table 6: Combination with ALMA-7B.\n",
      "\n",
      "[Title] Model\n",
      "\n",
      "[Title] BLEU Instance Aggregate\n",
      "\n",
      "[Title] Transformer Transformer+CReg\n",
      "\n",
      "[Title] LLaMA2-ICL LLaMA2-SFT LexMatcher\n",
      "\n",
      "[Title] 5.5 Compositional Generalization\n",
      "\n",
      "[Title] 6 Conclusion\n",
      "\n",
      "[Title] 7 Limitations\n",
      "\n",
      "[Title] References\n",
      "\n",
      "[Title] A Computational Details\n",
      "\n",
      "[Title] B Prompts Used for Manipulating\n",
      "\n",
      "[Title] ChatGPT and Terminology Translation\n",
      "\n",
      "[Title] C Corpus Preprocessing\n",
      "\n",
      "[Title] D Ablation Study\n",
      "\n",
      "[Title] 14https://github.com/Helsinki-NLP/Tatoeba-\n",
      "\n",
      "[Title] Challenge/tree/v2021-08-07/data\n",
      "\n",
      "[Title] 15https://huggingface.co/Unbabel/wmt22-cometkiwi-da 16https://spacy.io/\n",
      "\n",
      "[Title] Model\n",
      "\n",
      "[Title] Zh⇒En\n",
      "\n",
      "[Title] En⇒Zh\n",
      "\n",
      "[Title] De⇒En\n",
      "\n",
      "[Title] En⇒De\n",
      "\n",
      "[Title] Ru⇒En\n",
      "\n",
      "[Title] En⇒Ru\n",
      "\n",
      "[Title] BLEU/COMET BLEU/COMET BLEU/COMET BLEU/COMET BLEU/COMET BLEU/COMET\n",
      "\n",
      "[Title] Dev +Supplement +Retrieval LexMatcher\n",
      "\n",
      "[Title] Table 8: The detailed results of ablation study.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 遍历所有分块内容，并按类别组织\n",
    "for el in elements:\n",
    "\n",
    "        print(f\"[{el.category}] {el.text}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T15:31:32.820169100Z",
     "start_time": "2025-03-02T15:31:32.777320200Z"
    }
   },
   "id": "cdc7324c21eba7d",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 0 2\n",
      "b e F 6\n",
      "] L C . s c [\n",
      "2 v 4 7 6 1 1 . 9 0 3 2 : v i X r a\n",
      "Published as a conference paper at ICLR 2024\n",
      "A PARADIGM SHIFT IN MACHINE TRANSLATION: BOOSTING TRANSLATION PERFORMANCE OF LARGE LANGUAGE MODELS\n",
      "Haoran Xu♠, Young Jin Kim♡, Amr Sharaf♡, Hany Hassan Awadalla♡\n",
      "♠Johns Hopkins University, ♡Microsoft\n",
      "hxu64@jhu.edu {youki,amrsharaf,hanyh}@microsoft.com\n",
      "ABSTRACT\n",
      "Generative Large Language Models (LLMs) have achieved remarkable advance- ments in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that tradi- tional translation models usually depend on. Our approach consists of two fine- tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 (Touvron et al., 2023b) as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 trans- lation directions from the WMT’21 (2 directions) and WMT’22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model (NLLB TEAM et al., 2022) and GPT- 3.5-text-davinci-003, with only 7B or 13B parameters. This method estab- lishes the foundation for a novel training paradigm in machine translation. 1\n",
      "1\n",
      "INTRODUCTION\n",
      "Generative (decoder-only) large language models (LLMs) such as GPT models (Brown et al., 2020; OpenAI, 2023), PaLM (Chowdhery et al., 2022), OPT (Zhang et al., 2022), BLOOM (Scao et al., 2022), LLaMA (Touvron et al., 2023a;b), and others have exhibited remarkable capabilities across various NLP tasks. However, for the translation task, only very large models such as GPT-3.5 and GPT-4 can rival the supervised encoder-decoder state-of-the-art (SoTA) models like NLLB (NLLB TEAM et al., 2022), while they still fall short in translation for low-resource languages (Hendy et al., 2023; Jiao et al., 2023). The discrepancy becomes more evident when comparing other LLMs with traditional translation models (Zhu et al., 2023a). For instance, the OPT-175B model trails behind the NLLB-1.3B model by an average of more than 15 BLEU (Papineni et al., 2002) points for languages within the Indo-European-Romance family. The gap is even larger in smaller LLMs; for example, XGLM (Lin et al., 2021), with a parameter size of 7B, lags behind the NLLB-1.3B by a substantial 30 BLEU points (Zhu et al., 2023a). Therefore, there is an urgent need to narrow this performance gap between LLMs and conventional SoTA models.\n",
      "As exemplified by NLLB-1.3B, traditional machine translation models demonstrate proficiency in producing high-quality translations with a small number of parameters. By extension, smaller LLMs\n",
      "1We release our code and models at: https://github.com/fe1ixxu/ALMA.\n",
      "1\n",
      "Published as a conference paper at ICLR 2024\n",
      "should similarly possess the capability to adeptly manage the translation task. Recent research has sought to enhance translation performance by commencing with smaller LLMs (Yang et al., 2023; Zeng et al., 2023; Chen et al., 2023; Zhu et al., 2023b; Li et al., 2023; Zhang et al., 2023b), especially 7B or 13B parameters. Nevertheless, the achieved improvements remain modest and limited. As depicted in Figure 1, contemporary studies such as Balyling (Zhang et al., 2023b) and BigTranslate (Yang et al., 2023), which use LLaMA as their backbone, exhibit a maximum increment of 3 to 4 BLEU or COMET in relation to the zero-shot performance of LLaMA on the WMT’22 test set (8 directions).2 While these gains represent promising research direction for smaller LLMs in the translation task, a significant performance chasm persists when benchmarked against very large LLMs such as GPT-3.5-text-davinci-003 and SoTA translation models such as NLLB-54B. We posit that the modest translation gains observed in prior studies can be ascribed to an unsuitable training recipe.\n",
      "26\n",
      "LLaMA-2-7BLLaMA-2-13BLLaMA-1-7BLLaMA-1-13BALMA-7B (Ours)ALMA-13B (Ours)NLLB-54BGPT-3.5text-davinci-003BigTranslateBayling-13BBayling-7B\n",
      "54B\n",
      "Model Size (B)\n",
      "32\n",
      "24\n",
      "13B\n",
      "28\n",
      "7B\n",
      "22\n",
      "34BLEU\n",
      "175B\n",
      "30\n",
      "86COMET\n",
      "13B\n",
      "84\n",
      "78\n",
      "Model Size (B)\n",
      "80\n",
      "54B\n",
      "82\n",
      "175B\n",
      "7B\n",
      "LLaMA-2-7BLLaMA-2-13BLLaMA-1-7BLLaMA-1-13BALMA-7B (Ours)ALMA-13B (Ours)NLLB-54BGPT-3.5text-davinci-003BigTranslateBayling-13BBayling-7B\n",
      "(a) BLEU\n",
      "(b) COMET\n",
      "Figure 1: Translation performance of contemporary decoder-only LLM translation systems based on LLaMA (Yang et al., 2023; Zhang et al., 2023b), and zero-shot performance of LLaMA, for the WMT’22 test data across 8 directions (translating to or from English for German, Czech, Chinese, and Russian). Benchmark comparisons also include two leading translation models, NLLB-54B and GPT-3.5-text-davinci-003. Our systems, developed on LLaMA-2 with 7B and 13B pa- rameters, surpass previous models by an impressive margin of nearly 10 BLEU and 7 COMET. Furthermore, they even slightly outperform GPT-3.5 and NLLB-54B on average.\n",
      "learning general We hypothesize that an efficacious training recipe ought to follow two stages: multilingual linguistic knowledge and inducing (instructing) models toward translation genera- tion. Consequently, we propose a two-stage fine-tuning approach and introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Specifically, given most LLMs are trained on English-dominant data, the first stage is fine-tuning non-English mono- lingual data to enhance the model’s proficiency in other languages involved in the translation task. Secondly, drawing inspiration from the recognized significance of data quality in other applications (Zhou et al., 2023; Maillard et al., 2023; Gunasekar et al., 2023), we fine-tune the model with a small amount of high-quality parallel data.\n",
      "Our main contributions are summarized as follows:\n",
      "Diminished Necessity of Parallel Data Traditional translation frameworks rely on large amounts of parallel data, which may lead to a false impression that such data is essential for the translation task with LLMs. Prior studies have fine-tuned LLMs with datasets containing over 300M parallel instances (Yang et al., 2023). However, our empirical evaluations suggest that this strategy may not be optimal, and even harm the translation capabilities of LLMs.\n",
      "LLM Via A New Training Recipe: ALMA We introduce a novel two-stage fine-tuning method for translation with decoder-only LLMs. Leveraging LLaMA-2 as the base model, we attain an average improvement of more than 12 BLEU and COMET scores over its zero-shot performance\n",
      "2All COMET scores in the paper is COMET-22 (Unbabel/wmt22-comet-da) (Rei et al., 2022).\n",
      "2\n",
      "Published as a conference paper at ICLR 2024\n",
      "across 10 translation directions from WMT’21 and WMT’22 test datasets. Notably, the perfor- mance surpasses all previous work and is even better than the NLLB-54B model and GPT-3.5- text-davinci-003.\n",
      "Efficient Computational Cost Our ablation study reveals both stages are crucial factors for achiev- ing large improvements. The most computationally intensive part is monolingual data fine-tuning, however, we show that only fine-tuning 1B monolingual tokens is sufficient to have comparable performance to NLLB-54B in 10 translation directions, which only requires around 18 hours to complete with 16 MI200 GPUs.\n",
      "2 PRELIMINARY\n",
      "2.1 TASK DEFINITION\n",
      "We consider a decoder-only transformer model parameterized by θ for machine translation. Let x represent the source sentence and y its corresponding target sentence. We utilize a fixed prompt template, denoted as I, to guide the model in generating translation. The log-likelihood loss of the parallel sentence (x, y) with regard to the model parameters θ can be formulated as follows:\n",
      "LNLL(x,y,θ) = −logP(y|x,I;θ)\n",
      "= −\n",
      "T (cid:88)\n",
      "logP(yt|y<t,x,I;θ),\n",
      "t=1\n",
      "where T is length of the target sentence, and yt is the t-th target token. The loss is a standard causal language modeling (CLM) loss, which predicts the next token based on prior tokens. We use the same sentence-level translation prompt template suggested by Hendy et al. (2023), and illustrate the prompt and the model input/target in Figure 2. Note that we do not compute the loss for the prompt template and source sentence during training (Zhang et al., 2023a). In Appendix A, we show that CLM is more suitable for the translation task compared with other modeling methods, such as prefix language modeling (Wang et al., 2022) and mixture-of-denoisers (Tay et al., 2022a).\n",
      "Translate this from [source language] to [target language]: [source language]: <source sentence>[target language]:<target sentence>Prompt+Input / TargetPromptNo loss computedCLM loss\n",
      "Figure 2: The prompt used for training and evaluation. [source language] and [target language] represent the full name of the language, e.g., Translate this from German to English. Note that we do not compute loss for the prompt.\n",
      "2.2 A BACKBONE LLM FOR TRANSLATION\n",
      "We seek a robust LLM to serve as our foundational model. With the recent emergence of numerous LLMs, we prioritize evaluating the zero-shot translation performance of these models before delv- ing into optimal training recipes. As most of these models provide a 7B version, our comparative analysis centers on this magnitude: OPT-7B (Zhang et al., 2022), Falcon-7B (Almazrouei et al., 2023), BLOOM-7B (Scao et al., 2022), MPT-7B (MosaicML, 2023), LLaMA-1-7B (Touvron et al., 2023a), and LLaMA-2-7B (Touvron et al., 2023b). We additionally present results from GPT-3.5- text-davinci-003 (hereinafter referred to as GPT-3.5-D) and GPT-3.5-turbo-0301 (here- inafter referred to as GPT-3.5-T) to show the performance gap.3\n",
      "Zero-Shot Evaluation We conduct zero-shot evaluations on 5 English-centric language pairs, considering both from English and to English directions: German (de), Czech (cs), Icelandic (is), Chinese (zh) and Russian (ru), where Icelandic test data is from WMT’21 and the oth- ers are from WMT’22. We choose these test dataset because they are the recent and less likely\n",
      "3https://beta.openai.com/docs/model-index-for-researchers\n",
      "3\n",
      "(1)\n",
      "(2)\n",
      "Published as a conference paper at ICLR 2024\n",
      "to overlap the training data used by LLMs, and importantly, they have high-quality data to avoid problems of “translationese” (Zhang & Toral, 2019). The beam size is 5. We report sacre- BLEU (zh tokenizer for Chinese and 13a for the others) (Post, 2018). We also report COMET (Unbabel/wmt22-comet-da) (Rei et al., 2022) because BLEU only reflects the degree of lex- ical match. In this paper, We rely more on COMET than BLEU due to its better alignment with human evaluations (Freitag et al., 2022).4\n",
      "0\n",
      "MPT-7B\n",
      "xxen\n",
      "35BLEU\n",
      "30\n",
      "LLaMA-2-7B\n",
      "5\n",
      "10\n",
      "LLaMA-1-7B\n",
      "OPT-7B\n",
      "GPT-3.5-TModels\n",
      "15\n",
      "10.0412.8018.5023.0123.9026.4334.1637.363.417.6411.6614.5915.9013.8628.9632.17\n",
      "GPT-3.5-D\n",
      "BLOOM-7B\n",
      "enxx\n",
      "20\n",
      "25\n",
      "Falcon-7B\n",
      "80\n",
      "GPT-3.5-D\n",
      "enxx\n",
      "60\n",
      "70\n",
      "50\n",
      "GPT-3.5-TModels\n",
      "61.1064.3771.1175.0175.4377.1683.9085.4650.6759.8060.9467.4768.5468.8884.5986.56\n",
      "LLaMA-1-7B\n",
      "OPT-7B\n",
      "LLaMA-2-7B\n",
      "xxen\n",
      "90COMET\n",
      "MPT-7B\n",
      "BLOOM-7B\n",
      "Falcon-7B\n",
      "(a) BLEU\n",
      "(b) COMET\n",
      "Figure 3: Averaged zero-shot translation performance on 10 directions: cs↔en, de↔en, is↔en, zh↔en, ru↔en, where is↔en is from WMT’21 test data and the others from WMT’22 test data.\n",
      "LLM Translation Performance The overall results for the LLMs are presented in Figure 3, with scores averaged across five languages for translations to and from English. Among the 7B LLMs, LLaMA-2-7B exhibits superior performance translating into English, while MPT-7B leads in trans- lations out of English, as measured by BLEU. However, when evaluated with COMET, LLaMA-2- 7B wins in both directions. We show the numeric results in Appendix B. Consequently, we select LLaMA-2-7B and MPT-7B for further investigation into the necessity of parallel data for LLMs.\n",
      "3 DO LLMS HAVE AN APPETITE FOR PARALLEL DATA?\n",
      "Conventional machine translation training predominantly relies on utilizing large volumes of parallel datasets within encoder-decoder frameworks. This trend is not confined to training models from scratch but also pertains to strategies that fine-tune pre-trained LLMs, often involving millions of parallel sentences (Rothe et al., 2020; Liu et al., 2020; Xu et al., 2021; 2023; Yang et al., 2023). In this section, we examine whether the recently proposed decoder-only LLMs retain a dependence on substantial parallel data and adhere to the traditional training paradigm.\n",
      "3.1 EXPERIMENTAL DESIGN\n",
      "Following Section 2.2, our focus narrows to fine-tuning LLaMA-2-7B and MPT-7B. To allow for a deep analysis, we concentrate on one language pair, English→Russian (en→ru). We opted for a language pair that is translating out of English and to a non-Latin language, since those categories show larger gaps with SoTA models in our initial investigation in Section 2.2. We use the clean data filtered from 75M parallel sentences from Hendy et al. (2023) and split the data size in 5 levels: 10K, 100K, 1M, 5M, and 20M. We use the same prompt template and training scheme as described in Section 2.1, and train the model by updating all parameters. Detailed training settings can be found in Appendix C.\n",
      "3.2 OBSERVATIONS\n",
      "The fine-tuning results for LLaMA-2-7B and MPT-7B at each data size step are presented in Figure 4. Additionally, we benchmark these against the performance of the NLLB-54B model to show the disparity with one of the SoTA multilingual translation models.\n",
      "4According to Freitag et al. (2022), COMET holds the 2-nd position in alignment with human ratings,\n",
      "whereas BLEU is situated at the 19-th spot among 20 metrics\n",
      "4\n",
      "Published as a conference paper at ICLR 2024\n",
      "0 (zero-shot)\n",
      "NLLB-54B\n",
      "1M\n",
      "22.5\n",
      "17.5\n",
      "5M\n",
      "20.0\n",
      "100K\n",
      "25.0\n",
      "20MTraining Data Size\n",
      "10K\n",
      "LLaMA-2-7B\n",
      "Random Init\n",
      "30.0BLEU\n",
      "MPT-7B\n",
      "27.5\n",
      "15.0\n",
      "0 (zero-shot)\n",
      "87.5COMET\n",
      "100K\n",
      "82.5\n",
      "80.0\n",
      "NLLB-54B\n",
      "77.5\n",
      "Random Init\n",
      "LLaMA-2-7B\n",
      "5M\n",
      "72.5\n",
      "85.0\n",
      "75.0\n",
      "20MTraining Data Size\n",
      "1M\n",
      "10K\n",
      "MPT-7B\n",
      "(a) BLEU\n",
      "(b) COMET\n",
      "Figure 4: BLEU and COMET scores obtained during the fine-tuning of MPT-7B and LLaMA-2- 7B across each data step for en→ru. Additionally, we present the results for NLLB-54B and a 7B model trained from scratch. A notable decline in LLaMA-2-7B’s COMET score suggests that substantial parallel data might dilute its pre-existing knowledge.\n",
      "Small Training Data Is Enough According to COMET, there is a notable difference in the curve of LLaMA-2-7B and MPT-7B: LLaMA-2-7B peaks with 10K and 100K training data before experienc- ing a decline, while MPT-7B exhibits continuous improvement with more training data. LLaMA- 2-7B requires only limited training examples (10K and 100K) to achieve competent translation. However, a surplus of examples (5M or 20M) seems to dilute its existing knowledge in Russian. Conversely, MPT-7B, potentially due to its inherently weaker translation capability, exhibits im- proved performance with an increase in training data. This may suggest that LLaMA-2 or other well-trained LLMs may not necessitate substantial parallel data.\n",
      "Large Parallel Data Wash Out the Knowledge Both LLMs eventually achieve similar BLEU and COMET with 20M training data, regardless of their performance on smaller data. We hypothesize that this phenomenon is caused by catastrophic forgetting (French, 1999; Kirkpatrick et al., 2017), suggesting that too many parallel data wash out the pre-existing knowledge. To validate this hy- pothesis, we consider an extreme case: training the model from scratch using 20M data, thereby erasing all prior knowledge.5 As expected, it tends up with a similar performance in both BLEU and COMET evaluations (triangle in Figure 4), strengthening our speculation regarding the dilution of LLM’s intrinsic knowledge with extensive data training.\n",
      "Beyond BLEU COMET reveals a decline in translation performance for LLaMA-2-7B as the amount of parallel data increases, a trend not captured by BLEU which shows an increase. This discrepancy arises since BLEU primarily evaluates lexical overlap, and the extensive WMT training data, being similar in domain to the test set, likely enhances this measure. This highlights the neces- sity of utilizing additional metrics (like COMET) for a comprehensive evaluation of translation.\n",
      "From our observations, LLaMA-2 (potentially other well-trained LLMs) should not adopt the same training approach as earlier models—-whether randomly initialized or pre-trained—that rely heavily on vast amounts of training data.\n",
      "4 A NEW TRAINING RECIPE\n",
      "We demonstrate that LLMs like LLaMA-2-7B do not voraciously consume parallel data. We in- troduce a novel training strategy that markedly enhances translation performance without relying heavily on parallel data. The recipe comprises two stages: continuous monolingual data fine- tuning and high-quality parallel data fine-tuning. After applying our training recipe to LLMs, we name the resulting model as ALMA (Advanced Language Model-based trAnslator).\n",
      "Monolingual Data Fine-tuning LLMs like LLaMA are pre-trained on English-dominated corpora. This potentially explains their inadequate translation performance which necessitates cross-lingual capabilities. To ameliorate this, our first stage is fine-tuning LLMs with monolingual data of non- English languages involved in translation tasks, enhancing their proficiency in these languages. Note\n",
      "5We initialize parameters randomly based on the LLaMA-2-7B model, but use the same vocabulary.\n",
      "5\n",
      "Published as a conference paper at ICLR 2024\n",
      "that we also add English monolingual data during fine-tuning to prevent English knowledge forget- ting. Previous studies also offer some clues that monolingual data help in translation. For instance, Tan et al. (2023) utilizes a monolingual target corpus to bridge the gap in translation mismatches caused by domain discrepancies. BigTranslate (Yang et al., 2023) and PolyLM (Wei et al., 2023) use a huge amount of Chinese monolingual data and improve translation to or from Chinese. Fur- thermore, Li et al. (2023) utilizes monolingual generation instructions to improve translation. In Section 6.1, we show that utilizing small monolingual data and modest computational cost (e.g., 1B monolingual tokens mixed by 6 languages and fine-tuning under 18 hours), can facilitate significant improvements in 10 translation directions. Note that we employ full-weight fine-tuning at this stage.\n",
      "High-Quality Data Fine-tuning Drawing on insights from Section 3.2 that LLMs may require only small parallel data, coupled with previous research emphasizing training data quality (Zhou et al., 2023; Maillard et al., 2023; Gunasekar et al., 2023), we fine-tune the model using a small, yet high-quality parallel dataset in this stage. To ensure the data quality, we collect human-written datasets from WMT test data and Flores-200 (NLLB TEAM et al., 2022) development and test sets. Here, we explore both full-weight and lightweight Low-Rank Adaptation (LoRA) (Hu et al., 2022; Mangrulkar et al., 2022) fine-tuning, where we apply LoRA to the down-projection layer in each feed-forward network.\n",
      "5 EXPERIMENTS\n",
      "5.1 DATA\n",
      "For our parallel training data, we collect human-written test datasets from WMT’17 to WMT’20, plus the development and test sets from Flores-200 (NLLB TEAM et al., 2022), resulting in a total of 58K training examples across all languages. For the test data, we still use the same 10 transla- tion directions to be consistent with our study in Section 2: cs↔en, de↔en, is↔en, zh↔en, ru↔en, where is↔en is from WMT’21 and the others are from WMT’22. Test data in WMT’21 (except for is) is used for the development dataset (a total of 8K parallel sentences).6 The monolin- gual dataset is sourced from OSCAR (Ortiz Su’arez et al., 2019; Kreutzer et al., 2022). We mix the monolingual data and fine-tune the model with a sampling ratio of 20%, 14%, 8%, 19%, 22%, and 17% respectively for de, cs, is, zh, ru and en. We explain the reasoning behind the sampling ratios and show the detailed parallel data information in Appendix D.\n",
      "5.2 TRAINING SETUP\n",
      "We train the model in a many-to-many multilingual translation manner, and use LLaMA-2-7B (or 13B) as our backbone model given its best zero-shot performance. Our two-stage fine-tuning process yields two model types, differentiated based on the utilization of LoRA:\n",
      "ALMA-7B/ALMA-13B Full-Weight fine-tuning on monolingual data followed by Full-Weight fine- tuning on high-quality parallel data for LLaMA-2-7B or -13B models.\n",
      "ALMA-7B-LoRA/ALMA-13B-LoRA Full-Weight fine-tuning on monolingual data followed by LoRA fine-tuning on high-quality parallel data for LLaMA-2-7B or -13B models.\n",
      "If using LoRA, the LoRA rank is 16 and only updates 0.1% parameters (7.7M for 7B and 12M for 13B model). Both monolingual data fine-tuning and human-written data fine-tuning basically share the same hyperparameter settings. Specifically, we fine-tune LLaMA-2 with a batch size of 256, a warm-up ratio of 0.01, and a sequence containing a maximum of 512 tokens. For monolingual data fine-tuning, we train the LLaMA-2-7B up to 20B tokens and LLaMA-2-13B up to 12B tokens. However, it is very likely that the model would be better in translation with more monolingual data fine-tuning. For human-written data fine-tuning, we train the model for 2 epochs (enough to see a clear convergence) and pick the best model with the lowest validation loss. For both stages, we adopt deepspeed (Rasley et al., 2020) to accelerate our training.\n",
      "6There is no development dataset for Icelandic.\n",
      "6\n",
      "Published as a conference paper at ICLR 2024\n",
      "5.3 BASELINES\n",
      "We evaluate our method against two baseline categories. First, we consider prior studies with the goal aligning with ours: leveraging LLMs for translation. Secondly, we benchmark against the current SoTA translation models. It’s worth noting that this comparison isn’t entirely fair due to discrepancies in training data and model architectures (e.g., 175B GPT-3.5 vs. our 7B models). Nevertheless, utilizing the same test set provides insights into our model’s current standing.\n",
      "Prior Similar Work We compare our model with BigTranslate (Yang et al., 2023), which extends LLaMA-1-13B to over 100 translation directions; TIM (Zeng et al., 2023), which uses correct and incorrect examples to help LLM to learn translation; SWIE (Chen et al., 2023), which improves LLM in translation via instruction augmentation; and BayLing (Zhang et al., 2023b), which uses interactive translation instructions. Given that the same test data and evaluation metrics are utilized, we directly report BLEU and COMET from their papers (except for BigTranslate, we assess their released model using the prompt they provided).\n",
      "SoTA Models We consider the NLLB-54B model, which is the largest and best translation model released in the NLLB family (NLLB TEAM et al., 2022); and the zero-shot performance of GPT- 3.5-text-davinci-003 (GPT-3.5-D) and GPT-3.5-turbo-0301 (GPT-3.5-T). Additionally, we present the zero-shot results for GPT-4.7\n",
      "Models\n",
      "de\n",
      "cs\n",
      "is\n",
      "zh\n",
      "ru\n",
      "Avg.\n",
      "BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET\n",
      "NLLB-54B GPT-3.5-D, zero-shot GPT-3.5-T, zero-shot GPT-4, zero-shot\n",
      "34.50 31.80 34.40 35.38\n",
      "TIM-BLOOMZ-7B TIM-LLaMA-1-7B SWIE-BLOOMZ-7B SWIE-LLaMA-1-7B BigTranslate-13B Bayling-13B\n",
      "20.63 25.59 21.83 27.21 21.48 25.62\n",
      "LLaMA-2-7B, zero-shot ALMA-7B (Ours) ALMA-7B-LoRA (Ours)\n",
      "19.00 30.31 30.16\n",
      "13.69 LLaMA-2-13B, zero-shot ALMA-13B (Ours) 31.37 ALMA-13B-LoRA (Ours) 31.47\n",
      "SoTA Models 24.15 15.90 18.74 - Prior Similar Studies - - - - - - - - 2.28 80.65 - 78.22 Our Recipe with Backbone Model: LLaMA-2-7B 1.33 25.71 25.19 Our Recipe with Backbone Model: LLaMA-2-13B 2.36 26.67 26.68\n",
      "86.45 85.61 87.00 87.44\n",
      "27.38 38.30 44.90 43.98\n",
      "37.60 31.30 32.92 34.53\n",
      "81.76 76.28 81.04 -\n",
      "90.15 88.57 90.17 90.77\n",
      "37.20 19.33 36.88 31.24 28.56 37.92\n",
      "74.16 82.56 75.17 82.36 78.81 82.69\n",
      "- - - 35.56 - 43.83 85.52 85.44\n",
      "- - - 20.67 16.43\n",
      "- - - 35.56 - 43.83 85.52 85.44\n",
      "76.39 85.59 85.45\n",
      "16.97 36.48 36.47\n",
      "16.02 29.88 30.17\n",
      "79.13 89.10 89.05\n",
      "0.87 31.12 32.38\n",
      "38.47 85.85 86.08\n",
      "75.55 85.45 85.62\n",
      "68.57 89.42 89.79\n",
      "78.91 85.76 87.00 87.49\n",
      "84.89 75.46 84.53 80.63 81.31 84.62\n",
      "71.80 85.05 84.87\n",
      "79.70 85.76 85.96\n",
      "30.96 27.50 29.90 30.45\n",
      "- - - 17.66 12.77 16.00 27.09 26.93\n",
      "0.59 28.76 28.96\n",
      "87.92 86.74 87.60 88.87\n",
      "- - - 78.21 71.01 73.24 87.17 87.05\n",
      "63.84 87.50 87.53\n",
      "30.92 28.96 32.17 -\n",
      "- - - 18.13 - 13.86 29.89 29.78\n",
      "9.50 31.39 31.87\n",
      "85.04 84.59 86.56 -\n",
      "- - - 70.91 - 68.88 86.49 86.37\n",
      "65.23 86.80 87.00\n",
      "Table 1: The overall results in en→xx. ALMA models significantly outperform all prior simi- lar studies and are comparable to SoTA models. We categorize BLEU and COMET scores into three groups: scores that are more than 10 points below the higher value of GPT-4/GPT-3.5-T are emphasized in dark red boxes, those that are more than 5 points below are emphasized in shallow red boxes, and all other scores are emphasized in green boxes. Bold numbers represent the highest scores among ALMA models and prior similar studies.\n",
      "5.4 RESULTS\n",
      "We show our main results of en→xx and xx→en respectively in Table 1 and 2. In summary, our best system (ALMA-13B-LoRA) outperforms all previous studies, NLLB-54B, and GPT-3.5-D, while it marginally underperforms compared to GPT-3.5-T and GPT-4.\n",
      "Comparing With LLaMA-2 Zero-Shot For all 10 translation directions and both 7B and 13B mod- els, LLaMA-2 trained by our recipe significantly outperforms its original zero-shot performance. For instance, ALMA-7B achieves +16.12 BLEU and +17.61 COMET for en→xx on average. It is worth noting that LLaMA-2-13B suffers from the off-target issue in en→xx zero-shot translation. However, it can be substantially alleviated by few-shot in-context learning (Brown et al., 2020), but still largely lag behind our methods (e.g., over 10 BLEU and COMET when translating from English). We discuss this further in Appendix E.\n",
      "7GPT-4 results are sourced from Zhang et al. (2023b).\n",
      "7\n",
      "Published as a conference paper at ICLR 2024\n",
      "Models\n",
      "de\n",
      "cs\n",
      "is\n",
      "zh\n",
      "ru\n",
      "Avg.\n",
      "BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET\n",
      "NLLB-54B GPT-3.5-D, zero-shot GPT-3.5-T, zero-shot GPT-4, zero-shot\n",
      "26.89 30.90 33.10 33.87\n",
      "TIM-BLOOMZ-7B TIM-LLaMA-1-7B SWIE-BLOOMZ-7B SWIE-LLaMA-1-7B BigTranslate-13B Bayling-13B\n",
      "24.31 27.91 25.95 30.48 23.35 27.34\n",
      "LLaMA-2-7B, zero-shot ALMA-7B (Ours) ALMA-7B-LoRA (Ours)\n",
      "30.42 29.49 29.56\n",
      "31.06 LLaMA-2-13B, zero-shot ALMA-13B (Ours) 30.73 ALMA-13B-LoRA (Ours) 31.14\n",
      "SoTA Models 23.09 31.90 37.50 - Prior Similar Studies - - - - - - - - 6.51 81.19 - 81.65 Our Recipe with Backbone Model: LLaMA-2-7B 10.98 35.26 35.64 Our Recipe with Backbone Model: LLaMA-2-13B 15.77 36.46 36.95\n",
      "78.94 84.79 85.50 85.62\n",
      "39.11 44.50 47.20 48.67\n",
      "80.13 86.16 87.30 87.43\n",
      "71.66 82.13 85.50 -\n",
      "16.56 25.00 26.60 27.20\n",
      "23.42 19.33 23.40 21.30 14.16 20.12\n",
      "77.65 82.80 78.80 82.97 80.68 83.02\n",
      "- - - 33.67 33.87\n",
      "- - - 54.71 - 62.79 85.97 86.09\n",
      "- - - 54.71 - 62.79 85.97 86.09\n",
      "82.74 83.98 83.95\n",
      "36.56 42.91 43.49\n",
      "18.19 23.52 23.64\n",
      "82.42 85.90 85.93\n",
      "83.01 84.42 84.56\n",
      "21.81 24.65 25.46\n",
      "66.35 86.30 86.42\n",
      "83.27 86.29 86.47\n",
      "70.70 81.62 82.90 82.79\n",
      "79.50 75.46 79.36 76.48 74.26 77.72\n",
      "75.00 79.73 79.78\n",
      "78.10 79.90 80.21\n",
      "39.11 38.50 42.40 43.51\n",
      "- - - 26.81 33.95 36.02 38.93 39.21\n",
      "36.50 40.37 40.27\n",
      "81.88 84.80 86.10 86.18\n",
      "- - - 77.80 82.07 82.84 84.81 84.84\n",
      "82.91 85.09 85.27\n",
      "28.95 34.16 37.36 -\n",
      "- - - 20.90 - 26.43 34.02 34.31\n",
      "29.03 35.38 35.82\n",
      "76.66 83.90 85.46 -\n",
      "- - - 73.80 - 77.16 84.08 84.12\n",
      "78.73 84.40 84.59\n",
      "Table 2: The overall results in xx→en. ALMA models significantly outperform all prior similar studies and are comparable to SoTA models. The color and boldface are the same in Table 1.\n",
      "Compared with Prior Similar Studies ALMA significantly outperforms all prior studies. Big- Translate, which is fine-tuned on Chinese corpus and 300M parallel corpus, struggles to surpass LLaMA-2’s zero-shot performance, except for en→zh. This observation also aligns with our find- ings that an excessive amount of parallel data may damage the model, whereas target monolingual data is helpful to translation. Both TIM and SWIE specifically target two high-resource languages, de and zh. Their performance, however, is predominantly determined by their backbone models: effective translation is observed for zh but is lackluster for de when using BLOOMZ, and vice versa with LLaMA-1. In contrast, ALMA is versatile, showcasing strong results across all directions.\n",
      "Compared with SoTA models Our best model (ALMA-13B-LoRA) substantially outperforms NLLB-54B and GPT-3.5-D on average. In en→xx direction, it even outperforms GPT-3.5-T on average COMET (87.00 vs. 86.56) and has close performance when it comes to xx→en. Notably, SoTA models typically excel with high-resource languages but falter with low-resource languages such as is. With our recipe, the performance of is remains strong and performs the best.\n",
      "17.5\n",
      "12B\n",
      "2B\n",
      "9B\n",
      "20BModel\n",
      "6B\n",
      "15.0\n",
      "11B\n",
      "30.0\n",
      "25.0\n",
      "14B\n",
      "8B\n",
      "18B\n",
      "4B\n",
      "27.5\n",
      "NLLB-54B\n",
      "1B\n",
      "20.0\n",
      "0B (zero-shot)\n",
      "10B\n",
      "15B\n",
      "17B\n",
      "16B\n",
      "22.5\n",
      "29.9431.5620.1530.1230.7331.2531.2631.5731.4730.9831.1531.0130.8031.3531.6331.6531.7031.8931.8931.7631.7232.0531.96\n",
      "13B\n",
      "7B\n",
      "3B\n",
      "GPT-3.5-D\n",
      "19B\n",
      "5B\n",
      "32.5BLEU\n",
      "86COMET\n",
      "7B\n",
      "11B\n",
      "13B\n",
      "80.8584.2573.0284.4184.6384.8284.9585.0584.9284.8984.8684.8884.8185.0185.0885.0985.1485.1185.2085.0785.1885.2185.28\n",
      "4B\n",
      "18B\n",
      "10B\n",
      "15B\n",
      "80\n",
      "16B\n",
      "GPT-3.5-D\n",
      "9B\n",
      "8B\n",
      "82\n",
      "76\n",
      "6B\n",
      "74\n",
      "NLLB-54B\n",
      "84\n",
      "19B\n",
      "72\n",
      "70\n",
      "3B\n",
      "20BModel\n",
      "1B\n",
      "0B (zero-shot)\n",
      "78\n",
      "17B\n",
      "12B\n",
      "5B\n",
      "14B\n",
      "2B\n",
      "(a) BLEU\n",
      "(b) COMET\n",
      "Figure 5: The average performance of ALMA-7B at the completion of each 1B-token fine-tuning. The scores in the figure are averaged across 10 directions\n",
      "6 ANALYSES\n",
      "6.1 HOW MUCH MONOLINGUAL DATA TO USE?\n",
      "In our main results, we present ALMA with our best settings, fine-tuned on either 20B or 12B tokens. Yet, we snapshot all ALMA models after every 1B monolingual tokens (and human-written parallel data) they have been fine-tuned with, and evaluate all their translation performance. As illustrated\n",
      "8\n",
      "Published as a conference paper at ICLR 2024\n",
      "Use Mono.\n",
      "Parallel Data Quality\n",
      "Avg. xx→en\n",
      "Avg. en→xx\n",
      "✘ ✘ ✘ ✘ ✔ ✔ ✔ ✔\n",
      "✘ Random Filtered HW ✘ Random Filtered HW\n",
      "BLEU COMET BLEU COMET 26.43 28.24 28.39 29.39 28.49 32.47 32.32 34.02\n",
      "77.16 78.69 78.94 80.00 80.32 83.02 83.03 84.08\n",
      "13.86 19.68 19.56 22.17 26.35 26.98 27.38 29.89\n",
      "68.88 73.89 74.35 76.52 84.73 83.15 83.98 86.49\n",
      "Table 3: Ablation study on the effect of monolingual data and parallel data quality. The backbone model is LLaMA-2-7B. A red cross (✘) in the table denotes the omission of monolingual data fine- tuning or parallel data (indicative of zero-shot translation). A green check (✔) signifies that the model undergoes fine-tuning with monolingual data.\n",
      "in Figure 5, we report the ALMA-7B’s average performance across all directions after fine-tuning every 1B tokens. The test dataset remains the same, i.e., the 10 aforementioned directions. We provide detailed numeric results and similar analysis for ALMA-13B to Appendix F. Importantly, merely fine-tuning on 1B monolingual tokens, followed by fine-tuning on human-written data, yields performance comparable to NLLB-54B and GPT-3.5-D. In practice, we employ 16 MI200 GPUs with a batch size of 256 and sequence length of 512, which requires only 18 hours to complete the fine-tuning of 1B tokens and an additional hour allocated for human-written data fine-tuning. It takes around 19 hours of training to have a strong MMT model.\n",
      "6.2 THE EFFECT OF MONOLINGUAL DATA AND PARALLEL DATA QUALITY\n",
      "To scrutinize the impact of monolingual data, we juxtapose LLaMA-2-7B models fine-tuned with and without monolingual data (20B tokens), while keeping the same parallel data. Furthermore, to evaluate the impact of parallel data quality, we introduce three distinct parallel datasets for stage 2 fine-tuning. The first dataset is the human-written data (HW) utilized in prior experiments. The sec- ond is the filtered data (Filtered) referenced in Section 3.1. Lastly, we employ a randomly selected dataset (Random) sourced from the comprehensive WMT data. We anticipate the quality hierarchy as HW, followed by Filtered, and lastly, Random. For both Filtered and Random, each translation direction has 10K parallel data, aligning the total training dataset size with HW. We show the abla- tion results in Table 3. Using the LLaMA-2-7B as our foundational model, it’s evident that with the same parallel data, incorporation of monolingual data largely enhances translation results, e.g., an increase from 74.35 to 83.98 in en→xx COMET scores when training on the same Filtered data. Moreover, regardless of the monolingual data’s presence, models fine-tuned with higher-quality data exhibit better performance. Both monolingual and human-written data emerge as critical factors in improving translation. Detailed results for each language pair are deferred to the Appendix G.\n",
      "6.3 OTHER ANALYSES\n",
      "We also explore additional in-depth analyses and elaborate on them in the appendix: 1) The impact of the volume and domain of human-written data on translation performance is explored in Appendix H; 2) A comparison between stage 2 fine-tuning (parallel data fine-tuning) and in-context few-shot learning can be found in Appendix I; 3) An evaluation of the zero-shot cross-lingual capabilities of LLaMA-2 after stage 1 fine-tuning on other tasks is presented in Appendix J.\n",
      "7 CONCLUSION\n",
      "In this paper, we show that LLMs do not require as extensive a collection of parallel data as tradi- tional translation models do. Subsequently, we introduce a novel training recipe for decoder-only LLMs in translation, resulting in strong translation models, ALMA. When using our LLaMA-2 as our foundational model, ALMA exceeds the zero-shot translation performance of LLaMA-2 by more than 12 BLEU and COMET scores across 10 directions on average. Moreover, ALMA models surpass all preceding studies and even outperform NLLB-54B and GPT-3.5-D.\n",
      "9\n",
      "Published as a conference paper at ICLR 2024\n",
      "ACKNOWLEDGMENTS\n",
      "We extend our gratitude to Hieu Hoang, Marcin Junczys-Dowmunt, Yunmo Chen, Steven Tan, Huda Khayrallah, Thamme Gowda, Vikas Raunak, Matt Post, Anoop Kunchukuttan, Roman Grund- kiewicz, Tom Kocmi, Kenton Murray and Arul Menezes for their insightful and valuable sugges- tions.\n",
      "REFERENCES\n",
      "Roee Aharoni, Melvin Johnson, and Orhan Firat. Massively multilingual neural machine translation.\n",
      "arXiv preprint arXiv:1903.00089, 2019.\n",
      "Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co- jocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large lan- guage model with state-of-the-art performance. 2023.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.\n",
      "Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou.\n",
      "Improving translation faithfulness of large language models via augmenting instructions. arXiv preprint arXiv:2308.12674, 2023.\n",
      "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n",
      "Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053, 2018.\n",
      "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un- In Proceedings of the 58th Annual supervised cross-lingual representation learning at scale. Meeting of the Association for Computational Linguistics, pp. 8440–8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https: //aclanthology.org/2020.acl-main.747.\n",
      "Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and Andr´e F. T. Martins. Results of WMT22 metrics shared task: Stop using BLEU – neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 46–68, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https: //aclanthology.org/2022.wmt-1.2.\n",
      "Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences,\n",
      "3(4):128–135, 1999.\n",
      "Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot lan- guage model evaluation, September 2021. URL https://doi.org/10.5281/zenodo. 5371628.\n",
      "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C´esar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.\n",
      "Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Mat- sushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210, 2023.\n",
      "10\n",
      "Published as a conference paper at ICLR 2024\n",
      "Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Con- ference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9.\n",
      "Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. Is chatgpt a good\n",
      "translator? a preliminary study. arXiv preprint arXiv:2301.08745, 2023.\n",
      "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcom- ing catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\n",
      "Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoˆıt Sagot, Clara Rivera, Annette Rios, Isabel Pa- padimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo Rubungo, Toan Q. Nguyen, Mathias M¨uller, Andr´e M¨uller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaven- ture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine C¸abuk Ballı, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayo- dele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50–72, 2022. doi: 10.1162/ tacl a 00447. URL https://aclanthology.org/2022.tacl-1.4.\n",
      "Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Chen, and Jiajun Chen. Eliciting the translation ability of large language models via multilingual finetuning with translation instructions. arXiv preprint arXiv:2305.15083, 2023.\n",
      "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. Few-shot learning with multilingual language models. arXiv preprint arXiv:2112.10668, 2021.\n",
      "Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950–1965, 2022.\n",
      "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine trans- lation. Transactions of the Association for Computational Linguistics, 8:726–742, 2020. doi: 10.1162/tacl a 00343. URL https://aclanthology.org/2020.tacl-1.47.\n",
      "Jean Maillard, Cynthia Gao, Elahe Kalbassi, Kaushik Ram Sadagopan, Vedanuj Goswami, Philipp Koehn, Angela Fan, and Francisco Guzm´an. Small data, big impact: Leveraging minimal data for effective machine translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2740–2756, 2023.\n",
      "Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul. https://github.com/\n",
      "Peft: State-of-the-art parameter-efficient fine-tuning methods. huggingface/peft, 2022.\n",
      "MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.\n",
      "URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\n",
      "Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. Few- arXiv preprint\n",
      "shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv:2305.16938, 2023.\n",
      "Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. LS- In Proceedings of the 2nd Workshop on Link- DSem 2017 shared task: The story cloze test. ing Models of Lexical, Sentential and Discourse-level Semantics, pp. 46–51, Valencia, Spain,\n",
      "11\n",
      "Published as a conference paper at ICLR 2024\n",
      "April 2017. Association for Computational Linguistics. doi: 10.18653/v1/W17-0906. URL https://aclanthology.org/W17-0906.\n",
      "Marta R NLLB TEAM, Costa-juss`a, James Cross, Onur C¸elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.\n",
      "OpenAI. Gpt-4 technical report, 2023.\n",
      "Pedro Javier Ortiz Su’arez, Benoit Sagot, and Laurent Romary. Asynchronous pipelines for process- ing huge corpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pp. 9 – 16, Mannheim, 2019. Leibniz-Institut f”ur Deutsche Sprache. doi: 10.14618/ids-pub-9021. URL http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215.\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311–318, 2002.\n",
      "Matt Post. A call for clarity in reporting BLEU scores.\n",
      "In Proceedings of the Third Con- ference on Machine Translation: Research Papers, pp. 186–191, Brussels, Belgium, Octo- ber 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://aclanthology.org/W18-6319.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020.\n",
      "Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System opti- mizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 3505–3506, 2020.\n",
      "Vikas Raunak, Arul Menezes, and Hany Awadalla. Dissecting in-context learning of translations in GPT-3. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 866–872, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.61. URL https:// aclanthology.org/2023.findings-emnlp.61.\n",
      "Ricardo Rei, Jos´e G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr´e F. T. Martins. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pp. 578–585, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. wmt-1.52.\n",
      "Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation tasks. Transactions of the Association for Computational Linguistics, 8: 264–280, 2020. doi: 10.1162/tacl a 00313. URL https://aclanthology.org/2020. tacl-1.18.\n",
      "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagn´e, Alexandra Sasha Luccioni, Franc¸ois Yvon, Matthias Gall´e, et al. Bloom: A 176b- parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n",
      "Weiting Tan, Haoran Xu, Lingfeng Shen, Shuyue Stella Li, Kenton Murray, Philipp Koehn, Ben- jamin Van Durme, and Yunmo Chen. Narrowing the gap between zero- and few-shot machine translation by matching styles, 2023.\n",
      "Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. In The Eleventh International Conference on Learning Representations, 2022a.\n",
      "12\n",
      "Published as a conference paper at ICLR 2024\n",
      "Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q Tran, David R So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending scaling laws with 0.1% extra compute. arXiv preprint arXiv:2210.11399, 2022b.\n",
      "Alexey Tikhonov and Max Ryabinin.\n",
      "It’s All in the Heads: Using Attention Heads as a Base- line for Cross-Lingual Transfer in Commonsense Reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 3534–3546, Online, August 2021. Associ- ation for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.310. URL https: //aclanthology.org/2021.findings-acl.310.\n",
      "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko- lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\n",
      "Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective works best for zero-shot generalization? In International Conference on Machine Learning, pp. 22964–22984. PMLR, 2022.\n",
      "Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, et al. Polylm: An open source polyglot large language model. arXiv preprint arXiv:2307.06018, 2023.\n",
      "Haoran Xu, Benjamin Van Durme, and Kenton Murray. BERT, mBERT, or BiBERT? a study on In Proceedings of the 2021 Con- contextualized embeddings for neural machine translation. ference on Empirical Methods in Natural Language Processing, pp. 6663–6675, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.emnlp-main.534. URL https://aclanthology.org/2021. emnlp-main.534.\n",
      "Haoran Xu, Jean Maillard, and Vedanuj Goswami. Language-aware multilingual machine transla- tion with self-supervised learning. In Findings of the Association for Computational Linguistics: EACL 2023, pp. 526–539, Dubrovnik, Croatia, May 2023. Association for Computational Lin- guistics. doi: 10.18653/v1/2023.findings-eacl.38. URL https://aclanthology.org/ 2023.findings-eacl.38.\n",
      "Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. Bigtrans: Augmenting large lan- arXiv preprint\n",
      "guage models with multilingual translation capability over 100 languages. arXiv:2305.18098, 2023.\n",
      "Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou. Tim: Teaching large language models to\n",
      "translate with comparison. arXiv preprint arXiv:2307.04408, 2023.\n",
      "Mike Zhang and Antonio Toral. The effect of translationese in machine translation test sets.\n",
      "In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pp. 73–81, Florence, Italy, August 2019. Association for Computational Linguistics. doi: 10.18653/ v1/W19-5208. URL https://aclanthology.org/W19-5208.\n",
      "Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init atten- tion. arXiv preprint arXiv:2303.16199, 2023a.\n",
      "Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, et al. Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models. arXiv preprint arXiv:2306.10968, 2023b.\n",
      "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo- pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n",
      "13\n",
      "Published as a conference paper at ICLR 2024\n",
      "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n",
      "Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian Huang. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675, 2023a.\n",
      "Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan, Jingjing Xu, Shujian Huang, Lingpeng Kong, Ji- ajun Chen, and Lei Li. Extrapolating large language models to non-english by aligning languages. arXiv preprint arXiv:2308.04948, 2023b.\n",
      "14\n",
      "Published as a conference paper at ICLR 2024\n",
      "A COMPARING LLM TRAINING OBJECTIVES FOR MACHINE TRANSLATION\n",
      "We evaluate three potential training objectives for decoder-only LLM in machine translation.\n",
      "Causal Language Modeling (CLM) We first consider a standard language modeling loss that predicts the next token based on all prior tokens.\n",
      "Prefix Language Modeling (Prefix LM) For decoder-only models, a prefix can be defined with a non-causal attention mask. Analogous to standard language modeling, the model predicts each token outside the prefix based on previous tokens. In the context of machine translation, the provided prompt serves as the prefix, as depicted in Figure 2.\n",
      "Mixture-of-Denoisers (MoD) The UL2 model (Tay et al., 2022a) introduces a unified approach to masking methods, utilizing a mixture-of-denoisers (MoD) strategy, which has also been imple- mented in the fine-tuning of PaLM (Tay et al., 2022b). This strategy is grounded in three objectives:\n",
      "Regular Denoising: In this approach, noise is sampled in spans and replaced with sentinel tokens, aligning with the standard span corruption technique delineated in Raffel et al. (2020). The parameters set for this objective include a mean of 3 and a corruption rate of 15\n",
      "Extreme Denoising: This method amplifies the noise to a comparatively ’extreme’ level, characterized by a mean length of 32 and a corruption rate reaching up to 25\n",
      "Sequential Denoising: This is known as the Prefix LM objective previously mentioned.\n",
      "In our training process, we allocate a 25% probability each for both regular and extreme denoising, and a 50% probability for sequential denoising.\n",
      "We employ the MPT-7B as our backbone model. Our investigation considers four distinct training data sizes: 0 (zero-shot), 100K, 1M, and 5M, with translation directed from Russian to English. We use the parallel dataset previously described in Section 3.1. For each data size, the MPT-7B is fine-tuned using the corresponding training objective, noting that all trainings utilize full-weight fine-tuning.\n",
      "The results of the comparison between training objectives can be viewed in Figure 6. Although three objectives end up with similar performance under 5M training data, both prefix LM and MoD markedly lag behind CLM under limited parallel data (100K or 1M). Surprisingly, with 100K, mod- els fine-tuned using prefix LM and MoD even underperform their zero-shot performance. Con- versely, CLM demonstrates a healthy improvement as the amount of parallel data increases. Conse- quently, we adopt CLM as our primary training objective for machine translation.\n",
      "MoD\n",
      "5MTraining Parallel Data Size\n",
      "CLM\n",
      "1M\n",
      "40BLEU\n",
      "5\n",
      "20\n",
      "100K\n",
      "10\n",
      "30.4932.5934.5040.8530.4924.5032.3540.5430.4926.1230.9739.03\n",
      "0\n",
      "15\n",
      "Prefix LM\n",
      "0 (zero-shot)\n",
      "25\n",
      "30\n",
      "35\n",
      "100K\n",
      "85COMET\n",
      "Prefix LM\n",
      "79.3879.2680.2882.2379.3874.4078.9783.2279.3875.0178.0783.30\n",
      "5MTraining Parallel Data Size\n",
      "75\n",
      "80\n",
      "70\n",
      "CLM\n",
      "1M\n",
      "MoD\n",
      "65\n",
      "0 (zero-shot)\n",
      "60\n",
      "(a) BLEU\n",
      "(b) COMET\n",
      "Figure 6: The comparison of translation performance across various training objectives and parallel data sizes is depicted. or datasets of 100K and 1M, both prefix LM and MoD lag behind CLM and even undeperform the zero-shot performance. Notably, only CLM demonstrates a healthy improve- ment as the volume of training data increases.\n",
      "15\n",
      "Published as a conference paper at ICLR 2024\n",
      "B FULL RESULTS OF ZERO-SHOT EVALUATION\n",
      "In Section 2.2, we present the average zero-shot translation performance of recently released LLMs. Detailed results for each translation direction can be found in Table 4.\n",
      "Models\n",
      "de\n",
      "cs\n",
      "is\n",
      "zh\n",
      "ru\n",
      "Avg.\n",
      "BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET\n",
      "9.79 OPT-7B 7.31 BLOOM-7B Faclon-7B 19.23 LLaMA-1-7B 21.00 MPT-7B 20.91 LLaMA-2-7B 19.00\n",
      "24.43 OPT-7B BLOOM-7B 22.06 Faclon-7B 29.21 LLaMA-1-7B 29.14 MPT-7B 29.32 LLaMA-2-7B 30.42\n",
      "65.74 62.21 77.30 79.50 78.56 76.39\n",
      "78.37 74.10 82.02 81.90 81.80 82.74\n",
      "2.95 3.09 5.86 16.31 11.95 16.02\n",
      "14.82 6.06 20.06 32.93 27.45 36.56\n",
      "Translating from English (en→xx) 51.55 56.22 57.04 78.16 69.80 79.13 Translating to English (xx→en) 66.86 55.18 71.15 81.18 76.12 82.42\n",
      "1.42 1.49 1.69 2.42 3.21 1.33\n",
      "45.66 49.97 37.53 34.92 41.71 43.83\n",
      "1.59 20.41 26.90 15.63 25.41 16.97\n",
      "3.13 2.14 4.29 6.78 12.44 10.98\n",
      "52.63 48.70 52.53 58.15 62.76 62.79\n",
      "3.35 13.66 19.45 13.29 19.72 18.19\n",
      "48.84 74.03 79.28 68.03 80.20 71.80\n",
      "54.34 74.62 76.68 72.09 77.25 75.00\n",
      "1.31 5.89 4.61 17.61 13.99 16.00\n",
      "4.47 20.06 19.50 32.93 30.55 36.02\n",
      "41.57 56.55 53.55 76.73 72.43 73.24\n",
      "53.30 69.27 73.19 81.71 79.21 82.84\n",
      "50.67 3.41 59.80 7.64 11.66 60.94 14.59 67.47 15.09 68.54 13.86 68.88\n",
      "10.04 61.10 12.80 64.37 18.50 71.11 23.01 75.01 23.90 75.43 26.43 77.16\n",
      "Table 4: The detailed results of LLM zero-shot performance in Figure 3\n",
      "C TRAINING DETAILS\n",
      "We fine-tune the backbone model using a warm-up ratio of 0.01, a maximum sequence length of 512 tokens, and a weight decay of 0.01. The test data from WMT’21 serves as our development set. The training spans 3 epochs (for MPT-7B as detailed in Section 3, and 2 epochs for LLaMA-2 human-written data fine-tuning). The best model is selected based on the lowest validation loss, with validation performed every 10% of the total training progress. We utilize 16 MI200 GPUs for training; each GPU manages 4 batches and has a gradient accumulation step of 4, yielding an effective batch size of 256. The peak learning rate is set at 2e-5 , with an inverse square learning rate decay to 0. The training operates under fp16 precision, facilitated by deepspeed Rasley et al. (2020), employing ZeRO stage 2.\n",
      "D DATA INFORMATION\n",
      "D.1 SAMPLING RATIO FOR MONOLINGUAL DATA\n",
      "In Table 5, we observe a substantial imbalance in the volume of monolingual data available for differ- ent languages, denoted by their respective word counts8. Specifically, the English language dataset contains 523.9B words, vastly outnumbering other languages, such as Icelandic, which contains 0.3B words. Utilizing an unmodified concatenation and shuffling approach for this data would dis- proportionately prioritize English, undermining our objective of enhancing the model’s proficiency in non-English languages. To address this, we straightforwardly set the sampling ratio for English as P(l = en) = 1 6, thereby ensuring a balanced learning emphasis. The remaining 5 6 of the probability allocation employs temperature sampling, as suggested by Aharoni et al. (2019), a technique preva- lently adopted in the processing of unbalanced multilingual machine translation. Consequently, the process of selecting a monolingual example from language l adheres to the following distribution:\n",
      "P(l) ∝ (\n",
      "(cid:80)\n",
      "Dl l′∈L Dl′\n",
      ")\n",
      "1 T\n",
      "s.t.\n",
      "(cid:88)\n",
      "l′∈L\n",
      "P(l′) =\n",
      "5 6\n",
      "where Dl is the amount of the data in language l, T is the temperature, and L is the set of all languages except for English. The temperature we use is 6.\n",
      "8https://huggingface.co/datasets/oscar-corpus/OSCAR-2301\n",
      "16\n",
      "(3)\n",
      "Published as a conference paper at ICLR 2024\n",
      "Parallel Data\n",
      "Monolingual Data\n",
      "German (de) Czech (cs) Icelandic (is) Chinese (zh) Russian (ru) English (en)\n",
      "Train Development Test (from English) Test (to English) 14211 12076 2009 15406 15000 -\n",
      "1002 1002 - 1002 1002 -\n",
      "2037 2037 1000 2037 2037 -\n",
      "1984 1448 1000 1875 2016 -\n",
      "# Words 73.8B 9.7B 0.3B 44.4B 78.0B 523.9B\n",
      "Sampling Ratio 20% 14% 8% 19% 22% 17%\n",
      "Table 5: The statistics for the data we utilize for the monolingual data fine-tuning and human-written data fine-tuning.\n",
      "D.2 DATA STATISTICS\n",
      "We show data statistics in Table 5. The training parallel data is sourced from the WMT’17 to WMT’20. The development data was acquired from WMT’21, and the test data was derived from WMT’22, with the exception of the Icelandic dataset, which was procured from WMT’21. This means, Icelandic does not have development dataset. Additionally, the monolingual data was ex- tracted from the Oscar dataset.\n",
      "E OFF-TARGET ISSUE FOR LLAMA-2-13B\n",
      "In the zero-shot scenario, the performance of LLaMA-2-13 is reasonable for translations into En- glish. However, we identify a significant off-target issue with LLaMA-2-13B when translating from English to other languages. This issue is highlighted in Table 6 using a red highlighted box . An illustrative example of the off-target issue is provided below:\n",
      "Translate this from English to Russian: English: Plug the wall charger (not included) to a power outlet, and then connect your eReader to the wall charger. Russian: Comment: I’m voting to close this question as off-topic because it is not about programming.\n",
      "Models\n",
      "de\n",
      "cs\n",
      "is\n",
      "zh\n",
      "ru\n",
      "Avg.\n",
      "BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET\n",
      "13.69 zero-shot Prompt in Target Language 25.91 25.71 Filtered 1-shot 26.32 Filtered 5-shot 26.33 HW 5-shot 31.47 ALMA-13B-LoRA (ours)\n",
      "zero-shot 31.06 Prompt in Target Language 31.06 30.75 Filtered 1-shot 30.92 Filtered 5-shot 31.52 HW 5-shot 31.14 ALMA-13B-LoRA (ours)\n",
      "Backbone Model: LLaMA-2-13B, Translating from English (en→xx) 2.36 2.05 2.78 2.78 3.04 26.68 Backbone Model: LLaMA-2-13B, Translating to English (xx→en) 15.77 15.77 13.71 17.85 17.88 36.95\n",
      "75.55 81.88 80.85 81.67 82.63 85.62\n",
      "0.87 20.80 20.77 20.89 21.87 32.38\n",
      "68.57 81.82 81.30 81.45 82.66 89.79\n",
      "38.47 40.80 42.97 42.62 41.93 86.08\n",
      "30.00 31.82 31.70 32.01 30.73 39.84\n",
      "79.70 82.08 82.12 82.02 82.65 85.96\n",
      "83.01 83.01 82.91 83.41 83.57 84.56\n",
      "40.02 40.02 39.47 41.44 42.10 45.28\n",
      "83.27 83.27 82.90 83.81 84.69 86.47\n",
      "66.35 66.35 64.73 68.22 69.93 86.42\n",
      "21.81 21.81 21.00 19.86 23.26 25.46\n",
      "78.10 78.10 78.35 78.15 79.36 80.21\n",
      "0.59 22.66 22.32 23.26 22.77 28.96\n",
      "36.50 36.50 37.13 36.46 37.42 40.27\n",
      "63.84 83.29 83.03 83.28 84.21 87.53\n",
      "82.91 82.91 82.85 82.26 84.12 85.27\n",
      "9.50 20.65 20.66 21.05 20.95 31.87\n",
      "29.03 29.03 28.41 29.31 30.44 35.82\n",
      "65.23 73.97 74.05 74.21 74.82 87.00\n",
      "78.73 78.73 78.35 79.17 80.33 84.59\n",
      "Table 6: We demonstrate the off-target problem encountered during zero-shot translation from En- glish to other languages using the LLaMA-2-13B model. Instances of this issue are highlighted within red boxes . Implementing prompts in the target languages and incorporating few-shot learn- ing can markedly alleviate this issue. It is pertinent to note that the quality of the shots also influences the final outcomes.\n",
      "Expectedly, the model should produce translations in Russian. Yet, LLaMA-2-13B outputs “I’m voting to ...”, indicating a misinterpretation of the task, potentially linked to its pre-training phase. We address this off-target behavior through two methods.\n",
      "Prompt in the Target Language One approach is to utilize prompts in the target language (Rau- nak et al., 2023). For instance, when translating from English to Chinese, the preferred prompt\n",
      "17\n",
      "Published as a conference paper at ICLR 2024\n",
      "is: ”将其从英文翻译成中文：\\n英文：<source sentence>\\n中文：” as opposed to ”Translate this from English to Chinese:\\nEnglish:<source sentence>\\nChinese:”. Employing this technique markedly enhances the zero-shot performance of LLaMA-2-13B. Specifically, the BLEU score es- calates from 0.87 to 20.80 for en→cs, and from 0.59 to 22.66 for en→ru.\n",
      "In-Context Few-Shot Learning Employing in-context few-shot learning by including several ex- amples within the prompt has proven effective. We investigate both 1-shot and 5-shot learning scenarios. As delineated in Section I, we utilize two sets of examples: Filtered, extracted from the WMT training data, and another set randomly chosen from human-written data, termed HW. Table 6 demonstrates that both 1-shot and 5-shot configurations effectively counteract the off-target chal- lenges. Few-shot learning exhibits performance comparable to the strategy of using prompts in the target language. Moreover, echoing observations from Section I, examples of human-written quality outperform those from the Filtered set.\n",
      "Nevertheless, both strategies trail behind our proposed solution by a margin of approximately 5 BLEU and COMET points during translations into English, and by over 10 BLEU and COMET points in translations originating from English.\n",
      "Models\n",
      "de\n",
      "cs\n",
      "is\n",
      "zh\n",
      "ru\n",
      "Avg.\n",
      "BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET\n",
      "Translating from English (en→xx)\n",
      "34.50 NLLB-54B 31.80 GPT-3.5-D 28.02 1B 29.68 2B 29.25 3B 29.61 4B 29.52 5B 29.49 6B 29.46 7B 29.31 8B 29.36 9B 29.47 10B 29.55 11B 12B 29.71 12B, beam size=5 31.37\n",
      "26.89 NLLB-54B 30.90 GPT-3.5-D 30.66 1B 30.26 2B 30.14 3B 30.14 4B 30.20 5B 30.22 6B 30.37 7B 30.16 8B 30.11 9B 29.93 10B 30.57 11B 12B 30.40 12B, beam size=5 30.73\n",
      "86.45 85.61 84.24 85.04 84.82 85.24 85.04 85.01 85.11 84.92 84.85 84.82 85.14 85.02 85.45\n",
      "78.94 84.79 84.36 84.32 84.27 84.38 84.42 84.35 84.36 84.33 84.30 84.32 84.33 84.30 84.42\n",
      "37.60 31.30 25.40 27.18 28.26 28.27 28.29 28.45 28.25 27.93 27.86 29.18 28.94 28.78 31.12\n",
      "39.11 44.50 43.71 42.46 42.22 42.84 42.89 42.85 42.77 43.25 42.90 43.02 43.42 43.16 44.68\n",
      "90.15 27.38 38.30 88.57 35.54 87.34 36.12 88.00 37.06 88.31 37.26 88.29 37.19 88.43 37.16 88.43 37.26 88.45 37.19 88.33 37.15 88.11 37.41 88.41 37.60 88.41 37.75 88.49 39.05 89.42 Translating to English (xx→en) 16.56 80.13 25.00 86.16 23.22 86.06 22.66 85.86 22.56 85.98 23.18 86.03 23.32 86.14 23.40 86.22 22.76 86.11 22.90 85.98 22.50 85.97 22.54 86.10 22.98 86.11 23.89 86.17 86.29 24.65\n",
      "24.15 15.90 21.35 23.49 23.60 23.90 23.85 24.31 24.27 23.84 24.43 25.59 25.38 25.10 26.67\n",
      "81.76 76.28 83.05 84.30 84.62 84.42 84.58 84.63 84.78 84.74 84.60 85.09 85.18 84.98 85.85\n",
      "23.09 31.90 34.96 34.30 34.55 34.86 34.52 34.75 35.86 34.85 35.21 35.98 36.19 35.73 36.46\n",
      "71.66 82.13 85.54 85.63 85.79 85.75 85.87 85.96 86.12 85.83 85.85 86.09 86.14 86.19 86.30\n",
      "78.91 85.76 84.80 85.10 85.27 85.40 85.42 85.45 85.43 85.30 85.30 85.31 85.43 85.47 85.76\n",
      "70.70 81.62 79.88 79.88 79.64 79.95 80.07 79.94 79.86 79.82 79.52 79.77 79.84 80.17 79.90\n",
      "30.96 27.50 25.48 26.17 26.38 27.02 26.50 26.92 26.80 26.27 26.41 27.71 27.32 27.64 28.76\n",
      "39.11 38.50 38.87 37.30 38.31 38.45 38.07 38.25 37.95 37.42 37.74 37.86 38.40 38.49 40.37\n",
      "87.92 86.74 86.31 86.56 86.72 86.91 86.85 86.90 86.95 86.76 86.52 86.98 86.96 86.99 87.50\n",
      "81.88 84.80 84.88 84.70 84.77 84.90 85.02 84.90 84.90 84.84 84.92 84.88 84.88 84.89 85.09\n",
      "30.92 28.96 27.16 28.53 28.91 29.21 29.07 29.27 29.21 28.91 29.04 29.87 29.76 29.80 31.39\n",
      "28.95 34.16 34.28 33.40 33.56 33.89 33.80 33.89 33.94 33.72 33.69 33.87 34.31 34.33 35.38\n",
      "85.04 84.59 85.15 85.80 85.95 86.05 86.06 86.08 86.14 86.01 85.88 86.12 86.22 86.19 86.80\n",
      "76.66 83.90 84.14 84.08 84.09 84.20 84.30 84.27 84.27 84.16 84.11 84.23 84.26 84.34 84.40\n",
      "Table 7: The comprehensive numeric results for LLaMA-2-13B fine-tuned by every 1B monolingual tokens followed by human-written data fine-tuning.\n",
      "F NUMERIC RESULTS FOR MODELS FINE-TUNED WITH EVERY 1B TOKENS\n",
      "In Table 7 and 8, results for LLaMA-2-13B and LLaMA-2-7B are presented. Both models were fine-tuned at every 1B-token interval (comprising six languages) before subsequent fine-tuning with human-written parallel data. Full-weight fine-tuning was employed to ensure a consistent compar- ison. During inference, the 7B models utilized a beam search of size 5, while the 13B models adopted a greedy search strategy. For 13B models, we only utilize a beam size 5 for the final models we reported in the main manuscript (Table 1 and 2).\n",
      "18\n",
      "Published as a conference paper at ICLR 2024\n",
      "The data from these tables highlight that fine-tuning only 1B tokens, followed by human-written data fine-tuning, is adequate to compete with or even outperform the state-of-the-art (SoTA) models.\n",
      "Models\n",
      "de\n",
      "cs\n",
      "is\n",
      "zh\n",
      "ru\n",
      "Avg.\n",
      "BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET\n",
      "NLLB-54B 34.50 GPT-3.5-D 31.80 28.40 1B 28.96 2B 29.10 3B 29.02 4B 29.34 5B 28.78 6B 28.72 7B 29.03 8B 28.97 9B 29.25 10B 29.62 11B 29.85 12B 29.88 13B 29.95 14B 30.10 15B 30.12 16B 30.07 17B 29.63 18B 30.01 19B 30.31 20B\n",
      "86.45 85.61 84.45 84.62 84.66 84.75 84.89 84.61 84.83 84.78 84.79 84.81 85.23 85.15 85.20 85.23 85.22 85.32 85.32 85.40 85.25 85.59\n",
      "37.60 31.30 26.99 28.05 28.68 28.14 29.00 28.31 27.72 28.76 28.06 27.97 28.77 28.90 29.30 29.59 29.79 29.65 29.32 29.14 29.75 29.88\n",
      "Translating from English (en→xx) 90.15 88.57 87.91 88.34 88.46 88.53 88.82 88.56 88.49 88.64 88.39 88.52 88.68 88.67 88.80 89.09 89.09 89.14 88.71 89.02 89.06 89.10\n",
      "24.15 15.90 20.64 22.23 23.23 23.78 24.16 23.85 23.88 23.49 23.57 23.55 24.27 24.68 24.78 25.02 25.21 24.87 25.28 25.11 25.66 25.71\n",
      "81.76 76.28 83.22 84.44 84.74 84.94 84.76 84.82 84.86 84.94 85.04 85.08 85.08 85.27 85.24 85.20 85.40 85.34 85.13 85.33 85.37 85.52\n",
      "27.38 38.30 35.09 34.39 35.50 35.51 35.82 34.96 35.18 35.38 35.11 35.60 35.75 36.31 36.35 36.37 36.27 36.58 36.24 36.64 36.87 36.48\n",
      "78.91 85.76 84.41 84.08 84.40 84.65 84.71 84.43 84.33 84.66 84.49 84.66 84.73 84.78 84.77 84.83 84.78 84.93 84.89 84.96 85.11 85.05\n",
      "30.96 27.50 25.10 26.02 26.35 26.22 26.21 26.03 26.17 26.42 26.20 26.18 26.55 26.95 26.98 27.00 27.37 26.97 27.43 26.96 27.13 27.09\n",
      "87.92 86.74 86.33 86.37 86.75 86.68 86.74 86.67 86.54 86.45 86.70 86.58 86.91 87.00 87.05 87.10 86.94 86.98 87.05 87.02 86.98 87.17\n",
      "30.92 28.96 27.24 27.93 28.57 28.53 28.91 28.39 28.33 28.62 28.38 28.51 28.99 29.34 29.46 29.59 29.75 29.64 29.67 29.50 29.88 29.89\n",
      "85.04 84.59 85.26 85.57 85.80 85.91 85.98 85.82 85.81 85.89 85.88 85.93 86.13 86.17 86.21 86.29 86.29 86.34 86.22 86.35 86.35 86.49\n",
      "Translating to English (xx→en)\n",
      "NLLB-54B 26.89 GPT-3.5-D 30.90 29.40 1B 29.53 2B 30.15 3B 29.82 4B 30.09 5B 30.26 6B 29.44 7B 29.69 8B 29.76 9B 29.05 10B 29.39 11B 29.49 12B 29.51 13B 29.65 14B 29.48 15B 29.67 16B 29.48 17B 29.43 18B 29.54 19B 29.49 20B\n",
      "78.94 84.79 83.99 84.00 84.00 83.98 84.15 84.00 83.87 84.00 83.94 83.87 83.82 84.00 83.94 83.93 83.89 83.99 83.93 83.98 84.06 83.98\n",
      "39.11 44.50 41.64 43.32 43.08 43.26 43.39 43.91 42.53 42.85 42.89 41.88 43.42 43.22 42.90 42.89 43.21 43.27 43.16 43.25 42.85 42.91\n",
      "80.13 86.16 85.54 85.66 85.79 85.92 85.97 85.86 85.90 85.68 85.90 85.61 85.95 85.97 85.88 85.83 85.93 86.03 85.81 85.94 85.83 85.90\n",
      "23.09 31.90 33.35 33.79 34.43 34.55 35.26 35.46 34.33 34.38 34.47 33.69 34.87 35.24 35.34 35.47 35.72 35.54 35.19 35.16 35.97 35.26\n",
      "71.66 82.13 84.76 85.17 85.47 85.59 85.77 85.82 85.71 85.69 85.46 85.40 85.69 85.74 85.78 85.83 85.88 85.90 85.81 85.83 86.03 85.97\n",
      "16.56 25.00 22.45 22.19 22.70 23.27 23.65 23.75 23.23 22.92 23.03 22.97 22.68 22.94 22.80 22.40 22.74 22.95 22.90 23.57 23.42 23.52\n",
      "70.70 81.62 79.12 78.98 79.29 79.84 80.05 79.85 79.76 79.31 79.57 79.29 79.61 79.65 79.64 79.65 79.34 79.57 79.35 79.69 79.56 79.73\n",
      "39.11 38.50 38.15 38.82 39.32 39.00 38.81 39.37 38.60 38.54 38.01 37.83 38.13 38.72 38.65 38.67 39.04 39.28 38.50 38.32 39.34 38.93\n",
      "81.88 84.80 84.34 84.59 84.61 84.62 84.65 84.58 84.58 84.47 84.50 84.28 84.44 84.59 84.60 84.70 84.63 84.79 84.66 84.64 84.83 84.81\n",
      "28.95 34.16 33.00 33.53 33.94 33.98 34.24 34.55 33.63 33.68 33.63 33.08 33.70 33.92 33.84 33.82 34.04 34.14 33.85 33.95 34.22 34.02\n",
      "76.66 83.90 83.55 83.68 83.83 83.99 84.12 84.02 83.96 83.83 83.87 83.69 83.90 83.99 83.97 83.99 83.93 84.06 83.91 84.02 84.06 84.08\n",
      "Table 8: The comprehensive numeric results for LLaMA-2-7B fine-tuned by every 1B monolingual tokens followed by human-written data fine-tuning.\n",
      "G DETAILED RESULTS IN ABLATION STUDY\n",
      "We show the detailed results of the ablation study on the effect of monolingual data and the quality of the data in Table 9.\n",
      "H IS MORE HUMAN-WRITTEN PARALLEL DATA BETTER?\n",
      "The composition of our human-written data consists of the prior-year WMT test sets (approximately 10K parallel sentences per pair) and Flores data (around 2K per pair). In this analysis, we assess the\n",
      "19\n",
      "Published as a conference paper at ICLR 2024\n",
      "Use mono. Parallel Data Quality\n",
      "de\n",
      "cs\n",
      "is\n",
      "zh\n",
      "ru\n",
      "Avg.\n",
      "BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET BLEU COMET\n",
      "✘ ✘ ✘ ✘ ✔ ✔ ✔ ✔\n",
      "✘ ✘ ✘ ✘ ✔ ✔ ✔ ✔\n",
      "✘ Random Filtered HW ✘ Random Filtered HW\n",
      "✘ Random Filtered HW ✘ Random Filtered HW\n",
      "19.00 22.74 21.92 27.30 27.44 27.38 27.97 30.31\n",
      "30.42 29.15 29.29 29.95 28.28 28.89 28.63 29.49\n",
      "76.39 78.06 77.59 83.46 84.17 82.12 83.16 85.59\n",
      "82.74 82.33 82.42 83.93 82.48 82.74 82.85 83.98\n",
      "Translating from English (en→xx) 16.02 19.38 19.93 22.59 28.61 28.43 28.45 29.88 Translating to English (xx→en) 36.56 38.61 38.41 40.32 38.05 40.64 40.93 42.91\n",
      "79.13 79.59 80.24 84.59 88.57 86.82 87.26 89.10\n",
      "1.33 6.20 6.91 3.61 21.55 21.65 23.03 25.71\n",
      "43.83 50.45 51.42 45.81 83.69 80.53 82.40 85.52\n",
      "82.42 82.67 82.80 84.31 84.18 85.01 84.85 85.90\n",
      "10.98 17.14 17.89 15.61 32.79 35.11 35.12 35.26\n",
      "62.79 68.25 69.05 69.13 84.07 85.67 85.49 85.97\n",
      "16.97 27.66 27.15 33.74 28.51 31.68 31.55 36.48\n",
      "18.19 19.32 19.22 22.51 9.44 19.50 19.04 23.52\n",
      "71.80 79.70 80.09 83.81 81.56 81.73 82.26 85.05\n",
      "75.00 77.24 77.41 78.77 69.71 77.67 77.92 79.73\n",
      "16.00 22.40 21.90 23.63 25.65 25.74 25.92 27.09\n",
      "36.02 36.98 37.12 38.56 33.88 38.19 37.90 38.93\n",
      "73.24 81.64 82.42 84.94 85.67 84.53 84.84 87.17\n",
      "82.84 82.97 83.04 83.88 81.18 84.01 84.02 84.81\n",
      "13.86 19.68 19.56 22.17 26.35 26.98 27.38 29.89\n",
      "26.43 28.24 28.39 29.39 28.49 32.47 32.32 34.02\n",
      "68.88 73.89 74.35 76.52 84.73 83.15 83.98 86.49\n",
      "77.16 78.69 78.94 80.00 80.32 83.02 83.03 84.08\n",
      "Table 9: Detailed results of ablation study on the effect of monolingual data and parallel data qual- ity. The backbone model is LLaMA-2-7B. A red cross (✘) in the table denotes the omission of monolingual data fine-tuning or parallel data (indicative of zero-shot translation). A green check (✔) signifies that the model undergoes fine-tuning with monolingual data.\n",
      "Parallel Data Used\n",
      "Avg. xx→en\n",
      "Avg. en→xx\n",
      "BLEU COMET BLEU COMET\n",
      "Backbone: LLaMA-2-7B After Stage 1 29.28 29.89\n",
      "Flores Flores+WMT\n",
      "30.50 34.02\n",
      "83.24 84.08\n",
      "86.52 86.49\n",
      "Table 10: The performance of LLaMa-2-7B (post stage 1 fine-tuning) when fine-tuned exclusively on Flores versus when fine-tuned on both WMT and Flores.\n",
      "impact of additional human-written parallel data. Specifically, we compare models (LLaMa-2-7B after stage 1) fine-tuned exclusively on Flores against those fine-tuned on both Flores and WMT data. Results can be found in Table 10. Notably, upon integrating WMT data into the training set, we discern a modest improvement in COMET scores. However, there’s an uptick in BLEU scores, particularly for translations into English. We attribue the increase in lexical match (BLEU) to the domain alignment of WMT data. Consequently, our hypothesis is that while an augmented volume of human-written data might marginally enhance segment-level human judgment correlation (COMET), in-domain data can significantly enhance lexical matching.\n",
      "Methods\n",
      "Avg. xx→en\n",
      "Avg. en→xx\n",
      "BLEU COMET BLEU COMET\n",
      "Backbone: LLaMA-2-13B After Stage 1\n",
      "Zero-Shot Filtered 5-shot HW 5-shot Our Stage 2 Our Stage 2 + HW 5-shot\n",
      "33.07 33.12 33.75 34.31 34.14\n",
      "83.07 83.13 83.91 84.12 84.27\n",
      "26.76 27.50 27.59 29.78 28.56\n",
      "84.03 83.78 85.24 86.37 85.87\n",
      "Table 11: The performance between 5-shot ICL and stage 2 fine-tuning using the LLaMA-2-13B model post stage 1 as the backbone. Our findings indicate that the quality of shots affects ICL performance. Notably, stage 2 fine-tuning markedly surpasses the 5-shot ICL and ICL does not help more on stage 2.\n",
      "I PARALLEL DATA FINE-TUNING VS. IN-CONTEXT LEARNING\n",
      "An alternative way to instruct the model to have better translation is in-context learning (ICL) (Brown et al., 2020), as opposed to additional fine-tuning on parallel data. However, ICL is lim- ited to only a few shots given the length of translation examples, while fine-tuning can leverage\n",
      "20\n",
      "Published as a conference paper at ICLR 2024\n",
      "entirely available data. For ICL, we consider 5-shot evaluations. 5 examples are randomly selected from Filtered data (the Quality-Random examples used by Hendy et al. (2023)). We also consider another 5 examples randomly from the human-written data to examine the impact of example qual- ity. We here compare the performance of our fine-tuning method and 5-shot ICL.9 We assess the LLaMA-2-13B after stage 1 (12B token fine-tuning) and present results in Table 11.\n",
      "Interestingly, ICL also holds the same property that higher quality data leads to better performance (Filtered 5-shot vs. HW 5-shot). Moreover, as expected, ICL substantially underperforms our stage 2 fine-tuning possibly due to the small examples provided, which aligns with the findings in the previous work (Liu et al., 2022; Mosbach et al., 2023). This could also clarify why implementing ICL subsequent to stage 2 yields no additional benefits, as all high-quality data has already been incorporated during stage 2 fine-tuning (the last row in the Table).\n",
      "J CROSS-LINGUAL PROFICIENCY OF FINE-TUNED MODELS\n",
      "We explore the cross-lingual competencies of our models derived from LLaMA-2 after fine-tuning them on monolingual data. Our aim is to discern whether augmenting monolingual data enhances performance in cross-lingual tasks. Experiments were conducted on zero-shot cross-lingual tasks en- compassing three benchmarks: Cross-lingual language understanding (XNLI) Conneau et al. (2018), XStoryCloze—a translation of the English StoryCloze dataset into ten languages (Mostafazadeh et al., 2017), and XWinograd—a multilingual compilation of Winograd Schemas Tikhonov & Ryabinin (2021). Evaluations were restricted to languages overlapping with our fine-tuned lan- guages, namely, German (with only XNLI being inclusive), Chinese, Russian, and English. Unfor- tunately, none of these datasets covers Icelandic. We first consider baselines for some widely used models: XLM-R large (Conneau et al., 2020), XGLM-7.5B (Lin et al., 2021), BLOOM-7B (Scao et al., 2022), and MPT-7B (MosaicML, 2023). In these comparisons, LLaMA-2 demonstrates the top performance for the tested languages. Subsequent fine-tuning with either 1B or 20B monolin- gual tokens on both LLaMA-2-7B and 13B models yields substantial enhancements for non-English languages across all tasks. A consistent trend observed was that increased monolingual data cor- responds to greater performance boosts. Only English is observed for a negligible difference after fine-tuning monolingual data, which is an anticipated outcome given LLaMA-2’s proficient grasp of English. The tool we utilize for LLM evaluation is lm-evaluation-harness (Gao et al., 2021).10\n",
      "Models\n",
      "XNLI\n",
      "Xstorycloze\n",
      "XWinograd\n",
      "XLMR-Large XGLM-7.5B BLOOM-7B MPT-7B LLaMA-2-7B LLaMA-2-7B, 1B mono. LLaMA-2-7B, 20B mono. LLaMA-2-13B LLaMA-2-13B, 1B mono. LLaMA-2-13B, 12B mono.\n",
      "de 32.29 48.31 39.04 47.87 45.90 47.63 50.48 49.08 47.27 49.56\n",
      "en 27.51 54.06 53.37 55.62 56.51 57.87 57.35 53.21 49.56 54.02\n",
      "ru 31.20 46.55 42.61 46.10 41.33 44.02 46.47 44.74 44.14 47.55\n",
      "zh 33.41 34.66 35.50 36.71 34.82 34.70 33.82 36.14 38.35 40.72\n",
      "Avg. 31.10 45.90 42.63 46.58 44.64 46.06 47.03 45.79 44.83 47.96\n",
      "en 49.83 69.82 70.48 78.09 77.04 75.84 75.71 78.36 78.09 78.29\n",
      "ru 48.25 63.34 52.68 57.71 63.07 64.59 67.57 66.18 68.63 69.82\n",
      "zh 46.46 58.90 61.88 59.50 59.56 60.03 62.81 63.40 62.01 63.67\n",
      "Avg. 48.18 64.02 61.68 65.10 66.56 66.82 68.70 69.31 69.58 70.59\n",
      "en 47.31 79.35 82.06 86.58 87.91 85.63 86.06 88.99 88.47 88.82\n",
      "ru 48.89 63.17 56.83 68.89 68.89 66.98 66.67 68.25 69.21 70.16\n",
      "zh 44.05 72.82 74.21 73.21 70.63 71.83 75.20 77.98 78.17 78.17\n",
      "Table 12: We evaluate the zero-shot cross-lingual efficacy on three multilingual datasets. Our find- ings indicate that fine-tuning LLaMA-2 with more monolingual data results in enhanced perfor- mance for non-English languages.\n",
      "9Due to ICL’s extended prompt length, employing a large beam size is impractical; hence, we opt for a beam\n",
      "size of 1 for all to ensure a fair comparison.\n",
      "10https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor\n",
      "21\n",
      "Avg. 46.75 71.78 71.03 76.23 75.81 74.81 75.98 78.41 78.62 79.05\n"
     ]
    }
   ],
   "source": [
    "print(extracted_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T09:47:50.224322500Z",
     "start_time": "2025-03-02T09:47:50.208480900Z"
    }
   },
   "id": "f75bf7c4cbe71795",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pymupdf4llm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T11:40:37.441103Z",
     "start_time": "2025-03-02T11:40:37.148035Z"
    }
   },
   "id": "5d19c059915e812d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /mnt/d/PycharmCode/LLMscratch/essay_searcher/pdfs/LexMatcher.pdf...\n",
      "[========================================]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/mnt/d/PycharmCode/LLMscratch/essay_searcher/pdfs/LexMatcher.pdf\"\n",
    "md_text = pymupdf4llm.to_markdown(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:01:31.520389400Z",
     "start_time": "2025-03-02T12:00:20.280834100Z"
    }
   },
   "id": "d3f3dee043210a39",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "a= md_text.split(\"###\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:01:36.629966300Z",
     "start_time": "2025-03-02T12:01:36.620512500Z"
    }
   },
   "id": "6fb90817c6b3a94b",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "15"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:01:38.728109Z",
     "start_time": "2025-03-02T12:01:38.718751300Z"
    }
   },
   "id": "4f868f0f780cd204",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "' 1 Introduction gate a principled method, LexMatcher, for curating\\n\\nsupervised fine-tuning data for LLM-based transla\\nThe emergence of large language models (LLMs)\\n\\ntion. The objective is to collect a small yet carefully\\n\\nhas brought about new opportunities for machine\\n\\nselected dataset that follows a proper distribution\\n\\ntranslation. The modeling paradigm has gradually\\n\\nfor maximizing translation quality. In particular,\\n\\nshifted from training sequence-to-sequence mod\\nwe leverage a bilingual dictionary as a pivotal re\\nels from scratch to utilizing commercial or open\\nsource to ensure comprehensive coverage of word\\n\\nsourced LLMs (Hendy et al., 2023; Agrawal et al.,\\n\\nor phrase senses in bilingual contexts. The con\\n2023; Zhu et al., 2023; Jiao et al., 2023; Xu et al.,\\n\\nstruction of the dataset involves two steps: sense\\n\\n2024). Unlike traditional neural machine transla\\nretrieval and sense supplement. In the sense re\\ntion methods, which rely on abundant parallel sen\\ntrieval step, we traverse commonly-used corpora\\n\\ntences (Vaswani et al., 2017; Gordon et al., 2021;\\n\\n(e.g., WMT training data), and identify sentence\\n\\nFernandes et al., 2023) and monolingual sentences\\n\\npairs that contain at least a segment pair that has not\\n\\n(Sennrich et al., 2016; Edunov et al., 2018), it has\\n\\nyet reached a matching threshold in the retrieved\\n\\nbeen shown that LLMs do not require much su\\nsubset. To prioritize the selection of high-quality\\n\\npervised fine-tuning data to achieve competitive\\n\\nsentence pairs, we employ a quality estimation\\n\\n*Corresponding author model to sort the data. Inevitably, there may be\\n\\n\\n-----\\n\\nuncovered senses of polysemous words after the\\nretrieval, representing crucial long-tail knowledge\\nessential for accurate translation. To address this,\\nwe employ commercial LLMs (e.g., ChatGPT) to\\ngenerate precise and concise demonstrations for the\\nuncovered senses. Finally, we fine-tune LLMs using a combination of the retrieved and synthesized\\nsubsets.\\nWe primarily utilize the training data from\\nWMT22 and conduct extensive experiments on six\\nlanguage directions, including Zh⇔En, En⇔De,\\nand En⇔Ru. By employing LexMatcher, we extract 0.1% of the data from the original corpus\\nand utilize a maximum of only 1 million samples\\nacross all six language directions. The results of\\nfine-tuned LLMs on the WMT22 test sets deliver\\nsuperiority over the baselines in both common and\\nzero-shot settings. The fine-tuned models also\\nachieve comparable or better performance in terminology translation and translation disambiguation\\ncompared to the dedicated or commercial systems.\\nFurthermore, the analyses of different data collection methods and composition generalization underscore the significance of high-quality data distributions. Finally, we showcase the complementarity of\\nthe collected parallel data with large-scale monolingual post-training by the experiment of fine-tuning\\nALMA (Xu et al., 2024). The code, data, and\\nmodels are available at https://github.com/ARIESLM/Lexmatcher-MT.git.\\n\\n'"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-02T12:02:33.678996700Z",
     "start_time": "2025-03-02T12:02:33.630588Z"
    }
   },
   "id": "ddd209b3ed657d4",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8d575723f7eb6b8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
